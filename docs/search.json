[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Digital Signal Processing with Python, Matlab or Octave",
    "section": "",
    "text": "Welcome\n\n\n\nPreface\n\n\n\n\n\n\n\n\n\n\nPreface\n\n\n\nThe information age brought new possibilities to improve the learning process, which can complement the traditional lectures and homeworks. With a clear impact in engineering, it is the era in which ITU and other standardization bodies opened access to many of their documents and there are several interesting open source and open hardware projects to help learning through practice. Hence, this book suggests several practical applications for which open source tools (Octave, GNU Radio, GSM projects, etc.) and low cost hardware (USRP, DVB-T dongle, HackRF peripherals, etc.) are key ingredients.\n\n\n\nCurrent communication systems heavily rely on digital signal processing (DSP) and this motivated the effort to present both. This way the book can benefit from using the same nomenclature for both DSP and communications. But there is a well-known exploration versus exploitation tradeoff when defining the contents of a book, especially given that DSP and digital communications are broad areas. In fact, when I teach them, I feel like the guide of a group of tourists with seven days to visit Europe and low budget. This book is the result of my belief that, in spite of being impossible to visit all nice places, it is realistic to learn the basics of two very important engineering subjects and have fun along the process, especially by using resources of interesting open projects.\n\n\n\nBecause there are so many good and comprehensive textbooks on the subject, I took the route of leaving out topics that are often part of classical courses in favor of including topics that I observe being required when building modern systems such as software defined radios. The choices were biased by my experience in research and development projects with companies such as Ericsson, Brasilsat and Comunix, which substantially influenced my teaching. Hence, the book aims at self-taught readers with a bias towards practice. It does not claim to be a textbook or the ideal reading for those preparing for exams.\n\n\n\nThis book benefits from free and open source. Accordingly, software developed for the book was made available at the book web site http://www.aldebaro.ufpa.br. Even the figures can be reproduced by the reader with the provided source code. Both Mathwork’s Matlab and Octave are capable of running most of the code. Matlab’s object-oriented programming (OOP) and specific toolboxes that hide important details were avoided. The intention is to motivate the reader to understand and develop his/her own software, not to become familiar with a library or GUI. This strategy also allowed to make most of the code compliant with both Octave and Matlab. The scripts that run only on one of them were organized in specific directories.\n\n\n\nBecause URLs significantly change over time, instead of listing them in this printed copy, all references identified by  [?], such as [urlFMitu] (a unique identifier following the prefix url), are organized (and kept updated) at the book web site.\n\n\n\nI adopted a self publishing strategy that allows the whole book to be printed in full color with a relatively low list price. On the other hand, I performed several tasks that are typically taken care of by a specialized publisher.1 It was not possible to have a professional proofreader reviewing the text and I apologize for any grammar error.\n\n\n\nIn 2008, the State Government of Pará, via its research agency, FAPESPA, sponsored my first book and got me started in this activity. I acknowledge here not only this support but all the work that the State Government did at that point in favor of science and technology in Pará, a Brazilian state surrounded by the magnificent Amazon, known for the world’s largest tropical rain forest and river basin. The cover2 of this book depicts this region and its needs for information and communications technologies, to move its economy from primary exploration of natural resources and establish new and green industries.\n\n\n\nThe current book was finalized at the University of California, San Diego (UCSD), during a sabbatical leave from the Federal University of Para (UFPA), Brazil, sponsored by the CAPES Foundation. Ten years after my Ph.D. graduation, I found myself again very grateful to my always advisor, Professor Alon Orlitsky, for hosting my stay as a visiting scholar at UCSD.3\n\n\n\nI also acknowledge my students and colleagues at UFPA, researchers at Ericsson and Brasilsat, friends and family for their support. The book is dedicated to my parents, Aldebaro and Regina. Now that my spouse and I try to raise three kids, I better understand the wonderful parents I have. Their wisdom still amazes me when I recollect teachings and arguments. My father accomplished so many amazing things, some of strong impact to those living in Pará, and still managed to make his kids proud by winning sport matches (soccer, table tennis, etc.), coaching us, being our math teacher, our friend, etc., while my mother has always unconditionally supported me, with love so intense that it is visible. In the context, an example of her important presence is that when I was loosing interest for engineering, she gave me my first computer, which led me to ask “-How does this work?” and ended up defining my career. And to share my gratitude to God for letting me enjoy writing this book, I use a quote from St. Paul’s 2nd Letter to the Corinthians: “Not that we are sufficient of ourselves to think any thing as of ourselves; but our sufficiency is of God”.\n\n\n \n\n1 Regarding Latex typesetting, I was lucky to count with the expertise of Martin Sievers [urlFMlat].\n\n \n\n2 The book cover art is by Bernardo Magalhães with Libra Design [urlFMlib].\n\n \n\n3 I acknowledge the wonderful support from UCSD library staff, especially Ms. Deborah Kegel."
  },
  {
    "objectID": "ak_dsp_bookli1.html",
    "href": "ak_dsp_bookli1.html",
    "title": "Contents",
    "section": "",
    "text": "Contents\n\n\n\n\n\n\n\n\n\n\nContents\n\n\n List of Figures   List of Tables  Preface  1 Analog and Digital Signals  1.1  To Learn in This Chapter  1.2  Analog, Digital and Discrete-Time Signals  1.2.1  Ambiguous notation: whole signal or single sample  1.2.2  Digitizing Signals  1.2.3  Discrete-time signals  1.3  Basic Signal Manipulation and Representation\n 1.3.1  Manipulating the independent variable  1.3.2  When the independent variable is not an integer  1.3.3  Frequently used manipulations of the independent variable  1.3.4  Using impulses to represent signals  1.3.5  Using step functions to help representing signals  1.3.6  The rect function  1.4  Block Processing  1.5  Complex-Valued and Sampled Signals  1.5.1  Complex-valued signals  1.5.2  Sampled signals  1.6  Modeling the Stages in A/D and D/A Processes  1.6.1  Modeling the sampling stage in A/D  1.6.2  Oversampling  1.6.3  Mathematically modeling the whole A/D process  1.6.4  Sampled to discrete-time (S/D) conversion  1.6.5  Continuous-time to discrete-time (C/D) conversion  1.6.6  Discrete-time to sampled (D/S) conversion  1.6.7  Reconstruction  1.6.8  Discrete-time to continuous-time (D/C) conversion  1.6.9  Analog to digital (A/D) and digital do analog (D/A) conversions  1.6.10  Sampling theorem  1.6.11  Different notations for S/D conversion  1.7  Relating Frequencies in Continuous, Sampled and Discrete-time Signals  1.7.1  Units of continuous-time and discrete-time angular frequencies  1.7.2  Mapping frequencies in continuous and discrete-time domains  1.7.3  Nyquist frequency  1.7.4  Frequency normalization in Matlab/Octave  1.8  An Introduction to Quantization  1.8.1  Quantization definitions  1.8.2  Implementation of a generic quantizer  1.8.3  Uniform quantization  1.8.4  Granular and overload regions  1.8.5  Design of uniform quantizers  1.8.6  Design of optimum non-uniform quantizers  1.8.7  Quantization stages: classification and decoding  1.8.8  Binary numbering schemes for quantization decoding  1.8.9  Quantization examples  1.9  Signal Categorization  1.9.1  Even and odd signals  1.9.2  Random signals and their generation  1.9.3  Periodic and aperiodic signals  1.9.4  Power and energy signals  1.10  Power and Energy in Discrete-Time  1.10.1  Power and energy of discrete-time signals\n 1.10.2  Power and energy of signals represented as vectors  1.10.3  Power and energy of vectors whose elements are not time-ordered  1.10.4  Power and energy of discrete-time random signals  1.11  Relating Power in Continuous and Discrete-Time  1.12  Correlation: Finding Trends  1.12.1  Autocorrelation function  1.12.2  Cross-correlation  1.13  A Linear Model for Quantization  1.14  Applications  1.15  Comments and Further Reading  1.16  Review Exercises  1.17  Exercises  2 Transforms and Signal Representation  2.1  To Learn in This Chapter  2.2  Linear Transform  2.2.1  Matrix multiplication corresponds to a linear transform  2.2.2  Basis: standard, orthogonal and orthonormal  2.3  Inner Products to Obtain the Transform Coefficients  2.4  Block Transforms  2.4.1  DCT transform  2.4.2  DFT transform  2.4.3  Haar transform  2.4.4  Unitary matrices lead to energy conservation  2.4.5  Orthogonal but not unitary also allows easy inversion  2.5  Fourier Transforms and Series  2.5.1  Fourier series for continuous-time signals  2.5.2  Discrete-time Fourier series (DTFS)  2.5.3  Continuous-time Fourier transform using frequency in Hertz  2.5.4  Continuous-time Fourier transform using frequency in rad/s  2.5.5  Discrete-time Fourier transform (DTFT)  2.6  Relating discrete and analog frequencies  2.7  Summary of equations for DFT / FFT Usage  2.8  Laplace Transform  2.9  Z Transform  2.9.1  Some pairs and properties of the Z-transform  2.9.2  Z-transform region of convergence  2.10  Applications  2.11  Comments and Further Reading  2.12  Review Exercises  2.13  Exercises  3 Analog and Digital Systems  3.1  To Learn in This Chapter  3.2  Contrasting Signals and Systems  3.3  Analog and Digital Filters: A Quick Hands-On Tour\n 3.3.1  Cutoff and natural frequencies  3.3.2  Designing simple filters using specialized software  3.4  Linear Time-Invariant Systems  3.4.1  Impulse response and convolution for LTI systems  3.4.2  Convolution properties  3.4.3  Convolution via correlation and vice-versa  3.4.4  Discrete-time convolution in matrix notation  3.4.5  Approximating continuous-time via discrete-time convolution  3.4.6  Frequency response: Fourier transform of the impulse response  3.4.7  Fourier convolution property  3.4.8  Circular and fast convolutions using FFT  3.5  Sampling and Signal Reconstruction Revisited  3.5.1  A proof sketch of the sampling theorem  3.5.2  Energy and power of a sampled signal  3.5.3  Energy / power conservation after sampling and reconstruction  3.5.4  Sampling theorem uses a strict inequality  3.5.5  Undersampling or passband sampling  3.5.6  Sampling a complex-valued signal  3.5.7  Signal reconstruction and D/S conversion revisited  3.6  Facts About First and Second-Order Systems  3.6.1  First-order systems  3.6.2  Second-order systems  3.7  Bandwidth and Quality Factor  3.7.1  Bandwidth and Quality Factor of Poles  3.7.2  Bandwidth and Quality Factor of Filters  3.8  Importance of Linear Phase (or Constant Group Delay)  3.9  Filtering technologies: Surface acoustic wave (SAW) and others  3.10  Digital Filters  3.10.1  FIR, IIR, AR, MA and ARMA systems  3.10.2  Filter frequency scaling  3.10.3  Filter bandform transformation: Lowpass to highpass, etc.  3.11  IIR Filter Design  3.11.1  Direct IIR filter design  3.11.2  Indirect IIR filter design  3.11.3  Methods to convert continuous into discrete-time system functions  3.11.4  Summary of methods to convert continuous-time system function into discrete-time  3.12  Bilinear Transformation  3.12.1  Bilinear mapping between s and z planes and vice-versa  3.12.2  Non-linear frequency warping imposed by bilinear  3.12.3  Bilinear for IIR filter design using pre-warping  3.12.4  Bilinear design of IIR filter from continuous-time specifications  3.12.5  Bilinear for IIR design when continuous-time system function is provided  3.12.6  Properties of the bilinear transform\n 3.12.7  Pre-warped bilinear applied to a first-order system  3.12.8  Bilinear for digital controller  3.13  FIR Filter Design  3.13.1  A FIR filter does not have finite poles  3.13.2  The coefficients of a FIR coincide with its impulse response  3.13.3  Algorithms for FIR filter design  3.13.4  FIR design via least-squares  3.13.5  FIR design via windowing  3.13.6  Two important characteristics: FIRs are always stable and can have linear phase  3.13.7  Examples of linear and non-linear phase filters  3.13.8  Zeros close to the unit circle may impact the phase linearity  3.13.9  Four types of symmetric FIR filters  3.14  Realization of Digital Filters  3.14.1  Structures for FIR filters  3.14.2  Structures for IIR filters  3.14.3  Running a digital filter using filter or conv  3.14.4  Effects of finite precision  3.15  Multirate Processing  3.15.1  Upsampler and interpolator  3.15.2  Downsampler and decimator  3.16  Applications  3.17  Comments and Further Reading  3.18  Exercises  3.19  Extra Exercises  4 Spectral Estimation Techniques  4.1  To Learn in This Chapter  4.2  Introduction  4.3  Windows for spectral analysis  4.3.1  Popular windows  4.3.2  Leakage  4.3.3  Picket-fence effect  4.3.4  Figures of merit applied to windows  4.3.5  Alternative representation of discrete-time sinusoids  4.3.6  Example of combined effects of leakage and picket-fence  4.3.7  Example of using windows in spectral analysis  4.3.8  Estimating sinusoid amplitude and correction of scalloping loss  4.4  The ESD, PSD and MS Spectrum functions  4.4.1  Energy spectral density (ESD)  4.4.2  Power spectral density (PSD)  4.4.3  Fourier Modulation Theorem Applied to PSDs  4.4.4  Mean-square (MS) spectrum  4.5  Filtering Random Signals and the Impact on PSDs  4.5.1  Response of LTI systems to random inputs\n 4.5.2  Continuous-time signals with a white PSD and their filtering  4.5.3  Discrete-time signals with a white PSD and their filtering  4.6  Nonparametric PSD Estimation via Periodogram  4.6.1  Discrete-time PSD estimation using the periodogram  4.6.2  Relation between MS spectrum and periodogram  4.6.3  Periodogram of periodic or energy signals  4.6.4  Examples of PSD and MS spectrum estimation  4.6.5  Estimating the PSD from Autocorrelation  4.7  Nonparametric PSD Estimation via Welch’s method  4.7.1  The periodogram variance does not decrease with N  4.7.2  Welch’s method for PSD estimation  4.8  Parametric PSD Estimation via Autoregressive (AR) Modeling  4.8.1  Spectral factorization  4.8.2  AR modeling of a discrete-time PSD  4.8.3  AR modeling of a continuous-time PSD  4.8.4  Yule-Walker equations and LPC  4.8.5  Examples of autoregressive PSD estimation  4.9  Time-frequency Analysis using the Spectrogram  4.9.1  Definitions of STFT and spectrogram  4.9.2  Wide and narrowband spectrograms  4.10  Applications  4.11  Comments and Further Reading  4.12  Exercises  4.13  Extra Exercises  A *  B Useful Mathematics  B.1  Euler’s equation  B.2  Trigonometry  B.3  Manipulating complex numbers and rational functions  B.4  Manipulating complex exponentials  B.5  Q function  B.6  Matched filter and Cauchy-Schwarz’s inequality  B.7  Geometric series  B.8  Sum of squares  B.9  Summations and integrals  B.10  Partial fraction decomposition  B.11  Calculus  B.12  Sinc Function  B.13  Rectangular Integration to Define Normalization Factors for Functions  B.13.1  Two normalizations for the histogram  B.13.2  Two normalizations for power distribution using FFT  B.14  Linear Algebra  B.14.1  Inner products and norms  B.14.2  Projection of a vector using inner product\n B.14.3  Orthogonal basis allows inner products to transform signals  B.14.4  Moore-Penrose pseudoinverse  B.15  Gram-Schmidt orthonormalization procedure  B.16  Principal component analysis (PCA)  B.17  Fourier Analysis: Properties  B.18  Fourier Analysis: Pairs  B.19  Probability and Stochastic Processes  B.19.1  Joint and Conditional probability  B.19.2  Random variables  B.19.3  Expected value  B.19.4  Orthogonal versus uncorrelated  B.19.5  PDF of a sum of two independent random variables  B.20  Stochastic Processes  B.20.1  Cyclostationary random processes  B.20.2  Two cyclostationary signals: sampled and discrete-time upsampled  B.20.3  Converting a WSC into WSS by randomizing the phase  B.21  Estimation Theory  B.21.1  Probabilistic estimation theory  B.21.2  Minimum mean square error (MMSE) estimators  B.21.3  Orthogonality principle  B.22  One-dimensional linear prediction over time  B.22.1  The innovations process  B.23  Vector prediction exploring spatial correlation  B.24  Decibel (dB) and Related Definitions  B.25  Insertion loss and insertion frequency response  B.26  Discrete and Continuous-Time Impulses  B.26.1  Discrete-time impulse function  B.26.2  Why defining the continuous-time impulse? Some motivation  B.26.3  Definition of the continuous-time impulse as a limit  B.26.4  Continuous-time impulse is a distribution, not a function  B.26.5  Mathematical properties of the continuous-time impulse  B.26.6  Convolution with an impulse  B.26.7  Applications of the impulse  B.27  System Properties  B.27.1  Linearity (additivity and homogeneity)  B.27.2  Time-invariance (or shift-invariance)  B.27.3  Memory  B.27.4  Causality  B.27.5  Invertibility  B.27.6  Stability  B.27.7  Properties of Linear and time-invariant (LTI) systems  B.28  Fixed and Floating-Point Number Representations  B.28.1  Representing numbers in fixed-point  B.28.2  IEEE 754 floating-point standard\n C Useful Softwares and Programming Tricks  C.1  Matlab and Octave  C.1.1  Octave Installation  C.2  Manipulating signals stored in files  C.2.1  Hex / Binary File Editors  C.2.2  ASCII Text Files: Unix/Linux versus Windows  C.2.3  Binary Files: Big versus Little-endian  C.2.4  Some Useful Code to Manipulate Files  C.2.5  Interpreting binary files with complex headers  C Glossary  1  Text Conventions  2  Main Abbreviations  3  Main Symbols"
  },
  {
    "objectID": "ak_dsp_bookli2.html",
    "href": "ak_dsp_bookli2.html",
    "title": "List of Figures",
    "section": "",
    "text": "List of Figures\n\n\n\n\n\n\n\n\n\n\nList of Figures\n\n\n\n\n\n1.1 Example of analog signal. Note the abscissa is continuous and the amplitude is not quantized, assuming an infinite number of possible values.1.2 Example of digital signal obtained by digitalizing the analog signal in Figure 1.11.3 Example of discrete-time signal1.4 Example of a discrete-time cosine generated with Listing 1.1.1.5 Representation of signals related by y[n] = x[−n]1.6 Examples of manipulating the independent variable: time-shift, contraction and dilation.1.7 Three examples of simultaneously scaling and shifting a signal x(t).1.8 Examples of manipulating the independent variable (in this case x) of a sinc function.1.9 Graphs of continuous and discrete-time signals obtained with the rect function in Listing 1.4.1.10 Graphs of continuous and discrete-time signals obtained with the rect function in Listing 1.4.1.11 Graph of the signal described in Eq. (1.7).1.12 Signals classification including the sampled signals.1.13 Example of sampled signal1.14 An impulse train with unitary areas and Ts = 125 μs.1.15 Example of a sampled signal obtained with a sampling frequency smaller than the required for accurately representing the original signal (shown in dotted lines). 1.16 Example of discrete-time cosines generated with sampling intervals of 125 μs (top) and 625 ns (bottom) to illustrate the better representation achieved by using a large oversampling factor.1.17 Complete process of A/D conversion with intermediate stages and four signals: analog, sampled, discrete-time and digital.1.18 Example of S/D conversion assuming Ts = 0.2 s.1.19 Example of D/S conversion assuming Ts = 0.2 s. It implements the inverse operation of Figure 1.18.1.20 Example of ZOH reconstruction using the signals of Example 1.13 with Ts = 0.2 s. In this case, x[n] = 0.5δ[n]−2.8δ[n−1]+1.3δ[n−2]+3.5δ[n−3]−1.7δ[n−4]+1.1δ[n−5])+4δ[n−6].1.21 Example of D/A of signal xq[n] = δ[n] − 3δ[n − 1] + 3δ[n − 2] (with quantized amplitudes) with D/S using Ts = 0.2 s\nand reconstruction using sinc functions.1.22 Identification of the individual scaled sinc functions (dashed lines) after D/S with Ts = 0.2 s and signal reconstruction of xq[n] = δ[n] − 3δ[n − 1] + 3δ[n − 2] in Figure 1.21.1.23 Signal reconstruction of a signal with quantized amplitudes using ZOH.1.24 Complete processing chain of an input analog signal x(t) to generate an output y(t) using DSP.1.25 Sampling and sinc-based perfect reconstruction of a cosine as implemented in function ak_sinc_reconstruction.m.1.26 Single sinc parcel from Figure 1.27 corresponding to the sinc centered at t = −0.8 s as dashed line and the reconstructed signal as a solid line.1.27 All individual parcels in dashed lines corresponding to each sinc and the summation as reconstructed signal (solid line) of Listing 1.8.1.28 Sampling and reconstruction of x(t) = sinc(t∕0.2) − 3sinc((t − 0.2)∕0.2) + 3sinc((t − 0.4)∕0.2) using Ts = 0.1 s.1.29 Sinc parcels used in the reconstruction of x(t) of Figure 1.28.1.30 Example of failing to reconstruct the signal x(t) = cos ⁡ (2π2.5t − 0.5π) using Fs = 2fmax = 5 Hz.1.31 Reconstruction of signal given by Eq. (1.7) fails because the sampling theorem is not obeyed.1.32 Sinc parcels used in the signal reconstructed as depicted in Figure 1.31.1.33 Versions of the Nyquist frequency in continuous and discrete-time.1.34 Input/output mapping for the quantizer specified by ℳ = {−4,−1,0,3}.1.35 Input/output relation of a 3-bits quantizer with  Δ 1.1.36 Input/output relation of a 3-bits quantizer with  Δ 0.5.1.37 Theoretical and estimated Gaussian probability density functions with thresholds represented by dashed lines and the output levels indicated with circles in the abscissa.1.38 Input/output mapping for the quantizer designed with a Gaussian input and outputs given by ℳ = [−6.8,−4.2,−2.4,−0.8,0.8,2.4,4.2,6.8].1.39 Results for the quantizer designed with the mixture of Eq. (1.30) as input.1.40 A quantizer Q is composed by the classification and decoding stages, denoted as Q~c and Q~d, respectively.1.41 Example of conversion using proportion when the dynamic ranges of both analog and digital signal are available1.42 Even and odd components of a signal x[n] representing a finite-duration segment of the step function u[n]. Note the symmetry properties: xe[n] = xe[−n]\nand xo[n] = −xo[−n].1.43 Even and odd components of a signal x[n] = n2u[n] representing a parabolic function.1.44 Even and odd components of a signal x[n] representing a triangle that starts at n = 21 and has its peak with an amplitude x[60] = 40 at n = 60. Note that the peak amplitude of the two components is 20.1.45 Waveform representation of a random signal with 100 samples draw from a Gaussian distribution N(0,1).1.46 Histogram of the signal in Figure 1.45 with 10 bins.1.47 PDF estimate from the histogram in Figure 1.461.48 Comparison of normalized histogram and the correct Gaussian N(4,0.09) when using 10,000 samples and 100 bins1.49 Graph of the signal x[n] = sin ⁡ (0.2n).1.50 Graph of the signal x[n] = sin ⁡ ((3π∕17)n).1.51 A sinusoid of period N=8 samples and its autocorrelation, which is also periodic each 8 lags1.52 The a) unbiased and b) raw (unscaled) autcorrelations for the sinusoid of Figure 1.51 with a new period of N=15 samples.1.53 Sinusoid of amplitude 4 V immersed in AWGN of power 25 W. The bottom graph is a zoom showing the first 100 samples.1.54 Autocorrelations of sine plus noise1.55 Continuous-time version of the AWGN channel model.1.56 Example of sound recorded at Fs = 44.1 kHz with the Audacity sound editor.1.57 Some Audacity options for saving an uncompressed WAVE file. The two non-linear PCMs are indicated.1.58 Cosine obtained with Listing 1.26 and a loopback cable connecting the soundboard DAC and ADC.1.59 Setup for loopback of the sound system using an audio cable.1.60 Example of options provided by Windows and the sound board. All the enhancements for both recording and playback devices should be disabled.1.61 Audacity window after reading in the ’impulses.wav’ file.1.62 Audacity window after simultaneously recording and playing ’impulses.wav’ with a loopback.1.63 Zoom of the response to the second impulse in Figure 1.61.1.64 Scatter plot of customer age versus purchased units for three products.1.65 Autocorrelation of the sunspot data.1.66 Autocorrelation of a cosine of 300 Hz1.67 First graph shows signals x(t) and y(t − 0.25) contaminated by AWGN at a SNR of 10 dB.1.68 Example of continuous-time signal.2.1 Rotation of a vector x by an angle 𝜃 = π∕2 radians using y = Ax with A given by Eq. (2.2).2.2 The first three (k = 0,1,2) and the last (k = 31)\nbasis functions for a 32-points DCT.2.3 The angles corresponding to WN on the unit circle, for N = 3,4,5,6.2.4 Computational cost of the DFT calculated via matrix multiplication versus an FFT algorithm. Note that N = 4096 are used in standards such as VDSL2, for example, and it is clearly unreasonable to use matrix multiplication.2.5 The first four (k = 0,1,2,3) and the last (k = 31) basis functions for a 32-points Haar.2.6 Basis functions k = 0,17 and the last four (k = 28,29,30,31) for a 32-points Haar.2.7 Fourier series basis functions for analyzing signals with period T0 = 1∕50 seconds. Because the basis functions are complex-valued signals, the plots show their real (top) and imaginary (bottom) parts.2.8 Spectrum of x(t) = 4 + 10cos ⁡ (2π50t + 2) + 4sin ⁡ (2π150t − 1).2.9 Unilateral spectrum of (real) signal x(t) = 4 + 10cos ⁡ (2π50t + 2) + 4sin ⁡ (2π150t − 1).2.10 DTFS / DFT of x[n] = 10cos ⁡ (π 6 n + π∕3) calculated with N = 12.2.11 Complete representation of the DTFS / DFT of x[n] = 10cos ⁡ (π 6 n + π∕3) indicating the periodicity X[k] = X[k + N].2.12 Spectrum X(f) (top) and X(ejΩ when Fs = 60 Hz.2.13 Real part of e(σ+j10π)t. The values of σ are 0.3 and -0.3 for the first (left) and second graphs, respectively.2.14 Magnitude (in dB) of Eq. (2.45).2.15 Phase (in rad) of Eq. (2.45).2.16 Graph of Figure 2.14 with the identification of the corresponding values of the Fourier transform (magnitude).2.17 The values of the magnitude of the Fourier transform corresponding to Figure 2.16.2.18 Two dimensional representation of Figure 2.17 obtained with the command freqs in Matlab/Octave showing the peak at ω = 2 rad/s due to the respective pole.2.19 Magnitude (in dB) of Eq. (2.52).2.20 Phase (in rad) of Eq. (2.52).2.21 Pole / zero diagram for Eq. (2.52).2.22 Graph of Figure 2.19 with the identification of the corresponding values of the DTFT (unit circle |z| = 1).2.23 The values of the magnitude of the DTFT corresponding to Figure 2.22.2.24 Magnitude (top) and phase (bottom) of the DTFT corresponding to Eq. (2.52). These plots can be obtained with the Matlab/Octave command freqz and are a more convenient representation than, e. g., Figure 2.23.2.25 Signal x[n] = δ[n − 11] analyzed by 32-points DCT and Haar transforms.2.26 A segment of one channel of\nthe original ECG data.2.27 Original and reconstructed ECG signals with DCT of N = 32 points and discarding 26 (high-frequency) coefficients.2.28 Performance of five DCT-based ECG coding schemes. The number of points is varied N ∈{4,8,32,64,128} and K = 1,2,…,M − 1.2.29 A zoom of the eye region of the Lenna image.2.30 Alternative representation of the DTFS / DFT of x[n] = 10cos ⁡ (π 6 n + π∕3) using fftshift.2.31 Five cosine signals xi[n] = 10cos ⁡ (Ωn) with frequencies Ω = 0, 2π∕32, 4π∕32, π, 31π∕16 for i = 0,1,2,16,32, and the real part of their DTFS using N = 32 points. 2.32 Analysis with DFT of 32 points of x[n] composed by three sinusoids and a DC level.2.33 Spectrum of a signal x[n] = 4cos ⁡ ((2π∕6)n) with period of 6 samples obtained with a 16-points DFT, which created spurious components.2.34 Explicitly repeating the block of N cosine samples from Figure 2.33 to indicate that spurious components are a manifest of the lack of a perfect cosine in time-domain.2.35 Three periods of each signal: pulse train x[n] with N = 10 and N1 = 5 and amplitude assumed to be in Volts (a), the magnitude (b) and phase (c) of its DTFS.2.36 Behavior when N1 of Figure 2.35 is decreased from N1 = 4 to 1.2.37 DTFT of an aperiodic pulse with N1 = 5 non-zero samples and DTFT estimates obtained via a DFT of N = 20 points.2.38 A version of Figure 2.37 using a DFT of N = 256 points.2.39 A version of Figure 2.37 using freqz with 512 points representing only the positive part of the spectrum.2.40 Reproducing the graphs generated by freqz in Figure 2.39.2.41 DFT with N = 8 points of a signal sampled at Fs = 100 kHz.3.1 Ideal magnitude specifications for lowpass and highpass filters.3.2 Magnitude of the frequency response of a practical analog filter in linear scale (|H(f)|, top) and in dB (20log ⁡ 10|H(f)|), which should be compared to the ideal case of Figure 3.1.3.3 Example of\nspecification masks for designing low and highpass filters.3.4 Example of specification mask for designing bandpass filters.3.5 Lowpass active analog filter designed with Texas Instruments’ FilterPro software.3.6 Frequency response of filters designed in Listing 3.1. The magnitude specification masks are indicated. Note that the phase was unwrapped with the command unwrap for better visualization.3.7 The canonical interface of a digital filter H(z) with the analog world via A/D and D/A processes.3.8 Comparison of two analog filters with its digital counterpart of Eq. (3.2). The “ideal” analog corresponds to Eq. (3.1) while the “10% tolerance” corresponds to its realization using the schematic of Figure 3.5.3.9 Diagram of systems, emphasizing the linear and time-invariant (LTI) systems and the systems described by linear, constant-coefficient differential (or difference) equations (LCCDE).3.10 Example of convolution between x[n] = 2δ[n] − 3δ[n − 1] and h[n] = δ[n] − 2δ[n − 1] + δ[n − 2].3.11 Convolution of a pulse p(t) = 4rect(5t − 0.1) with itself, obtained with Listing 3.7.3.12 Frequency response of H(ω) = 1 jω+2 represented in polar form: magnitude (top) and phase (bottom).3.13 Frequency response represented in polar form: magnitude (top) and phase (bottom).3.14 Version of Figure 3.13 obtained with the command freqz(1,[1 -0.7]).3.15 Spectrum Xs(f) of a sampled signal xs(t) obtained by the convolution between X(f) and P(f) as indicated in Eq. (3.18).3.16 Passband signal with BW = 25 Hz and center frequency fc = 70 Hz.3.17 Result of sampling X(f) in Figure 3.16 with Fs = 56 Hz.3.18 Sampling with Fs = 450 Hz a complex-valued signal with non-symmetrical spectrum.3.19 Result of converting x[n] with spectrum X(ejΩ into xs(t) with Xs(ω) = X(ejωTs) via a D/S conversion using Fs = 10 Hz.3.20 Extended version of Figure 1.24 using an arbitrary reconstruction filter h(t) and incorporating the filters A(s) and R(s).3.21 Reconstruction of a digital signal with BW = 25 kHz and Fs = 200 kHz using an analog filter with cutoff frequency fc = 25 kHz.3.22 Same as Figure 3.21, but for a signal with BW = fc = 80 kHz.3.23 Relations\nbetween natural frequency ωn, pole center frequency ω0, decay rate α and damping ratio ζ for a pair of complex conjugate poles.3.24 Magnitude of the frequency response for the SOS expressed by Eq. (3.30) and Eq. (3.33).3.25 Time-domain performance parameters for an underdamped system based on its step response.3.26 Bandwidth BW = f2 − f1 defined by the cutoff frequencies where the gain falls  − 3 dB below the reference value at f0.3.27 DTFT magnitude in dB of a 4-th order Butterworth filter with cutoff frequency of 50 Hz and the equivalent ideal filter with absolute bandwidth of 102.4 Hz.3.28 Effect of adding a linear phase e−j2πN0k∕N (N0 = 4 and N = 50) to the DTFS of x[n] resulting in a delayed version y[n] = x[n − 4] .3.29 Effect of adding the specified nonlinear phase (top) to the pulse in Figure 3.28, which leads to a distorted signal (bottom).3.30 Performance of a commercial SAW filter. The insertion gain IG(f) at two resolutions at the left plot and superimposed to the group delay at the right.3.31 Performance of a commercial ceramic filter.3.32 Comparison of analog filter specification (a) and two corresponding digital versions assuming Fs = 2000 Hz: (b) was obtained with ω = Ωs and (c) uses the convention adopted in Matlab/Octave.3.33 The choice of the sampling frequency Fs in the bilinear transformation depends on the application, and can be arbitrary in the case of digital filter design.3.34 Examples (a) and (b) of bilinear mappings from the unit circle in z to s plane. Mappings (c) and (d) of chosen points in s to z plane. Each example shows the points identified by numbers in their original and mapped planes.3.35 Version of Figure 1.33 in which the mapping between ω and  Ωses the bilinear transformation instead of the fundamental equation ω = Ωs of Eq. (1.22).3.36 Bilinear leads to a nonlinear warping between ω (rad/s) and  Ωrad). This example uses Fs = 0.5 Hz such that ω = tan ⁡ (Ω2).3.37 Relation\nimposed by the bilinear transform between ω (rad/s) and  Ωrad) for Fs = 100 Hz. In this case, the frequency ω0 = 540.4 rad/s is mapped to Ω = 2.433 ± k2π rad.3.38 |H(f)| corresponding to H(s) = 101(s − 1)(s − 1)∕[(s + 1)(s + 1 − j10)(s + 1 + j10)] of Eq. (3.74), to illustrate the bilinear transformation.3.39 Frequency responses from H(z) obtained via the bilinear transform of H(s) in Eq. (3.74) using Fs = 1, 3, 5 and 7 Hz.3.40 Magnitude (in dB) for bilinear transforms of H(s).3.41 Steps for using the bilinear transform to design H(s) and then convert to H(z). The set of requirements can be in discrete (Φ) or continuous-time (ϕ).3.42 Frequency responses of Hs(ωa) given by Eq. (3.76), Hz(ejΩ obtained via bilinear and Hz(ejωdTs) (from top to bottom).3.43 Version of Figure 1.33 and Figure 3.35 that considers both the bilinear transform and the conversion of continuous to discrete-time via ω = Ωs.3.44 Version of Figure 3.42 for which bilinear used pre-warping for obtaining ωa = ωd = 20 rad/s.3.45 Summary of scenarios of use for the bilinear transform when a continuous-time system function (or controller) H(s) is provided.3.46 Mask for a differentiator filter specified with the syntax for arbitrary filter magnitudes.3.47 Frequency response of filters obtained with firls.3.48 FIR design described in both time and frequency domains.3.49 Group delay and phase for a channel represented by a symmetric FIR with linear phase and constant group delay of 3 samples.3.50 Group delay and phase for a channel represented by a non-symmetric FIR h=[0.3 -0.4 0.5 0.8 -0.2 0.1 0.5] with non-linear phase.3.51 Impulse and frequency responses for the four types of symmetric FIR filters exemplified in Listing 3.30.3.52 Two distinct realizations of y[n] = 5x[n] − 5x[n − 1].3.53 Two structures for FIR realizations.3.54 Two alternatives for implementing the digital filter of Eq. (3.93).3.55 IIR of Eq. (3.93) implemented with the transposed direct form II. The intermediate diagram in (a) is obtained by transposing Figure 3.54(c), while (b) simply reorganizes it.3.56 Realization of Eq. (3.93) with transposed direct form II second order sections.3.57 The original magnitude response of the 8-th order filter and its 16-bits per coefficient Q7.8 quantized version (bi = 7 and\n\nbf = 8).3.58 Zeros and poles for the original and quantized filters discussed in Figure 3.57.3.59 Magnitude of quantized filter with Q10.15 using 26 bits (bi = 10 and bf = 15) and the original filter of order 8 in Figure 3.57.3.60 Magnitude responses for original filter of order 14 and its quantized version with b = 26 bits (bi = 10 and bf = 15).3.61 Zeros and poles for the corresponding filter in Figure 3.60. Note the occurrence of poles outside the unit circle, which make the quantized filter unstable.3.62 Example of filter outputs with floating-point double precision and fixed-point using Q2.3 generated with Listing 3.37.3.63 Original spectrum X(ejΩ (top) and its upsampled version Q(ejΩ = X(ejLΩ with L = 4 (bottom).3.64 Zoom of the bottom plot of Figure 3.63: due to the upsampling by L = 4, Q(ejΩ has four replicas of the original lowpass spectrum.3.65 Original spectrum Z(ejΩ (top) and the result Y (ejΩ (bottom) of downsampling it by M = 3.3.66 Screenshot of the DigitalFilter GUI after user informed the coefficients of the filter obtained with Matlab/Octave command [B,A]=butter(4,0.5).3.67 Result of experiment with Listing 3.41 and soundboards of two computers: an analog bandpass filter implemented with the canonical interface of Figure 3.7.3.68 Result similar to Figure 3.67 but without silence intervals in acquired signal (top plot).3.69 Frequency responses of Hnon(z) and its minimum-phase counterpart Hmin(z).3.70 Group delay for the non-minimum and minimum phase systems of Figure 3.69.3.71 Magnitudes of frequency responses for two resonators with H(s) as in Eq. (3.29).3.72 Poles (left) and magnitude |H(ω)| (right) for the sixth-order Butterworth filter of Listing 3.45.3.73 Similar to Figure 3.72, but with a sixth-order Chebyshev type 1 filter.3.74 Zero-pole plot (top) and magnitude in dB (bottom) for the sixth-order elliptic filter of Listing 3.45.3.75 Sound system phase frequency response ∠H(f) estimated from an impulse response.3.76 DSL line topology and corresponding impulse response3.77 Impulse response and group delay of IIR filter obtained with [B,A]=butter(8,0.3).3.78 Input signal\n\nx[n] and its FFT.3.79 Input x[n] and output y[n] signals obtained via convolution with truncated h[n].3.80 Input x[n] and output y[n] signals obtained via filter.3.81 Output y[n] aligned with the input x[n] and corresponding error x[n] − y[n].3.82 Filtering in blocks of N = 5 samples but not updating the filter’s memory.3.83 Linear phase FIR channel obtained with h=fir1(10,0.8).3.84 IIR channel obtained with [B,A]=butter(5,0.8).3.85 Spectrum of a band-limited even and real-valued signal x[n].4.1 Selected windows w[n] of duration N = 32 samples in time-domain.4.2 The DTFT W(ejΩ of the windows in Figure 4.1.4.3 The DTFT of windows in Figure 4.1 with their values normalized such that |W(ejΩ| = 1 for  Ω 0 rad.4.4 Comparison of spectra obtained with four windows in case both sinusoids are bin-centered (left plots) and not (right plots).4.5 Individual spectra of the two sinusoids superimposed obtained using the Kaiser window.4.6 Individual spectra of the two sinusoids superimposed, obtained using the rectangular window.4.7 PSD Sx(f) of a continuous-time white noise with N0∕2 = 3 W/Hz (top) and its discrete-time counterpart Sx(ejΩ obtained with Fs = 200 Hz (bottom). 4.8 Periodogram of x[n] = 10cos ⁡ ((2π∕64)n) in dBW/Hz estimated by periodogram.m with a 1024-point FFT and assuming Fs = 8 kHz.4.9 Discrete-time PSD of x[n] = 10cos ⁡ ((2π∕8)n) in linear scale estimated with periodograms.4.10 Periodogram and MS spectrum for a sum of sinusoids. Both are in dB scale.4.11 Periodograms of a white noise with power equal to 600 W estimated with N = 300 (top) and N = 3000 (bottom) samples.4.12 Periodograms of a cosine contaminated by AWGN at an SNR of  − 3 dB with an FFT of N = 1024 points (top plot) and 16384 (bottom). In this case the SNR cannot be inferred directly from the noise level.4.13 PSD of a filtered white noise x[n] estimated via the autocorrelation.4.14 PSD of a white noise\n\nx[n] with power equal to 600 W estimated by Welch’s method with M = 32 (top) and M = 256 (bottom) samples per segment.4.15 PSD of a filtered white noise x[n] estimated by Welch’s method.4.16 The prediction-error filter is A(z) = 1 −A~(z), where A~(z) provides a prediction y~[n] of the current n-th signal sample y[n], based on previous samples y[n − 1],…,y[n − P].4.17 PSDs estimated from a realization y[n] of a autoregressive process. The model adopted for the AR-based estimation matches the one used to generate y[n].4.18 PSDs estimated from a realization y[n] of a moving average process that does not match the model adopted for the AR-based estimation.4.19 PSD (top) and spectrogram (bottom) of a cosine that has its frequency increased from  Ω 2π∕30 to 2π∕7 and its power decreased by 20 dB at half of its duration.4.20 All twelve DTMF symbols: 1-9,*,0,#, each one composed by a sum of a low [697,770,852,941] and a high [1209,1336,1477] (Hz) frequencies.4.21 Example of narrowband spectrogam of a speech signal.4.22 Example of wideband spectrogam of a speech signal.4.23 DTFT magnitude of a cosine of frequency Ω = 1.7279 rad.4.24 DTFT and FFT magnitudes of a cosine of frequency Ω = 2.1206 rad.4.25 Relationship between roots location and spectrum for a rectangular window with N = 32 samples.4.26 Relationship between roots location and spectrum for a Kaiser window with N = 32 samples.4.27 DFT filter bank with rectangular window of N = 8 samples. The circles mark the DFT bin centers. The filter for k = 3 is emphasized.4.28 DFT filter bank with Kaiser window of N = 8 samples. The filter for k = 6 is emphasized.4.29 Sound system magnitude frequency response |H(f)| estimated from an impulse response.4.30 Sound system magnitude frequency response\n\n|H(f)| estimated from a white noise input.4.31 Spectrogram and tracks of the first four formant frequencies estimated via LPC for a speech sentence “We were away”.4.32 Eight DTMF symbols, each one with a 100 ms duration.B.1 Q function for three different SNR ranges.B.2 The perpendicular line for obtaining the projection pxy of a vector x onto y in ℝ2.B.3 Projections of a vector x and y onto each other.B.4 Scatter plot of the input data and the basis functions obtained via PCA and Gram-Schmidt orthonormalization.B.5 Scatter plots of two-dimensional Gaussian vector x (represented by x) and PCA transformed vectors y (represented by +)B.6 Scatter plots of two-dimensional Gaussian vector x (x) and Gram-Schmidt transformed vectors y (+).B.7 PMF for a dice result.B.8 Obtaining probability from a pdf (density function) requires integrating over a range.B.9 Example of a finite-duration random signal.B.10 Example of five realizations of a discrete-time random process.B.11 Example of evaluating a random process at time instants n = 4 and n = 6, which correspond to the values of two random variables X[4] and X[6].B.12 Example of a joint pdf of the continuous random variables X[4] and X[6].B.13 Correlation for data in matrix victories.B.14 Version of Figure B.13 using an image.B.15 Comparison between the 3-d representation of the autocorrelation matrix in Figure B.13 and the one using lags as in Eq. (B.72).B.16 Comparison between autocorrelation representations using images instead of 3-d graphs as in Figure B.15.B.17 Representation of a WSS autocorrelation matrix that depends only on the lag l.B.18 Suggested taxonomy of random processes.B.19 Correlation for random sequences with two equiprobable values:B.20 Alternative representation of the correlation values for the polar case of Figure B.19.B.21 One-dimensional ACF RX[l] for the data corresponding to Figure B.19 (unipolar and polar codes).B.22 ACF estimated using ergodicity and waveforms with 1,000 samples for each process.B.23 Functions used to characterize cyclostationary processes.B.24 Single realization\n\nx[n] of Eq. (B.81) (top) and the ensemble variance over time (bottom plot), which has a period of P∕2 = 15 samples.B.25 Cyclostationary analysis of the modulated white Gaussian noise.B.26 Realizations of polar signal mu[n] after upsampling by L = 4 samples.B.27 Autocorrelation of the cyclostationary mu[n] polar signal upsampled by L = 4.B.28 Autocorrelation of the cyclostationary mu[n] unipolar signal obtained with upsampling by L = 4.B.29 Realizations of an upsampled polar signal (L = 4) with random initial sample.B.30 ACF for the same polar process that generated the realizations in Figure B.29.B.31 Three realizations of the random process corresponding to upsampling by 2 and randomly shifting a sinusoid of period N = 4 and amplitude A = 4.B.32 Correlation of the WSS process corresponding to Figure B.31.B.33 Autocorrelation matrices for the a) WSS process obtained by phase randomization of the b) WSC process.B.34 Basic setup for measuring the insertion loss of a device under test (DUT).B.35 PDF for a dice roll result when the random variable is assumed to be continuous.B.36 A 3-d representation of the system H(z) = 3 − 2z−1.B.37 Fixed-point representation Q3.4 of the real number 5.0625. B.38 Comparison of step sizes for IEEE 754 floating points with single and double precisionC.1 Screenshot of the FileViewer software.C.2 Screenshot of the FileViewer dialog window that allows to convert between Linux and Windows text files.C.3 Screenshot of the FileViewer software showing the result of converting the Windows file of Figure C.1 to the Linux format.C.4 Interpretation of the file in Figure C.3 as short (2-bytes) big-endian elements.C.5 Contents in hexadecimal of file floatsamples.bin generated with Listing C.1.C.6 Interpretation as big-endian floats of file floatsamples.bin generated with Listing C.1.C.7 Contents interpreted as floats of big-endian file htk_file.bin generated with HTK."
  },
  {
    "objectID": "ak_dsp_bookli3.html",
    "href": "ak_dsp_bookli3.html",
    "title": "List of Tables",
    "section": "",
    "text": "List of Tables\n\n\n\n\n\n\n\n\n\n\nList of Tables\n\n\n\n\n\n1.1 Notation used for continuous and discrete-time signals. 1.2 New signals y1[n] = x[n − 2], y2[n] = x[−n + 3], y3[n] = x[2n] and y4[n] = x[n2], obtained by manipulating x[n] = t2(u[n − 3] − u[n − 7]). 1.3 Typical sampling frequencies.1.4 The notation and values of the Nyquist frequency in distinct domains.1.5 Input/output mapping for the quantizer specified by ℳ = {−4,−1,0,3} of Figure 1.34.1.6 Input/output mapping for a generic quantizer designed for a Gaussian input with variance σ2 = 10.1.7 Examples of binary numbering schemes used as output codes in A/D conversion for b = 3 bits.1.8 Total energy E and average power P for two kinds of signal assuming an infinite time interval.1.9 Autocorrelation functions and their respective equation numbers.1.10 Example of autocorrelation for a real signal [1,2,3] (n = 0,1,2). 1.11 Example of calculating the unscaled autocorrelation for a complex-valued signal [1 + j,2,3] (n = 0,1,2), where j = −1. 2.1 Examples of transforms and applications.2.2 Examples of inner product definitions.2.3 The four pair os equations for Fourier analysis with eternal sinusoids and the description of their spectra: ck, X(f) (or X(ω)), X[k] and X(ejΩ. For periodic continuous and discrete-time signals the periods are T0 and N0, respectively,\nwith fundamental (angular) frequencies ω0 = 2π∕T0 rad/s and Ω = 2π∕N0 rad. For continuous-time signals, one can alternatively use the linear frequency f instead of ω = 2πf, such that f0 = 1∕T0 is the fundamental frequency in Hz.2.4 Duality of periodicity and discreteness in Fourier analysis. 2.5 Units for each pair of Fourier equations in Table 2.3.2.6 Summary of equations useful for signal processing with FFT.3.1 Relations of the impulse response to the system function and frequency response of LTI systems.3.2 Some distinct options for the numerator of a SOS.3.3 Parameters of a second-order system as described by Eq. (3.30).3.4 Specifications of a commercial SAW filter.3.5 Specifications of a commercial ceramic filter where fn = 455 kHz is the nominal frequency and fc is the center of the 6-dB BW.3.6 Methods to convert H(s) into H(z).3.7 Pre-warped bilinear as a method to convert H(s) into H(z).3.8 Types of linear-phase FIR filters3.9 Matlab/Octave functions to convert among the formats: transfer function (tf), zero-pole (zp) and second order sections (sos)4.1 Difference in dB between the window main lobe and highest sidelobe amplitudes.4.2 LPC result for different orders P for a ramp signal with added noise.4.3 LPC result for different orders P for an AR(2) realization.B.1 Analogy between using the histogram and DFT for estimation, where ĝ(x[n]) is the estimated function and f^(x[n]) = κĝ(x[n]) its normalized version. The unit of f^(x[n]) is indicated within parentheses.1 Nomenclature of special frequencies."
  },
  {
    "objectID": "ak_dsp_bookli4.html",
    "href": "ak_dsp_bookli4.html",
    "title": "Listings",
    "section": "",
    "text": "Listings\n\n\n\n\n\n\n\n\n\n\nListings\n\n\n1.1 MatlabOctaveCodeSnippets/snip_signals_signal_generation.m1.2MatlabOctaveCodeSnippets/snip_signals_timereversal.m1.3MatlabOctaveCodeSnippets/snip_signals_sinc.m1.4MatlabOctaveCodeSnippets/snip_signals_rect.m1.5MatlabOctaveCodeSnippets/snip_signals_three_rects.m1.6MatlabOctaveCodeSnippets/snip_transforms_segmentation.m1.7MatlabOctaveCodeSnippets/snip_signals_oversampling.m1.8MatlabOctaveCodeSnippets/snip_signals_cosine_reconstruction.m1.9MatlabOctaveCodeSnippets/snip_signals_reconstruction_sinc.m1.10MatlabOctaveCodeSnippets/snip_signals_sampling_inequality.m1.11MatlabOctaveCodeSnippets/snip_signals_reconstruction_failure.m1.12MatlabOctaveCodeSnippets/snip_signals_sinusoid_generation.m1.13MatlabOctaveCodeSnippets/snip_signals_nonuniform_quantization.m1.14MatlabOctaveCodeSnippets/snip_signals_nonuniform_quant2.m1.15MatlabOctaveCodeSnippets/snip_signals_quantizer.m1.16MatlabOctaveFunctions/ak_quantizer.m1.17MatlabOctaveCodeSnippets/snip_signals_quantizer_use.m1.18MatlabOctaveCodeSnippets/snip_signals_estimate_pdf.m1.19MatlabOctaveCodeSnippets/snip_signals_gaussian_rand_gen.m1.20MatlabOctaveCodeSnippets/snip_signals_unscaled_autocorrelation.m1.21MatlabOctaveCodeSnippets/snip_signals_sinusoid_autocorrelation.m1.22MatlabOctaveCodeSnippets/snip_signals_noisy_sinusoid.m1.23MatlabOctaveCodeSnippets/snip_signals_wavread.m1.24MatlabOnly/snip_signals_recordblocking.m1.25MatlabOnly/snip_signals_wavwrite.m1.26MatlabOnly/snip_signals_realtimeLoopback.m1.27MatlabOnly/snip_signals_digitize_signals.m1.28MatlabOnly/snip_signals_realtimeWithDspSystem.m1.29MatlabOctaveCodeSnippets/snip_signals_inpulse_train.m1.30MatlabOctaveCodeSnippets/snip_signals_amplitude_normalization.m1.31MatlabOctaveCodeSnippets/snip_signals_peak_detection.m1.32MatlabOctaveCodeSnippets/snip_signals_fundamental_frequency.m1.33MatlabOctaveCodeSnippets/snip_signals_cross_correlation.m1.34MatlabOctaveCodeSnippets/snip_signals_time_delay.m1.35MatlabOctaveCodeSnippets/snip_signals_time_aligment.m1.36MatlabOctaveCodeSnippets/snip_signals_2Drandom.m2.1MatlabOctaveFunctions/ak_dctmtx.m2.2MatlabOctaveCodeSnippets/snip_transforms_DTFS.m2.3MatlabOctaveCodeSnippets/snip_transforms_laplace_basis.m2.4MatlabOctaveCodeSnippets/snip_transforms_granschmidt_debug.m2.5MatlabOnly/snip_transforms_ilaplace.m2.6MatlabBookFigures/figs_transforms_dctimagecoding2.7 MatlabOctaveBookExamples/ ex_transforms_check_orthogonality.m2.8MatlabOctaveCodeSnippets/snip_transforms_fftshift.m2.9MatlabOctaveCodeSnippets/snip_transforms_DTFS_sinusoid.m2.10MatlabOctaveCodeSnippets/snip_transforms_DTFS_pulses.m2.11MatlabOctaveCodeSnippets/snip_transforms_DTFT_pulse.m2.12MatlabOctaveCodeSnippets/snip_transforms_freqz.m2.13MatlabOctaveCodeSnippets/snip_transforms_KLT.m3.1MatlabBookFigures/figs_systems_elliptic.m3.2Digital filter implementation in pseudo-code.3.3MatlabOctaveFunctions/ak_convolution.m3.4MatlabOctaveFunctions/ak_convolution2.m3.5MatlabOctaveCodeSnippets/snip_systems_ak_convolution.m3.6MatlabOctaveCodeSnippets/snip_systems_convolution_correlation.m3.7MatlabOctaveCodeSnippets/snip_systems_continuous_discrete_conv3.8MatlabOctaveCodeSnippets/snip_systems_circularConvolution.m3.9MatlabOctaveCodeSnippets/snip_systems_overlapAdd.m3.10MatlabOctaveCodeSnippets/snip_systems_sos_parameters.m3.11Code/MatlabBookFigures/figs_digicomm_enbw3.12MatlabOctaveCodeSnippets/snip_systems_linear_phase_system.m3.13MatlabOnly/snip_systems_filter_conversion.m3.14MatlabOnly/snip_systems_lowpass_to_bandpass.m3.15MatlabOnly/snip_systems_bandstop_conversion.m3.16MatlabOctaveCodeSnippets/snip_systems_elliptic_filter_design.m3.17MatlabOctaveCodeSnippets/snip_systems_FIR_filter_design.m3.18MatlabOctaveCodeSnippets/snip_systems_notch.m3.19MatlabOctaveCodeSnippets/snip_systems_impulseinvariance3.20MatlabOctaveCodeSnippets/snip_systems_sos_bilinear.m3.21MatlabOctaveCodeSnippets/snip_systems_check_sosbilinear.m3.22MatlabOctaveCodeSnippets/snip_systems_bilinearmap.m3.23MatlabOctaveCodeSnippets/snip_systems_iir_bilinear.m3.24MatlabOctaveCodeSnippets/snip_systems_iir_elliptic.m3.25MatlabOctaveCodeSnippets/snip_systems_sampling.m3.26MatlabOctaveCodeSnippets/snip_systems_firls.m3.27MatlabOctaveCodeSnippets/snip_systems_rectangular_window.m3.28Group delay estimation for linar phase system3.29Code for plotting the group delay and phase.3.30MatlabOctaveCodeSnippets/snip_systems_FIR_types.m3.31MatlabOctaveCodeSnippets/snip_systems_residuez.m3.32MatlabOctaveCodeSnippets/snip_systems_SOS_decomposition.m3.33MatlabOctaveCodeSnippets/snip_systems_coefficient_quantization.m3.34MatlabOnly/snip_systems_Matlab_coef_quant.m3.35OctaveOnly/snip_systems_Octave_coef_quant.m3.36OctaveOnly/snip_systems_filtering_precision.m3.37MatlabBookFigures/figs_systems_roundoffErrors3.38MatlabOctaveThirdPartyFunctions/ak_universalChannelInitialization.m3.39MatlabOctaveThirdPartyFunctions/ak_universalChannel1.m3.40MatlabOctaveThirdPartyFunctions/ak_universalChannel2.m3.41MatlabOnly/snip_systems_realtimeLoopback.m3.42MatlabBookFigures/figs_systems_minphase3.43MatlabOctaveCodeSnippets/snip_systems_IIR_design.m3.44MatlabOctaveCodeSnippets/snip_systems_Qfactor.m3.45MatlabOctaveFunctions/ak_showQfactors.m3.46MatlabOctaveCodeSnippets/snip_systems_phase_estimation.m3.47MatlabOctaveCodeSnippets/snip_systems_filtering.m3.48MatlabOctaveCodeSnippets/snip_systems_compensate_grpdelay.m3.49MatlabOctaveCodeSnippets/snip_systems_filtering_blocks.m3.50MatlabOctaveCodeSnippets/snip_systems_matrixBlockConvolutions.m3.51MatlabOctaveCodeSnippets/snip_systems_buttord.m4.1MatlabOctaveCodeSnippets/snip_frequency_windows.m4.2MatlabOctaveCodeSnippets/snip_frequency_fftCosineExample.m4.3MatlabOctaveCodeSnippets/snip_frequency_sequence_generation.m4.4MatlabOctaveCodeSnippets/snip_frequency_analysis.m4.5MatlabOctaveCodeSnippets/snip_frequency_scalloping.m4.6MatlabOctaveCodeSnippets/snip_frequency_exp_esd.m4.7MatlabOctaveCodeSnippets/snip_frequency_mssFromPeriodogram.m4.8MatlabOctaveCodeSnippets/snip_frequency_comparePeriodograms.m4.9MatlabOctaveCodeSnippets/snip_frequency_correctPeriodograms.m4.10MatlabOctaveCodeSnippets/snip_frequency_normalized_periodogram.m4.11MatlabOctaveCodeSnippets/snip_frequency_periodogram.m4.12MatlabOctaveCodeSnippets/snip_frequency_not_bin_cent_cos.m4.13MatlabOctaveCodeSnippets/snip_frequency_noise_PSD.m4.14MatlabOctaveCodeSnippets/snip_frequency_noisy_cosine.m4.15MatlabOctaveCodeSnippets/snip_frequency_PSD_using_xcorr.m4.16MatlabOctaveCodeSnippets/snip_frequency_impulse_PSD.m4.17MatlabOctaveCodeSnippets/snip_frequency_pwelch.m4.18MatlabOctaveCodeSnippets/snip_frequency_filtered_noise_PSD.m4.19MatlabOctaveCodeSnippets/snip_frequency_aryule.m4.20MatlabOctaveCodeSnippets/snip_frequency_ma1.m4.21MatlabOctaveCodeSnippets/snip_frequency_lpcExample.m4.22MatlabOctaveCodeSnippets/snip_frequency_AR_PSD.m4.23MatlabOctaveCodeSnippets/snip_frequency_AR_continuousPSD.m4.24MatlabOctaveCodeSnippets/snip_frequency_PSD_estimation.m4.25MatlabOctaveCodeSnippets/snip_frequency_MA_process_PSD.m4.26MatlabOctaveCodeSnippets/snip_frequency_cosine_spectogram.m4.27MatlabOctaveCodeSnippets/snip_frequency_narrow_wide_spec.m4.28MatlabOctaveCodeSnippets/snip_frequency_msspectrum.m4.29MatlabOctaveCodeSnippets/snip_frequency_not_bin_cent_pwelch.m4.30MatlabOctaveCodeSnippets/snip_systems_smothing_FFT.m4.31MatlabOctaveCodeSnippets/snip_systems_recorded_noise.m4.32MatlabOctaveCodeSnippets/snip_frequency_formant_frequencies.m4.33MatlabOctaveFunctions/ak_spectralDistortion.m4.34Applications/SpeechAnalysis/spectralDistortion.m4.35MatlabOctaveCodeSnippets/snip_frequency_testHannDTFT.m4.36MatlabOctaveCodeSnippets/snip_frequency_Levinson_Durbin.mB.1MatlabOctaveCodeSnippets/snip_transforms_projection.mB.2MatlabOctaveCodeSnippets/snip_transforms_non_orthogonal_basis.mB.3MatlabOctaveCodeSnippets/snip_systems_pseudo_inverse.mB.4MatlabOctaveFunctions/ak_gram_schmidt.mB.5MatlabOctaveFunctions/ak_pcamtx.mB.6MatlabOctaveFunctions/ak_correlationEnsemble.mB.7MatlabOctaveFunctions/ak_convertToWSSCorrelation.mB.8MatlabOctaveCodeSnippets/snip_appprobability_modulatednoise.mB.9MatlabOctaveCodeSnippets/snip_appprobability_cyclo_analysis_ensemble.mB.10MatlabOctaveCodeSnippets/snip_appprobability_cyclo_analysis.mB.11MatlabOctaveCodeSnippets/snip_appprobability_cyclo_spectrum_periodogramsB.12MatlabOctaveCodeSnippets/snip_digi_comm_upsampled_autocorr.mB.13MatlabOctaveCodeSnippets/snip_app_correlationEnsemble.mB.14MatlabOnly/snip_appprediction_example.mB.15MatlabOctaveCodeSnippets/snip_appprediction_spatialLinearPredictionExample.mB.16MatlabOctaveCodeSnippets/snip_signals_data_precision.mB.17MatlabOctaveCodeSnippets/snip_signals_delta_calculation.mB.18MatlabOctaveCodeSnippets/snip_signals_numerical_error.mB.19MatlabOctaveCodeSnippets/snip_signals_single_precision.mC.1Java_Language/FileManipulation.java"
  },
  {
    "objectID": "ak_dsp_bookch1.html",
    "href": "ak_dsp_bookch1.html",
    "title": "Analog and Digital Signals",
    "section": "",
    "text": "1.1  To Learn in This Chapter  1.2  Analog, Digital and Discrete-Time Signals  1.2.1  Ambiguous notation: whole signal or single sample  1.2.2  Digitizing Signals  1.2.3  Discrete-time signals  1.3  Basic Signal Manipulation and Representation  1.3.1  Manipulating the independent variable  1.3.2  When the independent variable is not an integer  1.3.3  Frequently used manipulations of the independent variable  1.3.4  Using impulses to represent signals  1.3.5  Using step functions to help representing signals  1.3.6  The rect function  1.4  Block Processing  1.5  Complex-Valued and Sampled Signals  1.5.1  Complex-valued signals  1.5.2  Sampled signals  1.6  Modeling the Stages in A/D and D/A Processes  1.6.1  Modeling the sampling stage in A/D  1.6.2  Oversampling  1.6.3  Mathematically modeling the whole A/D process  1.6.4  Sampled to discrete-time (S/D) conversion  1.6.5  Continuous-time to discrete-time (C/D) conversion  1.6.6  Discrete-time to sampled (D/S) conversion  1.6.7  Reconstruction  1.6.8  Discrete-time to continuous-time (D/C) conversion  1.6.9  Analog to digital (A/D) and digital do analog (D/A) conversions  1.6.10  Sampling theorem  1.6.11  Different notations for S/D conversion  1.7  Relating Frequencies in Continuous, Sampled and Discrete-time Signals  1.7.1  Units of continuous-time and discrete-time angular frequencies  1.7.2  Mapping frequencies in continuous and discrete-time domains  1.7.3  Nyquist frequency  1.7.4  Frequency normalization in Matlab/Octave  1.8  An Introduction to Quantization  1.8.1  Quantization definitions  1.8.2  Implementation of a generic quantizer  1.8.3  Uniform quantization  1.8.4  Granular and overload regions  1.8.5  Design of uniform quantizers  1.8.6  Design of optimum non-uniform quantizers\n 1.8.7  Quantization stages: classification and decoding  1.8.8  Binary numbering schemes for quantization decoding  1.8.9  Quantization examples  1.9  Signal Categorization  1.9.1  Even and odd signals  1.9.2  Random signals and their generation  1.9.3  Periodic and aperiodic signals  1.9.4  Power and energy signals  1.10  Power and Energy in Discrete-Time  1.10.1  Power and energy of discrete-time signals  1.10.2  Power and energy of signals represented as vectors  1.10.3  Power and energy of vectors whose elements are not time-ordered  1.10.4  Power and energy of discrete-time random signals  1.11  Relating Power in Continuous and Discrete-Time  1.12  Correlation: Finding Trends  1.12.1  Autocorrelation function  1.12.2  Cross-correlation  1.13  A Linear Model for Quantization  1.14  Applications  1.15  Comments and Further Reading  1.16  Review Exercises  1.17  Exercises"
  },
  {
    "objectID": "ak_dsp_bookse1.html",
    "href": "ak_dsp_bookse1.html",
    "title": "1  To Learn in This Chapter",
    "section": "",
    "text": "Some of the topics discussed in this chapter are:\n\n &lt;ul class='itemize1'&gt;\n &lt;li class='itemize'&gt;Distinguish digital, analog, discrete-time and sampled signals\n &lt;/li&gt;\n &lt;li class='itemize'&gt;Represent signals using impulses and step functions\n &lt;/li&gt;\n &lt;li class='itemize'&gt;Relate sampled and discrete-time signals via S/D and D/S conversions\n &lt;/li&gt;\n &lt;li class='itemize'&gt;Models for the sampling and quantization processes\n &lt;/li&gt;\n &lt;li class='itemize'&gt;Review binary number systems used in digital signal processing\n                                                                          \n                                                                          \n &lt;/li&gt;\n &lt;li class='itemize'&gt;Calculate power, energy, fundamental period and correlation\n &lt;/li&gt;\n &lt;li class='itemize'&gt;Use correlation for detecting periodicity and aligning signals\n &lt;/li&gt;\n &lt;li class='itemize'&gt;Artificially generate signals (random and deterministic) for simulations\n &lt;/li&gt;\n &lt;li class='itemize'&gt;Analyze signals using Matlab/Octave\n &lt;/li&gt;\n &lt;li class='itemize'&gt;Use actual hardware to digitize signals\n &lt;/li&gt;\n &lt;li class='itemize'&gt;Read/write digitized signals from/to binary files&lt;/li&gt;&lt;/ul&gt;\n\n\nSpecific topics are organized as “Examples” along the text or “Applications” in the end of the chapter. These can be eventually skipped, but the reader is invited to stop just reading and explore these topics using a computer."
  },
  {
    "objectID": "ak_dsp_bookse2.html",
    "href": "ak_dsp_bookse2.html",
    "title": "2  Analog, Digital and Discrete-Time Signals",
    "section": "",
    "text": "In general, a signal is anything that can be interpreted as carrying useful information.\n\n\n\nFew examples can illustrate what information means in this context. For instance, monochromatic and color images are two-dimensional (2D) and 3D signals, respectively, in which the information are the pixel values. For instance, a color image X with resolution of 480 × 640 pixels can be represented in the RGB color space1 by a 3D matrix with dimension 480 × 640 × 3. A video is a sequence X[n] of images (each one called a frame) that are indexed by the integer n. While an image X does not depend on time, the value of n in the video X[n] is interpreted as the time instant. For example, a video X[n] with 500 frames can be represented by a 4D multidimensional array (also called tensor) of dimension 500 × 480 × 640 × 3, in which X = X[3] is the image (frame) with dimension 480 × 640 × 3 corresponding to time n = 3.\n\n\n\nAnother example of a signal is the electrocardiogram (ECG). Typically, the ECG is recorded with several channels and constitutes a multivariate or multidimensional signal x(t), where t is the time dimension. The information in the ECG corresponds to the amplitudes of each channel, which for a given t can be organized as a vector x. For instance, assuming six channels and a given time t0, the vector x = x(t0) contains the six amplitudes.\n\n\n\nThe provided examples illustrate that there are signals with very different characteristics. To be more concrete, unless otherwise stated, it is assumed hereafter a real-valued signal that describes how an amplitude varies over time. Hence, it is useful to classify signals according to the behavior of these two variables: the independent variable representing progress in time and the amplitude, which is the dependent variable. If time evolution is represented by a real-valued variable\n\nt ∈ ℝ, the function x(t) is called a continuous-time signal. If the progress over time is represented by an integer index n ∈ ℤ, the sequence x[n] is called a discrete-time signal.\n\n\n\n\n\n\n\nTable 1.1: Notation used for continuous and discrete-time signals. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContinuous-time, t ∈ℝ\n\n\n\n\n\nDiscrete-time, n ∈ℤ\n\n\n\n\n\nQuantized amplitude \n\n\n\n\n\nxq(t)\n\n\n\n\n\n\nxq[n] (digital)\n\n\n\n\n\nNot quantized \n\n\n\n\n\nx(t) (analog)\n\n\n\n\n\n\nx[n]\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1.1: Example of analog signal. Note the abscissa is continuous and the amplitude is not quantized, assuming an infinite number of possible values.\n\n\n\n\n\n\n\n\n\n\nFigure 1.2: Example of digital signal obtained by digitalizing the analog signal in Figure 1.1. Note both the abscissa (dimensionless indices) and amplitude of the digital signal are discrete. In this case, the quantized amplitudes assume M = 10 possible values from ℳ = {−3,−2,…,5,6}.\n\n\n\n\n\nSimilarly, the amplitude can freely assume any real value or be restricted to only pre-specified values from a finite set ℳ . In the latter case, the amplitude is said to be quantized. The quantized amplitudes can be eventually non-integer numbers. The number M of elements in ℳ , i. e., the cardinality M = |ℳ |, indicates whether the system is binary (M = 2), quaternary (M = 4) or M-ary. A subscript q will be used to denote a quantized signal, such as in xq(t).\n\n\n\nBased on the previous definitions, it is possible to define analog and digital signals, which are the most common signals in practice. A digital signal xq[n] is a discrete-time signal with quantized amplitudes. An analog signal x(t) is a continuous-time signal in which the amplitudes are not quantized (are not restricted to a finite number of M distinct values). Table 1.1 summarizes a useful taxonomy of signals. Figure 1.1 and Figure 1.2 provide examples of analog and digital signals, respectively.\n\n\n\nSignals that exist in the real-world are inherently analog. Even a DC power supply regulated to output 0 or 5 Volts will present a small random fluctuation due to circuit imperfections and noise. It could then be (strictly) classified as an analog signal x(t). But the circuits of a traditional digital system (e. g., a computer) can tolerate amplitude variations within a given range an, consequently, one often assumes the analog signal x(t) can be interpreted as a continuous-time signal with quantized amplitudes xq(t). It is also common to find authors calling xq(t) a “digital” signal, but this nomenclature will be avoided in this text.\n\n\n\nThe interfaces between digital systems and the analog world require analog to digital (A/D) and digital to analog (D/A) conversions, which will be discussed in Section 1.6.\n\n\n1.2.1  Ambiguous notation: whole signal or single sample\n\n\n\nIt should be noted that the notation x[n] (and x(t)) is ambiguous in the sense that it is widely used to represent both: a) the complete sequence and b) a sample at time n\n(or t). In most scenarios both interpretations are valid because if someone provides an equation for a given sample, such as\n\n\n\n\n\n\n\n\n\n x[n] = 3n, \n\n\n(1.1)\n\n\n\n\n\n\nwhich is valid for all n ∈ ℤ, this equation can be repeatedly used to reconstruct the whole sequence by varying n. In some cases, to disambiguate the two interpretations, a notation such as x[n0] or x(t0) is adopted, where n0 and t0 denote specific time instants instead of a generic variable that is used to represent the complete sequence.2\n\n\n\n\n\n1.2.2  Digitizing Signals\n\n\n\nIn many digital signal processing applications, it is required to convert a real-world analog signal into a digital format, and then process it with a computer, microcontroller, FPGA (field programmable gate array) or digital signal processor (DSP) chip, for example. Therefore, a brief review of the A/D process is discussed in the sequel.\n\n\n\nThe A/D converter (ADC) transforms the input analog signal x(t) into a digital signal xq[n], consisting of a sequence of quantized samples. The ADC executes two tasks:\n\n &lt;ul class='itemize1'&gt;\n &lt;li class='itemize'&gt;sampling: extract samples to accurately describe the signal;\n &lt;/li&gt;\n &lt;li class='itemize'&gt;quantization: represent each of these samples with a reasonable accuracy.&lt;/li&gt;&lt;/ul&gt;\n                                                                          \n                                                                          \n\n\nThe first task is called sampling and depends on the adopted sampling frequency Fs, which is the number of samples extracted from the signal per time unit (more specifically, one second). For example, Fs = 8000 Hz corresponds to obtaining 8000 samples to represent each second of signal. The higher Fs, the more accurate the representation tends to be.\n\n\n\nThe second task is called quantization and depends on the number b of bits used to represent each sample. For example, if b = 2, each sample can be represented by only N = 4 distinct values: 00, 01, 10 and 11. The mapping between these binary values and amplitudes is somehow arbitrary. For example, 00 can represent  − 5 V while 01 can represent  − 100 V. The b bits allow the ADC to output N = 2b distinct values. ADCs with large values for Fs and b are more expensive. Sometimes the tradeoff is to use a relatively large Fs with small b (e.g., 2.2 GHz with 10 bits) or vice-versa (e.g., 2.5 MHz with 24 bits).\n\n\n\nThe chip that performs the digital to analog conversion is called DAC. It also operates according to the values of Fs and b.\n\n\n\nNote that the operation performed by an ADC chip is in general lossy and, consequently, non-invertible.3 Therefore, cascading an ADC and a DAC chips recovers only an approximation of the original signal x(t). In this text we will learn how to properly choose Fs and b to control the A/D and D/A processes, in order to recover x(t) with the accuracy that the given application demands. These two parameters are the most relevant in the A/D and D/A processes, but when choosing commercial chips, there are many others (see exercises in Section 1.17). One important measurement to estimate this accuracy is the signal-to-noise ratio (SNR), which is the ratio between the power of x(t) and the power of a “noise” signal, in this case the error caused by the given processing stages. When this error is solely caused by quantization, the SNR is often called signal-to-quantization-noise ratio (SQNR).\n\n\n\nIn Section 1.14, some experiments are suggested for practicing the concepts of A/D\nconversion using a computer sound board. Before proceeding, it is recommended to try them.\n\n\n\n\n\n1.2.3  Discrete-time signals\n\n\n\nThe A/D conversion is typically performed by a single ADC chip. As mentioned, it is convenient to mathematically model this A/D conversion by splitting it in two stages: sampling and quantization, in spite of the actual electronics in the ADC not necessarily using them. One important reason for adopting these two stages when modeling the A/D conversion is that, while sampling is a linear operation, the input-output relation of quantizers is nonlinear (you may want to take a peek at Figure 1.35 and note that the relation corresponds to a non-linear stairs function). There are several tools, such as the Laplace and Fourier transforms discussed in Chapter 2 for dealing with linear operations. Working with nonlinear systems is more difficult. Because of that, most “digital” signal processing theory does not assume the amplitude is quantized to enable the usage of tools restricted to linear operations. Therefore, more strictly, most of the DSP theory could be called “discrete-time” signal processing. However, the name “digital signal processing” is more popular.\n\n\n\n\n\n\n\n\nFigure 1.3: Example of discrete-time signal x[n]. Note the abscissa is discrete, but the amplitude (ordinate) is not quantized.\n\n\n\n\n\nFigure 1.3 illustrates a discrete-time signal. Note the abscissa is given in discrete-time n ∈ ℕ, similar to the notation adopted for digital signals. But for a discrete-time signal x[n], the amplitude can assume any value. This figure should be compared with Figure 1.1 and Figure 1.2, observing each axis.\n\n\n\n Example 1.1. Example of creating a discrete-time signal from a continuous-time sinusoid. Consider extracting samples from the analog signal x(t) = 6cos ⁡ (2π400t) with amplitude 6 Volts and frequency fc = 400 Hz. The goal is to use Fs = 8,000 samples to represent each segment of one second, where Fs is called sampling frequency. The time interval between consecutive samples is Ts = 1∕Fs, which in this case is Ts = 1∕8,000 = 125 μs. Listing 1.1 illustrates one possible implementation.\n\n\n\nListing 1.1: MatlabOctaveCodeSnippets/snip_signals_signal_generation.m\n\n\nFs=8000; %sampling frequency (Hz) \nTs=1/Fs; %sampling interval (seconds) \nf0=400; %cosine frequency (Hz) \nN=100; %number of desired samples \n5n=0:N-1; %generate discrete-time abscissa \nt=n*Ts; %discretized continuous-time axis (sec.) \nx=6*cos(2*pi*f0*t); %amplitude=6 V and frequency = f0 Hz \nstem(n,x); %plot discrete-time signal\n  \n\n\n\n\n\n\n\nFigure 1.4: Example of a discrete-time cosine generated with Listing 1.1.\n\n\n\n\n\nFigure 1.4 indicates the result of executing Listing 1.1. This cosine has a period of T = 1∕fc = 1∕400 = 2.5 ms. Therefore, each of its period is being represented by T∕Ts = (2.5 × 10−3)∕(125 × 10−6) = 20 samples. Note in Figure 1.4 that, in this specific case, at each sample n = 20,40,60,…, the sample x[0] = 6 at n = 0 is repeated, and a new cosine cicle starts.   □\n\n\n \n\n1 RGB stands for the colors red, green and blue.\n\n \n\n2 This notation will be adopted, for instance, in Eq. (1.3).\n\n \n\n3 A lossless operation from x to y, allows recovering x^ from y, such that x^ = x. If the operation is lossy, then x^ = x + e, where e is a non-zero error.\n\n                                                                                                        &lt;/div&gt;"
  },
  {
    "objectID": "ak_dsp_bookse3.html",
    "href": "ak_dsp_bookse3.html",
    "title": "3  Basic Signal Manipulation and Representation",
    "section": "",
    "text": "This section discusses basic tools for mathematically representing and manipulating signals. The motivation is that it is important not only being knowledgeable on manipulating signals using software, but also algebraically, developing expressions and “theoretical” analysis.\n\n\n\n\n\n1.3.1  Manipulating the independent variable\n\n\n\nMany signal processing tasks require manipulating t (for continuous-time signals) or n (for discrete-time). A convenient way to introduce this manipulation is by using examples. Define the signal x[n] = n2 for n = 3,4,5 and 6, and zero otherwise. This signal has only four non-zero sample values [9,16,25,36] at n = 3,4,5 and 6, respectively. The task here is to find new signals based on x[n], namely: y1[n] = x[n − 2], y2[n] = x[−n + 3], y3[n] = x[2n] and y4[n] = x[n2].\n\n\n\nAn interpretation of y1[n] = x[n − 2] is that it is a sequence of events that is related to the original (“mother”) sequence x[n] but with a time difference. For example, if n = 6 is interpreted as 6 o’clock and x[6] = 36\nis the heart rate of an animal at that moment, the same measurement y1[8] = 36 is found at y1[n] but two time units later (8 o’clock). Hence, the notation y1[n] = x[n − 2] indicates that x[n] and y1[n] present the same ordinate values, but these values occur two samples later in y1[n] when compared to x[n].\n\n\n\nOne can create a mapping such as Table 1.2, which fills up the first column with values of n in the region of interest. Then, it is simple to get columns for n − 2,  − n + 3, 2n and n2. Columns y1[n],y2[n],y3[n] and y4[n] are filled up based on the auxiliary columns n − 2,  − n + 3, 2n and n2, but they correspond to amplitudes at the value of n given in column 1. For example, the second row indicates that y1[−2] = 0,y2[−2] = 25,y3[−2] = 0 and y4[−2] = 16.\n\n\n\n\n\n\n\nTable 1.2: New signals y1[n] = x[n − 2], y2[n] = x[−n + 3], y3[n] = x[2n] and y4[n] = x[n2], obtained by manipulating x[n] = t2(u[n − 3] − u[n − 7]). \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nn\n\n\n\n\n\nx[n]\n\n\n\n\n\nn−2\n\n\n\n\n\ny1[n]\n\n\n\n\n\n−n + 3\n\n\n\n\n\ny2[n]\n\n\n\n\n\n2n\n\n\n\n\n\ny3[n]\n\n\n\n\n\nn2\n\n\n\n\n\ny4[n]\n\n\n\n\n\n\n\n\n−3\n\n\n\n\n\n\n0\n\n\n\n\n\n\n−5\n\n\n\n\n\n\n0\n\n\n\n\n\n\n6\n\n\n\n\n\n\n36\n\n\n\n\n\n\n−6\n\n\n\n\n\n\n0\n\n\n\n\n\n\n9\n\n\n\n\n\n\n0\n\n\n\n\n\n\n\n\n−2\n\n\n\n\n\n\n0\n\n\n\n\n\n\n−4\n\n\n\n\n\n\n0\n\n\n\n\n\n\n5\n\n\n\n\n\n\n25\n\n\n\n\n\n\n−4\n\n\n\n\n\n\n0\n\n\n\n\n\n\n4\n\n\n\n\n\n\n16\n\n\n\n\n\n\n\n\n−1\n\n\n\n\n\n\n0\n\n\n\n\n\n\n−3\n\n\n\n\n\n\n0\n\n\n\n\n\n\n4\n\n\n\n\n\n\n16\n\n\n\n\n\n\n−2\n\n\n\n\n\n\n0\n\n\n\n\n\n\n1\n\n\n\n\n\n\n0\n\n\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\n\n\n\n\n−2\n\n\n\n\n\n\n0\n\n\n\n\n\n\n3\n\n\n\n\n\n\n9\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\n\n\n\n\n\n\n1\n\n\n\n\n\n\n0\n\n\n\n\n\n\n−1\n\n\n\n\n\n\n0\n\n\n\n\n\n\n2\n\n\n\n\n\n\n0\n\n\n\n\n\n\n2\n\n\n\n\n\n\n0\n\n\n\n\n\n\n1\n\n\n\n\n\n\n0\n\n\n\n\n\n\n\n\n2\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\n\n\n\n\n1\n\n\n\n\n\n\n0\n\n\n\n\n\n\n4\n\n\n\n\n\n\n16\n\n\n\n\n\n\n4\n\n\n\n\n\n\n16\n\n\n\n\n\n\n\n\n3\n\n\n\n\n\n\n9\n\n\n\n\n\n\n1\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\n\n\n\n\n0\n\n\n\n\n\n\n6\n\n\n\n\n\n\n36\n\n\n\n\n\n\n9\n\n\n\n\n\n\n0\n\n\n\n\n\n\n\n\n4\n\n\n\n\n\n\n16\n\n\n\n\n\n\n2\n\n\n\n\n\n\n0\n\n\n\n\n\n\n−1\n\n\n\n\n\n\n0\n\n\n\n\n\n\n8\n\n\n\n\n\n\n0\n\n\n\n\n\n\n16\n\n\n\n\n\n\n0\n\n\n\n\n\n\n\n\n5\n\n\n\n\n\n\n25\n\n\n\n\n\n\n3\n\n\n\n\n\n\n9\n\n\n\n\n\n\n−2\n\n\n\n\n\n\n0\n\n\n\n\n\n\n10\n\n\n\n\n\n\n0\n\n\n\n\n\n\n25\n\n\n\n\n\n\n0\n\n\n\n\n\n\n\n\n6\n\n\n\n\n\n\n36\n\n\n\n\n\n\n4\n\n\n\n\n\n\n16\n\n\n\n\n\n\n−3\n\n\n\n\n\n\n0\n\n\n\n\n\n\n12\n\n\n\n\n\n\n0\n\n\n\n\n\n\n36\n\n\n\n\n\n\n0\n\n\n\n\n\n\n\n\n7\n\n\n\n\n\n\n0\n\n\n\n\n\n\n5\n\n\n\n\n\n\n25\n\n\n\n\n\n\n−4\n\n\n\n\n\n\n0\n\n\n\n\n\n\n14\n\n\n\n\n\n\n0\n\n\n\n\n\n\n49\n\n\n\n\n\n\n0\n\n\n\n\n\n\n\n\n8\n\n\n\n\n\n\n0\n\n\n\n\n\n\n6\n\n\n\n\n\n\n36\n\n\n\n\n\n\n−5\n\n\n\n\n\n\n0\n\n\n\n\n\n\n16\n\n\n\n\n\n\n0\n\n\n\n\n\n\n64\n\n\n\n\n\n\n0\n\n\n\n\n\n\n\n\n9\n\n\n\n\n\n\n0\n\n\n\n\n\n\n7\n\n\n\n\n\n\n0\n\n\n\n\n\n\n−6\n\n\n\n\n\n\n0\n\n\n\n\n\n\n18\n\n\n\n\n\n\n0\n\n\n\n\n\n\n81\n\n\n\n\n\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Example 1.2. Time-reversal of signals. The operation y[n] = x[−n] corresponds to flipping the signal over the ordinate axis. In Matlab/Octave it can be implemented with flipr (left-right) or flipud (up-down) for row and column vectors, respectively. For example, assume that x[n] = 3δ[n] + 4δ[n − 2] + 5δ[n − 4]. One can represent x[n] as the row vector x=[3 0 4 0 5] and flip it using y=fliplr(x). It is important to note that, when representing a signal, one vector can store the amplitude values but not their respective time instants. An additional vector is required to store the time information. Listing 1.2 is a snippet (part of the script) used to generate Figure 1.5. It illustrates the care that must be exercised to properly represent the signals y[n] = x[−n] using a computer program. Note that the user must relate the amplitude vector and the “time” vector.\n\n\n\nListing 1.2: MatlabOctaveCodeSnippets/snip_signals_timereversal.m\n\n\nx=[3 0 4 0 5]; %some signal samples \ny=fliplr(x); %time-reversal \nn1=0:4; %the 'time' axis \nn2=-4:0; %the 'time-reversed' axis \n5subplot(211); stem(n1,x); title('x[n]'); \nsubplot(212); stem(n2,y); title('y[n]');\n  \n\n\n\n\n\n\n\nFigure 1.5: Representation of signals related by y[n] = x[−n], where x[n] = 3δ[n] + 4δ[n − 2] + 5δ[n − 4].\n\n\n\n\n\nFigure 1.5 was produced using three dots to indicate that the signals have infinite duration in spite of being represented by finite-length vectors. Another convention is that, when an amplitude value is not explicitly shown, it is assumed to be zero.    □\n\n\n1.3.2  When the independent variable is not an integer\n\n\n\nAlmost all manipulations of t are equally applied to manipulating n of discrete-time signals. But a discrete-time signal x[n] is undefined if n∉ℤ (e. g. x[1.5]).\n\n\n\nFor instance, x[2n] behaves slightly different than x(2t) given that the discrete-time signal is not defined for non-integer n = ±0.5,±1.5,±2.5,…. Therefore, only the amplitude values of x[n] for n even are present in x[2n], while the values for n odd are discarded.\n\n\n\nUsing a similar reasoning, when dealing with a discrete-time signal y[n] = x[n∕2], it is important to adopt a definition such as:\n\n\n\n\n\n\n\n\n\n y[n] =   {  x[n∕2],when n∕2 is an integer 0,   otherwise   \n\n\n(1.2)\n\n\n\n\n\n\nthat emphasizes the assumption n ∈ ℤ.\n\n\n\n\n\n1.3.3  Frequently used manipulations of the independent variable\n\n\n\nIn signal processing, one is often manipulating the dependent variable (for example, multiplying the signal amplitude by two) and also the independent variable, which is the time abscissa in most cases in this text. These two ways of manipulating a variable are rather different. This section discusses manipulating the independent variable.\n\n\n\nOne can always use a procedure such as the one illustrated in Table 1.2 to obtain the signal after a manipulation of the independent variable. However, it is worth to memorize the resulting signals in two common situations. The first one is the time shift and the second situation is when simultaneously scaling and shifting the original signal. They are discussed in the next paragraphs.\n\n\n\n Example 1.3. Time advance and delay rules of thumb.\n\n\n\nIt may be convenient to memorize the time advance and delay rules. Assuming t0 &gt; 0, these rules are (t and t0 are assumed, but the same applies to discrete-time n and n0):\n\n &lt;ul class='itemize1'&gt;\n &lt;li class='itemize'&gt;&lt;!-- l. 243 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt; &lt;mo class='MathClass-bin' stretchy='false'&gt;−&lt;/mo&gt; &lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\n is a delayed version (right-shifted) of &lt;!-- l. 243 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\n and &lt;!-- l. 243 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt; &lt;mo class='MathClass-bin' stretchy='false'&gt;+&lt;/mo&gt; &lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\n is an anticipated version (left-shifted) of &lt;!-- l. 243 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;;\n &lt;/li&gt;\n &lt;li class='itemize'&gt;&lt;!-- l. 244 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mo class='MathClass-bin' stretchy='false'&gt;−&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt; &lt;mo class='MathClass-bin' stretchy='false'&gt;+&lt;/mo&gt; &lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\n is obtained by first considering &lt;!-- l. 244 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt; &lt;mo class='MathClass-bin' stretchy='false'&gt;+&lt;/mo&gt; &lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;,\n which is obtained by advancing the signal by &lt;!-- l. 244 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;,\n and then flipping the result &lt;!-- l. 245 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt; &lt;mo class='MathClass-bin' stretchy='false'&gt;+&lt;/mo&gt; &lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\n with respect to the &lt;!-- l. 245 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;/math&gt;-axis\n to obtain &lt;!-- l. 245 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mo class='MathClass-bin' stretchy='false'&gt;−&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt; &lt;mo class='MathClass-bin' stretchy='false'&gt;+&lt;/mo&gt; &lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\n &lt;/li&gt;\n &lt;li class='itemize'&gt;&lt;!-- l. 246 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mo class='MathClass-bin' stretchy='false'&gt;−&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt; &lt;mo class='MathClass-bin' stretchy='false'&gt;−&lt;/mo&gt; &lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\n is also obtained by delaying &lt;!-- l. 246 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\n to create &lt;!-- l. 246 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt; &lt;mo class='MathClass-bin' stretchy='false'&gt;−&lt;/mo&gt; &lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\n and then flipping this intermediate result over the &lt;!-- l. 246 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;/math&gt;-axis.&lt;/li&gt;&lt;/ul&gt;\n\n\nFor example, x(−t + 3) corresponds to\nfinding the temporary result x(t + 3) by advancing 3 units of time and then flipping x(t + 3) over the y-axis. Alternatively, one can think of obtaining x(−t + 3) by first flipping x(t) with respect to the y-axis and then delaying x(−t) by 3 (instead of anticipating it). However, a common mistake is to think that x(−t + 3) = x(−(t − 3)), and start by delaying x(t) by 3 and then flipping over the vertical axis. In case of any confusion, the safest option is to map some values of the abscissa such as Table 1.2.    □\n\n\n\nFigure 1.6 provides the example y(t) = x(t − 1) of time-shift, corresponding to a delay of 1. This figure also contrasts manipulating the independent and dependent variables. In the latter case, it is the ordinate (y-axis) that is modified, with y(t) = 2x(t) changing the y-axis peak value from 8 to 16. Besides, Figure 1.6 illustrates other two manipulations: contraction and dilation. Note that all three examples of manipulating the independent variable did not modify the ordinate peak value (it is equal to 8). Contraction and dilation are the topic of the next example.\n\n\n\n\n\n\n\n\nFigure 1.6: Examples of manipulating the independent variable: time-shift, contraction and dilation.\n\n\n\n\n\n Example 1.4. Time scaling rules of thumb: contraction and dilation.\n\n &lt;ul class='itemize1'&gt;\n &lt;li class='itemize'&gt;Assuming that &lt;!-- l. 267 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;mi&gt;α&lt;/mi&gt; &lt;mo class='MathClass-rel' stretchy='false'&gt;&gt;&lt;/mo&gt; &lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/math&gt;,\n &lt;!-- l. 267 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mi mathvariant='italic'&gt;αt&lt;/mi&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\n corresponds to contracting the time axis by a factor of &lt;!-- l. 268 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mi&gt;α&lt;/mi&gt;&lt;/math&gt;\n while &lt;!-- l. 268 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo class='MathClass-bin' stretchy='false'&gt;∕&lt;/mo&gt;&lt;mi&gt;α&lt;/mi&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\n corresponds to a dilation by &lt;!-- l. 268 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mi&gt;α&lt;/mi&gt;&lt;/math&gt;.&lt;/li&gt;&lt;/ul&gt;\n\n\nFor instance, a pulse x(t) with support4 T1 leads to x(2t) and x(t∕5) with supports T1∕2 and 5T1, respectively. In Figure 1.6, x(t) has a support of 6, which starts at t = 3, while x(3t) and x(t∕2) have support 2 and 12, respectively.   □\n\n\n\nAnother important manipulation of the independent variable is the simultaneous combination of time-shift and scaling, discussed next.\n\n\n\n Example 1.5. Simultaneous scale and shift rules of thumb. The signal x(αt + β), with α,β ∈ ℝ, is commonly found in signal processing operations. It can also be found as x((t + γ)∕ξ), where γ,ξ ∈ ℝ. These two representations are related by α = 1∕ξ and β = γ∕ξ. The rules of thumb are:\n\n &lt;ul class='itemize1'&gt;\n &lt;li class='itemize'&gt;for &lt;!-- l. 285 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt; &lt;mo class='MathClass-rel' stretchy='false'&gt;=&lt;/mo&gt; &lt;mi&gt;x&lt;/mi&gt; &lt;mrow&gt;&lt;mo fence='true' form='prefix'&gt; (&lt;/mo&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo class='MathClass-bin' stretchy='false'&gt;+&lt;/mo&gt;&lt;mi&gt;γ&lt;/mi&gt;&lt;/mrow&gt;\nξ ), first expand or contract x(t) by a factor of ξ and then shift the intermediate result by γ;\n\n &lt;li class='itemize'&gt;for &lt;!-- l. 286 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt; &lt;mo class='MathClass-rel' stretchy='false'&gt;=&lt;/mo&gt; &lt;mi&gt;x&lt;/mi&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mi mathvariant='italic'&gt;αt&lt;/mi&gt; &lt;mo class='MathClass-bin' stretchy='false'&gt;+&lt;/mo&gt; &lt;mi&gt;β&lt;/mi&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;,\n first expand or contract the original signal &lt;!-- l. 286 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\n according to the value of &lt;!-- l. 287 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mi&gt;α&lt;/mi&gt;&lt;/math&gt;,\n then shift this intermediate result by &lt;!-- l. 287 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;mi&gt;β&lt;/mi&gt;&lt;mo class='MathClass-bin' stretchy='false'&gt;∕&lt;/mo&gt;&lt;mi&gt;α&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;.&lt;/li&gt;&lt;/ul&gt;\n\n\nFigure 1.7 provides two examples of simultaneously scaling and shifting a signal x(t).    □\n\n\n\nThe first example in Figure 1.7 aims at interpreting y1(t) = x  (t−3 2 ). One can always carefully map the signals as done in Table 1.2. But using the rule of thumb is faster. In this case, x(t) can be expanded by a factor of ξ = 2 and the result shifted to the right (delayed) by γ = −3. Note that the support of x(t) is 6, in the range t ∈ [4,10]. The support of y1(t) is t ∈ [11,23], twice the support of x(t). As expected, the amplitude does not change: for both signals, it has a maximum value of 5.\n\n\n\n\n\n\n\n\nFigure 1.7: Three examples of simultaneously scaling and shifting a signal x(t).\n\n\n\n\n\nThe second task suggested in Figure 1.7 is to obtain y2(t) = x(2t + 6). One possibility is to convert the representation x(αt + β) into x((t + γ)∕ξ) and proceed as done for y1(t). In this case, y2(t) = x(2t + 6) =  (t+3 0.5 ). To obtain y2(t), x(t) can be scaled (in this case a contraction) by a factor of ξ = 0.5 and the result shifted to the left by γ = 3. The support of y2(t) is t ∈ [−1,2].\n\n\n\nInstead of using x((t + γ)∕ξ), one can directly operate on x(αt + β), but noting that the time-shift is β∕α instead of β. In other words, it is an error to try to obtain x(αt + β) by first finding x(αt) and then shifting by β. For instance, y2(t) = x(2t + 6) in Figure 1.7 can be quickly obtained by noting that α = 2 made the support of x(2t) to be t ∈ [2,5] (half of the support of x(t)), and the time advance β∕α = 6∕2 = 3 shifted x(2t) to the left.\n\n\n\nAs a final example that incorporates three manipulations, consider the case of y3(t) = x(−4t + 6) in Figure 1.7. The signal y3(t) can be obtained by first finding x(4t) (a new signal with support from t = 1 to 2.5), then anticipating x(4t) by β∕α = 6∕4 = 1.5 (that creates an intermediate signal with support from t = −0.5 to 1) and then flipping x(4t + 6) with respect to the y-axis (as in x(−t)) to obtain the final result with support from t = −1\nto 0.5.\n\n\n\nFigure 1.8 provides two examples of manipulating the independent variable of the sinc function, which is defined in Appendix B.12. The sinc function is zero when the abscissa x is an integer number x ∈ ℤ, with the exception of x = 0. Hence, for x &gt; 0, the first zero of y1(x) = sinc(x∕4) is at x = 4. The first zero of the signal y2(t) = sinc(3x − 6) for x &gt; 0 can be found using 3x − 6 = 1, which leads to x = 7∕3 ≈ 2.33, as indicated in Figure 1.8. The contraction and time-shift of sinc(t) is widely used as part of the D/A conversion, as will be discussed in Eq. (3.19).\n\n\n\n\n\n\n\n\nFigure 1.8: Examples of manipulating the independent variable (in this case x) of a sinc function.\n\n\n\n\n\n Example 1.6. Generating graphs of of the sinc function. The function sinc in Matlab/Octavecan be used to plot a generic sinc with amplitude A, support ξ and centered at γ seconds. The command x = A*sinc((t-gamma)/xi), where t is an array of time instants, provides the amplitude values for the signal\n\n\n\n\n\n\n x(t) = A sinc  (t − γ ξ ). \n\n\n\n\n\n\nListing 1.3: MatlabOctaveCodeSnippets/snip_signals_sinc.m\n\n\nTs = 0.001; %sampling interval (in seconds) \nxi = 0.2; %support of the sinc in seconds \ngamma = 0.5; %time shift \nt = -1.6:Ts:1.6; %define discrete-time abscissa with fine resolution \n5x = 3*sinc((t-gamma)/xi); %define the signal x(t) \nplot(t,x);\n  \n\n\n\n\n\n\n\nFigure 1.9: Graphs of continuous and discrete-time signals obtained with the rect function in Listing 1.4.\n\n\n\n\n\nListing 1.3 provides an example of using sinc to create a graph of a continuous-time signal sinc, which is depicted in Figure 1.9. The continuous-time sinc in Listing 1.3 corresponds to:\n\n\n\n\n\n\n x(t) = 3 sinc  (t − 0.5 0.2 ), \n\n\n\n\n\n\nand the chosen sampling interval was Ts = 0.001 s.   □\n\n\n1.3.4  Using impulses to represent signals\n\n\n\nIf you are not familiar with impulses, please first check Appendix B.26.\n\n\n\nRecall that the notation δ(t − t0) indicates that a continuous-time impulse occurs at time t = t0. For example, δ(t − 3) is a delayed impulse occurring at t = 3 and δ(t + 2) is an anticipated impulse occurring at t = −2. A similar reasoning applies to the discrete-time δ[n − n0].\n\n\n\nAny discrete-time signal can be conveniently represented by a sum of scaled and shifted impulses. To get the basic idea, consider the following example. An array [3.0,−1.3,2.1,0,4.2] stores the only non-zero samples of a signal x[n], with the first element corresponding to time n = 0, the second one to n = 1 and so on. How would you write an expression for x[n] without using impulses? A wrong guess would be x[n] = 3.0 − 1.3 + 2.1 + 0 + 4.2 because that corresponds to having a constant DC value of x[n] = 8,∀ ⁡n. The impulse helps locating the desired amplitude in time. In this example, given that the first sample occurs at\n\nn = 0, an expression for x[n] using impulses is x[n] = 3.0δ[n] − 1.3δ[n − 1] + 2.1δ[n − 2] + 4.2δ[n − 4]. Generalizing, any discrete-time signal can be written as\n\n\n\n\n\n\n\n\n\n x[n] = ∑ n0=−∞∞x[n 0]δ[n − n0]. \n\n\n(1.3)\n\n\n\n\n\n\nIn our example, x[n0] assumes the values 3.0,−1.3,2.1,0,4.2 for n0 = 0,…,4, respectively. As explained, writing x[n] = x[0] + x[1] + x[2] + x[3] + x[4] does not lead to the proper result because shifted impulses δ[n − n0] are required to specify the corresponding locations of the amplitudes as they vary over time.\n\n\n\n Example 1.7. Revisiting the notation for signals. To complement the discussion in Section 1.2.1, consider interpreting Eq. (1.3) as a single sample at a specific “time” n = n1. To make that clear, one can write:\n\n\n\n\n\n\n\n\n\n x[n1] = ∑ n0=−∞∞x[n 0]δ[n1 − n0] \n\n\n(1.4)\n\n\n\n\n\n\nand interpret that δ[n1 − n0] = 1 if n1 = n0 and zero otherwise. Hence, the summation eliminates all amplitudes of x[n] other than\n\nx[n1], which is the amplitude of x[n] at n1. In this interpretation, Eq. (1.3) and Eq. (1.4) represent a single sample. Alternatively, one can interpret Eq. (1.3) as the whole sequence. Understanding both interpretations is also useful when dealing with continuous-time impulses as discussed in the next paragraph.    □\n\n\n\nThe continuous-time version of Eq. (1.3) is\n\n\n\n\n\n\n\n\n\n x(t) = ∫ −∞∞x(τ)δ(τ − t)dτ, \n\n\n(1.5)\n\n\n\n\n\n\nwhich uses the sifting property of impulses (see Appendix B.26) to show that any continuous-time signal x(t) can be represented as a composition of impulses. Besides, similar to to its discrete-time version (Eq. (1.3)), Eq. (1.5) can be interpreted as representing the whole signal or a single sample value at time t.\n\n\n\n\n\n1.3.5  Using step functions to help representing signals\n\n\n\nThe continuous-time step function u(t) is defined as 1 for t &gt; 0 and 0 for t &lt; 0. There is a discontinuity at t = 0 and the amplitude is conveniently assumed to be u(t)|t=0 = 0.5. The discrete-time version u[n] does not have discontinuities and is defined as 1 for n ≥ 0 and 0 for n &lt; 0. The step function is very useful to indicate signals that are zero for negative values of the independent variable t or n. Another use is to describe finite-duration signals. When a signal\n(x[n] or x(t)) has finite support, it is called a finite-duration signal.\n\n\n\nIt is easy to represent finite-duration signals using a programming language (Python, Java, C, etc.) or an environment such as Matlab or Octave. The step function can be used with the same goal when dealing with expressions. Say for example that the signal x[n] has only four non-zero samples [9,16,25,36] at n = 3,4,5 and 6. This signal could be written as x[n] = n2(u[n − 3] − u[n − 7]).\n\n\n\nA side note is that u(t) can simplify integrals by limiting the integration interval. For example, ∫ −∞∞x(t)dt when x(t) = e−2tu(t) can be obtained by\n\n\n\n\n\n\n\n\n\n ∫ −∞∞e−2tu(t)dt = ∫ 0∞e−2tdt, \n\n\n(1.6)\n\n\n\n\n\n\nwhere the role played by u(t) is absorbed by changing the inferior limit to 0. After this step, u(t) can be eliminated from the integral. In other words, when u(t) appears in integrals it can be often taken care of by simply adjusting the integral limits.5\n\n\n\n\n\n1.3.6  The rect function\n\n\n\nThe rectangular or rect function corresponds to a normalized pulse with unitary support from t = −0.5 to 0.5 and amplitude equals to 1. It can be written as rect(t) = u(t + 0.5) − u(t − 0.5) and in some textbooks it is denoted as Π(t).\nThe rect(t) is defined here as a continuous-time function, but there are other definitions, for rectangular discrete-time signals.\n\n\n\nAs an example of a signal derived from rect(t), consider a pulse x(t) with amplitude A = 4 Volts and support from 5 to 8 seconds can be denoted as x(t) = 4rect((t − 6.5)∕3). In this case, as explained in Example 1.5, the value 6.5 is obtained by observing that the intermediate result rect(t∕3) has support (or width) from t = −1.5 to 1.5, and this range must be delayed by 6.5 to obtain the new support as t = 5 to 8 s.\n\n\n\n Example 1.8. Generating graphs of continuous and discrete-time signals with the rect function. The function rectpuls in Matlab/Octavecan be used to plot a generic pulse with amplitude A, support of T and centered at tc seconds. The command x = A*rectpuls(t-tc,T), where t is an array of time instants, provides the amplitude values for the signal\n\n\n\n\n\n\n x(t) = A rect  (t − tc T ). \n\n\n\n\n\n\nListing 1.4: MatlabOctaveCodeSnippets/snip_signals_rect.m\n\n\n%% Mimicking a continuous-time signal by using plot \nTs = 0.001; %sampling interval in seconds \nt = -0.8:Ts:0.8; %discrete-time in seconds \npulse_width = 0.2; %support of this rect in seconds \n5tc = 0.4; %center (in seconds) of this rect \nA = 2.5; %amplitude (in Volts) of this rect \nx = A*rectpuls(t-tc,pulse_width); \nsubplot(211), plot(t,x) %plot as a continuos-time signal \nxlabel('t'), ylabel('x(t)') \n10%% Making explicit the signal is discrete-time by using stem \nn = -10:10; %discrete-time in seconds \npulse_width = 5; %support of this rect in samples \nnc = -3; %center (in samples) of this rect \nA = 7; %amplitude (in Volts) of this rect \n15x = A*rectpuls(n-nc,pulse_width); \nsubplot(212), stem(n,x) %plot as a discrete-time signal \nxlabel('n'), ylabel('x[n]')\n  \n\n\n\n\n\n\n\nFigure 1.10: Graphs of continuous and discrete-time signals obtained with the rect function in Listing 1.4.\n\n\n\n\n\nListing 1.4 provides examples of using rectpuls to create graphs of discrete and continuous-time signals, which are depicted in Figure 1.10. The continuous-time rect in Listing 1.4 corresponds to:\n\n\n\n\n\n\n x(t) = 2.5 rect  (t − 0.4 0.2 ). \n\n\n\n\n\n\nOne important trick to get a graph that resembles a pulse is to use a small sampling interval, as done in Listing 1.4.\n\n\n\nIn spite of rect(t) being defined in continuous-time, it is possible to use it to obtain a discrete-time signal as done, e. g., in Listing 1.4. Defining the sampling interval as Ts = 1 second, allows to create a discretized time axis n composed only by integers, and then generate\n\n\n\n\n\n\n x(t) = x[n] = 7 rect  (n + 3 5 ), \n\n\n\n\n\n\nwith the key assumption of Ts = 1.   □\n\n\n\n Example 1.9. Generating a signal composed by several rect functions. The goal in this example is to generate the signal\n\n\n\n\n\n\n\n\n\n x(t) = rect  ( t 0.2 ) − 3 rect  (t − 0.2 0.2 ) + 3 rect  (t − 0.4 0.2 ) \n\n\n(1.7)\n\n\n\n\n\n\nusing rectpuls in Matlab/Octave. Listing 1.5 provides a solution, and generates Figure 1.11.\n\n\n\nListing 1.5: MatlabOctaveCodeSnippets/snip_signals_three_rects.m\n\n\n%% Mimicking a continuous-time signal by using plot \nTs = 0.001; %sampling interval in seconds \nt = -1.6:Ts:1.6; %discrete-time in seconds \npulse_width = 0.2; %support of this rect in seconds \n5x = rectpuls(t,pulse_width) - 3*rectpuls(t-0.2,pulse_width) + ... \n    3*rectpuls(t-0.4,pulse_width); %define the signal \nplot(t,x) %plot as a continuos-time signal \nxlabel('t'), ylabel('x(t)')\n  \n\n\n\n\n\n\n\nFigure 1.11: Graph of the signal described in Eq. (1.7).\n\n\n\n\n\nListing 1.5 illustrates a general procedure: one can define a common time axis t and compose a sophisticated signal x(t) by summing the appropriate parcels.   □\n\n\n \n\n4 The support X of a function f(x) is the set of values x ∈X such that f(x)≠0.\n\n \n\n5 There is no need for using, e. g., the integration by parts of Eq. (B.27) to solve Eq. (1.6).\n\n    &lt;/div&gt;"
  },
  {
    "objectID": "ak_dsp_bookse4.html",
    "href": "ak_dsp_bookse4.html",
    "title": "4  Block Processing",
    "section": "",
    "text": "Many algorithms segment the signal into blocks of samples and process each block individually. For example, when writing Matlab/Octave code for DSP, it is tempting to organize the information in a single vector, which is then processed and converted into another vector. This works well when the number of elements in each vector is relatively small. However, many situations require processing billions of samples, and the corresponding long vectors would eventually not fit in memory. Hence, in many cases the best strategy is to write DSP code using block-processing.\n\n\n\nWhen processing a discrete-time “infinite” duration signal x[n] in blocks (also called frames), x[n] is iteratively segmented and represented by finite dimensional vectors xm with N elements each. Assuming that x[n] is a right-sided signal (n = 0 corresponds to the first non-zero sample, n = 1 to the second and so on), the first block x0 is formed by the samples\n\n\n\n\n\n\n x0 = [x[0],x[1],…,x[N − 2],x[N − 1]]T . \n\n\n\n\n\n\nIn general, the m-th block is represented by the column vector\n\n\n\n\n\n\n\n\n\n xm = [x[mN],x[mN + 1],…,x[mN + N − 2],x[mN + N − 1]]T . \n\n\n(1.8)\n\n\n\n\n\n\nTo avoid too many indexes, the dependence on m will be omitted hereafter.\n\n\n\n Example 1.10. Segmenting a signal into blocks. Suppose the task is to segment a very long signal into blocks and then calculate the power of each block. Listing 1.6 illustrates how to perform this task in Matlab/Octave.\n\n\n\nListing 1.6: MatlabOctaveCodeSnippets/snip_transforms_segmentation.m\n\n\nS=10000; %some arbitrary number of samples \nx=rand(1,S); %create some very very long vector \nN=50; %number of samples per frame (or block) \nM=floor(S/N); %number of blocks, floor may discard last block samples \n5powerPerBlock=zeros(M,1); %pre-allocate space \nfor m=0:M-1 %following the book convention \n    beginIndex = m*N+1; %index of the m-th block start \n    endIndex = beginIndex+N-1; %m-th block end index \n    xm=x(beginIndex:endIndex) %the samples of m-th block \n10    powerPerBlock(m+1)=mean(abs(xm).^2); %estimate power of block xm \nend \nclf; subplot(211); plot(x); subplot(212); plot(powerPerBlock) %plot\n  \n\n\nNote that Eq. (1.8) assumes the first index is 0, which is adopted in Python, C, Java and other languages. However, in Matlab/Octave the first index is 1 and this required using beginIndex = mN+1 instead of beginIndex = mN as in Eq. (1.8).\n\n\n\nThe reader is invited to use a speech signal instead of the random signal and observe how the signal power varies over time, comparing it for vowels and consonants.    □"
  },
  {
    "objectID": "ak_dsp_bookse5.html",
    "href": "ak_dsp_bookse5.html",
    "title": "5  Complex-Valued and Sampled Signals",
    "section": "",
    "text": "Some signals are useful for developing theoretical models, even if they do not exist in practice. This section describes two important mathematical representations for signals.\n\n\n\n\n\n1.5.1  Complex-valued signals\n\n\n\nThe definition of a signal x(t) (or x[n]) can be expanded to include complex-valued amplitudes x(t) ∈ ℂ. In several situations, it is mathematically convenient to use a single complex-valued signal to represent two (typically related) real-valued (or simply real) signals. For instance, the complex-valued exponential z(t) = ejwct = cos ⁡ (wct) + jsin ⁡ (wct) (see Euler’s Eq. (B.1)) plays important role in Fourier analysis and simplifies the notation that, otherwise, would rely on two signals: cos ⁡ (wct) and sin ⁡ (wct).\n\n\n\nA complex-valued signal x(t) = xre(t) + jxim(t) can be interpreted as representing two distinct signals, corresponding to the real xre(t) and imaginary xim(t) components. This representation is widely used in digital communications to compose quadrature signals. In quadrature processing, the component xre(t) is called in-phase and denoted as xi(t) while xim(t) is called the quadrature component and denoted as xq(t). The quadrature xquadrature(t) signal is real-valued and can be obtained via\n\n\n\n\n\n\n\n\n\n xquadrature(t) = xi(t)cos ⁡ (ωct) + xq(t)sin ⁡ (ωct), \n\n\n(1.9)\n\n\n\n\n\n\nwhere ωc is a carrier frequency in radians per second. The name quadrature comes from the fact that the sine is a cosine delayed by 90 degrees.\n\n\n\nAn alternative way of implementing Eq. (1.9) is by defining the complex-valued signal xce(t) = xi(t) − jxq(t) and using the compact notation\n\n\n\n\n\n\n\n\n\n xquadrature(t) = Real{xce(t)ejωct}, \n\n\n(1.10)\n\n\n\n\n\n\nwhere Real{⋅} denotes the real part of a complex-valued signal.\n\n\n\nLater (for instance, after transmitting the quadrature signal through a communication channel), the components xi(t) and xq(t) can be recovered from xquadrature(t). The systems that recover these components typically use quadrature sampling, in which two ADCs deal with the two components xi(t) and xq(t) that may be interpreted as composing a complex-valued signal xce(t).\n\n\n\n\n\n1.5.2  Sampled signals\n\n\n\nIn a sampled signal  xs(t), the information is represented by uniformly spaced impulses with distinct areas. Because the impulses have undefined (infinite) amplitude values, xs(t) will not be considered here a continuous-time signal, but an intermediate representation between\nthe continuous and discrete-time worlds as indicated in Figure 1.12. Signals other than impulses (e. g., pulses) can be used for sampling, but an impulse train is the only option discussed here due to its mathematical convenience. Assuming impulses, the sampled signal is not physically realizable but it is very useful for mathematically modeling the sampling step of the A/D conversion, as will be discussed in the next paragraphs.\n\n\n\n\n\n\n\n\nFigure 1.12: Signals classification including the sampled signals.\n\n\n\n\n\n\n\n\n\n\nFigure 1.13: Example of a sampled signal obtained from Figure 1.1. Note the abscissa is continuous but the amplitudes of the impulses go to infinite. Conventionally, the impulse height is used to indicate the impulse area and the contour created by the arrow heads resembles the original analog signal.\n\n\n\n\n\nIn this text, a sampled signal xs(t) is assumed to be the result of periodic sampling a continuous-time signal x(t). Hence, the sampled signal is composed by periodically spaced impulses whose areas correspond to the value of the original analog signal x(t) at the impulse location. There is a convention for showing plots of sampled signals with the impulse heights corresponding to their areas (see Appendix B.26), but the infinite amplitude of xs(t) is considered  ±∞ at the impulses positions. The amplitude is zero if t is not a multiple of Ts, i. e., x(t) = 0,∀ ⁡t≠kTs, where k ∈ ℤ is any integer.6\n\n\n\nFigure 1.13 presents an example of a sampled signal. It can be compared with Figure 1.1 and Figure 1.2. Figure 1.13 was obtained assuming the time interval between consecutive impulses is Ts = 125 μs (or, equivalently, that Fs = 1∕Ts = 8 kHz). Note also that the sampled signal xs(t) in Figure 1.13 exists for all t, while a discrete-time signal x[n] is considered to exist only for n ∈ ℤ. Another distinction is that t in xs(t) (as well as in x(t)) has dimension of time (assumed to be in seconds) while n in x[n] is dimensionless.\n\n\n \n\n6 A signal z(t) that contain impulses δ(t) but is not obtained via periodic sampling, will not be denoted with the subscript s nor considered a sampled signal.\n\n                                                                                         &lt;/div&gt;"
  },
  {
    "objectID": "ak_dsp_bookse6.html",
    "href": "ak_dsp_bookse6.html",
    "title": "6  Modeling the Stages in A/D and D/A Processes",
    "section": "",
    "text": "This section discusses mathematical models that are used to conveniently represent in a high-level, the individual signal processing stages in the A/D and D/A processes.\n\n\n\nDifferent electronic circuits can be used when designing an ADC chip to perform the A/D process such as successive approximation and sigma-delta. If needed, the signal processing stages used in each of these techniques can be modeled with details. For instance, a detailed description may be necessary in case the goal is to optimize the electronic circuits to be more robust to impairments and noise. In the scope of this text, these details are not important and the A/D process is considered ideal. A similar approach is adopted for the D/A process.\n\n\n\n\n\n1.6.1  Modeling the sampling stage in A/D\n\n\n\nThe goal here is to extract samples to represent an analog signal x(t). In periodic (or uniform) sampling of a continuous-time function x(t) with a sampling interval Ts, the sampling frequency is\n\n\n\n\n\n\n\n\n\n Fs = 1 Ts. \n\n\n(1.11)\n\n\n\n\n\n\nFor example, digital telephony adopts Fs = 8000 Hz, which corresponds to Ts = 125 μs.\n\n\n\n\n\n\n\n\nFigure 1.14: An impulse train with unitary areas and Ts = 125 μs.\n\n\n\n\n\nIn the A/D context, periodic sampling can be modeled as the multiplication of x(t) by a periodic impulse train p(t). Figure 1.14 depicts the infinite duration impulse train for sampling at 8 kHz. As conventionally done, the impulses heights indicate their areas. The sampled signal is denoted as\n\n\n\n\n\n\n xs(t) = x(t)p(t) = ∑ k=−∞∞x(kT s)δ(t − kTs), \n\n\n\n\n\n\nwhere x(kTs) is the impulse area in xs(t), which coincides with the amplitude of x(t) at t = kTs. The amplitude of xs(t) is not defined when t = kTs and zero otherwise. The notation xsarea(t) will sometimes be used to emphasize we mean the area of the impulse at time t.\n\n\n\n Example 1.11. Example of periodic sampling. Consider sampling the analog signal in Figure 1.1 via multiplying it by the train of impulses in Figure 1.14. Using the impulse sifting property (see Section B.26.5.0) leads to the sampled signal in Figure 1.13, which is depicted with the original analog signal superimposed.\n\n\n\nIn this case, the signal segment x(t) in Figure 1.1 has D = 6.125 ms of duration. Given that Fs = 8 kHz, this segment is represented by N = 50 samples. To understand how N is calculated in this case, observe that xs(t) starts and ends with samples, such that it contains N − 1 intervals of duration Ts = 1∕Fs with a total duration D = (N − 1)Ts = (50 − 1)(1∕8000) = 6.125 ms.\n\n\n\nAn alternative representation would be to assume that there are N intervals of duration Ts, such that xs(t) would end after an interval of Ts seconds after its last impulse. One can find both representations in the literature and associated software.   □\n\n\n\nFigure 1.13 shows that, in this case, the sampled signal xs(t) represents very well the original analog signal x(t), in the sense that the envelope of the impulse areas correspond to a good match with respect to the original signal. This is not the case in the following example. Consider now that the sampling interval is increased by a factor of 4 to Ts = 500μs. Figure 1.15 indicates that in this case xs(t) cannot represent well, for example, the variation between the samples indicated in the figure.\n\n\n\n\n\n\n\n\nFigure 1.15: Example of a sampled signal obtained with a sampling frequency smaller than the required for accurately representing the original signal (shown in dotted lines). \n\n\n\n\n\nIn terms of computational cost, it is often convenient to use the smallest possible value of Fs. But choosing Fs too small may lead to a sampled signal that does not represent well the original signal as noted in Figure 1.15. Section 1.6.10 discusses a theorem that informs the minimum sampling rate for perfect reconstruction of a signal x(t) from its samples x[n] given the maximum frequency of x(t).\n\n\n1.6.2  Oversampling\n\n\n\nWhen the sampling rate Fs is larger than the minimum required sampling rate, the signal is considered to be oversampled. Oversampling an analog signal may facilitate the consequent digital signal processing, especially when real-time is not a requirement. For instance, consider that a signal could be sampled with 8 kHz, but Fs = 32 kHz is adopted. If real-time processing is required, instead of processing 8,000 samples per second, now the system must process 32,000. In this case, the oversampling factor would be L = 32,000∕8,000 = 4. Oversampling by a factor of L, can also be interpreted as decreasing the sampling interval Ts = 1∕Fs by L. For the given example, the interval 1∕8,000 = 125 μs between neighbor samples is reduced to 31.25 μs.\n\n\n\nIn spite of the potential increase in computational cost of oversampling, it may be the case that higher accuracy can be obtained or eventually simpler algorithms adopted. It is very common to observe commercial DSP systems adopting L = 2,4 or larger.\n\n\n\nWhen simulating DSP in computers, a very large L can be adopted to generate graphs of oversampled discrete-time signals that look like continuous-time signals.\n\n\n\n Example 1.12. Mimicking continuous-time using discrete-time signals with a large oversampling factor. When representing a signal via a vector, one is implicitly dealing with a discrete-time signal. But even if a signal x[n] is representing the samples\nof an analog signal x(t), sometimes it is desirable to generate graphs that resemble x(t). In such situations, one alternative is to use a large oversampling factor, as previously used in Listing 1.4. Listing 1.7 provides another example.\n\n\n\nListing 1.7: MatlabOctaveCodeSnippets/snip_signals_oversampling.m\n\n\nFs=8000; %sampling frequency (Hz) \nTs=1/Fs; %sampling interval (seconds) \nf0=2400; %cosine frequency (Hz) \nN=40; %number of desired samples \n5n=0:N-1; %generate discrete-time abscissa \nt=n*Ts; %discretized continuous-time axis (sec.) \nx=6*cos(2*pi*f0*t); %amplitude=6 V and frequency = f0 Hz \nsubplot(211), plot(t,x); %plot discrete-time signal \n%% Create an oversampled version of signal x \n10oversampling_factor = 200; \noversampled_Ts = Ts/oversampling_factor; \noversampled_n = n(1)*oversampling_factor:n(end)*oversampling_factor; \noversampled_t=oversampled_n*oversampled_Ts;%oversampled discrete-time \nxo=6*cos(2*pi*f0*oversampled_t); %oversampled discrete-time signal \n15subplot(212), plot(oversampled_t,xo);\n  \n\n\n\n\n\n\n\nFigure 1.16: Example of discrete-time cosines generated with sampling intervals of 125 μs (top) and 625 ns (bottom) to illustrate the better representation achieved by using a large oversampling factor.\n\n\n\n\n\nListing 1.7 generates x with a sampling interval Ts = 1∕8,000 = 125 μs. It also creates an oversampled version xo using a new sampling interval of Ts∕200 = 625 ns. Instead of using stem in Matlab/Octave, both graphs were generated with plot, assuming the goal is to depict a continuous-time signal. However, as can be seen in Figure 1.16, xo corresponds to a coarse representation of a cosine that uses N = 40 samples (length of x), while xo is a much better one using No = 7801 samples.   □\n\n\n\nThere are several ways to create versions with different sample rates from a given signal x(t). In the specific case of Listing 1.7, the signals x and xo were created such that their first and last samples correspond to the same time instant, t = 0 and t = 4.875 ms, respectively. In this case, the number No of samples in the oversampled xo is given by\n\n\n\n\n\n\n\n\n\n No = L(N − 1) + 1, \n\n\n(1.12)\n\n\n\n\n\n\nwhere N is the number of samples in x and L is the oversampling factor. This equation can be understood by considering that xo has all N samples of x plus the extra L − 1 samples between each of the N − 1 consecutive pairs of samples of x. This leads to No = N + (L − 1)(N − 1), which can be rewritten as Eq. (1.12).\n\n\n\nWhen Eq. (1.12) is used for the signals in Listing 1.7, it leads to No = 7801 samples, as can be confirmed by the command length(x).\n\n\n1.6.3  Mathematically modeling the whole A/D process\n\n\n\nAccording to the convention adopted in this text, the following four signals appear during the A/D process along the stages of sampling (SAMPLING) and quantization (QUANTIZATION): analog, sampled, discrete-time and digital. Therefore, the A/D process is conveniently represented via the block diagram in Figure 1.17.\n\n\n\n\n\n\n\n\nFigure 1.17: Complete process of A/D conversion with intermediate stages and four signals: analog, sampled, discrete-time and digital.\n\n\n\n\n\nFigure 1.17 indicates that the sampled signals xs(t) is converted into the discrete-time signal x[n] via the sampled to discrete-time (S/D) conversion. The sampling and S/D stages can be combined into the continuous to discrete-time (C/D) conversion. We first discuss the S/D conversion and then the C/D.\n\n\n1.6.4  Sampled to discrete-time (S/D) conversion\n\n\n\nThe S/D conversion represents an intermediate stage in the periodic sampling process and is always associated to a given sampling interval Ts. The actual signal processing performed by the electronic circuits of an ADC do not generate impulses or sampled signals, but the S/D conversion is a convenient mathematical model for the ideal periodic sampling. For example, if xs(t) = 3δ(t + 2Ts) − 1δ(t + Ts) + 8δ(t) − 5δ(t − 3Ts), the S/D conversion outputs x[n] = 3δ[n + 2] − 1δ[n + 1] + 8δ[n] − 5δ[n − 3].\n\n\n\nRecall that xs(t) exists for all values of t while x[n] is a discrete time signal with n ∈ ℤ. Another mathematical detail is that the non-zero amplitudes of xs(t) are  ±∞, but the corresponding areas xsarea(t) lead to the amplitudes of the corresponding x[n] and are well-defined. For instance, xsarea(0) = 8 and leads to the parcel 8δ[n] in discrete-time.\n\n\n\n Example 1.13. Example of S/D conversion. Figure 1.18 illustrates an example of S/D conversion assuming Ts = 0.2 s.\n\n\n\n\n\n\n\n\nFigure 1.18: Example of S/D conversion assuming Ts = 0.2 s.\n\n\n\n\n\nThe visible samples of the input are xs(t) = 0.5δ(t)−2.8δ(t−Ts)+1.3δ(t−2Ts)+3.5δ(t−3Ts)−1.7δ(t−4Ts)+1.1δ(t−5Ts)+4δ(t−6Ts). Their corresponding output values are x[n] = 0.5δ[n] − 2.8δ[n− 1] + 1.3δ[n− 2] + 3.5δ[n− 3] − 1.7δ[n− 4] + 1.1δ[n− 5]) + 4δ[n− 6].   □\n\n\n1.6.5  Continuous-time to discrete-time (C/D) conversion\n\n\n\nIt is sometimes convenient to group the sampling and S/D stages into a single operation called continuous to discrete-time (C/D) conversion:\n\n\n\n\n\n\n x(t)→ C/D →x[n]. \n\n\n\n\n\n\nThe only distinction between C/D and A/D conversions is that the former does not incorporate the quantization stage, as indicated in Figure 1.17.\n\n\n\n\n\n1.6.6  Discrete-time to sampled (D/S) conversion\n\n\n\nThe discrete-time to sampled (D/S) conversion is basically the inverse of S/D. Figure 1.19 illustrates this, by depicting the inverse of the operation in Figure 1.18 of Example 1.13.\n\n\n\n\n\n\n\n\nFigure 1.19: Example of D/S conversion assuming Ts = 0.2 s. It implements the inverse operation of Figure 1.18.\n\n\n\n\n\nIn Figure 1.19, the visible samples of the input are x[n] = 0.5δ[n] − 2.8δ[n− 1] + 1.3δ[n− 2] + 3.5δ[n− 3] − 1.7δ[n− 4] + 1.1δ[n− 5]) + 4δ[n− 6]. Giventhat Ts = 0.2 seconds, the corresponding output of the D/S conversion is xs(t) = 0.5δ(t)−2.8δ(t−0.2)+1.3δ(t−0.4)+3.5δ(t−0.6)−1.7δ(t−0.8)+1.1δ(t−1)+4δ(t−1.2).\n\n\n\nWhile the S/D is a stage of the A/D process model, the D/S is part of the D/A.\n\n\n1.6.7  Reconstruction\n\n\n\nIn the context of A/D and D/A processes, the sampling inverse operation is called reconstruction, and converts a sampled signal xs(t) into an analog signal x(t). Reconstruction is also called interpolation or filtering. It consists of choosing a function h(t) that is combined with xs(t) as follows:\n\n\n\n\n\n\n\n\n\n x(t) = ∑ n=−∞∞x sarea(nT s)h(t − nTs), \n\n\n(1.13)\n\n\n\n\n\n\nwhere xsarea(nTs) is the area of the impulse in xs(t) at time t = nTs.\n\n\n\nThe complete mathematical description of reconstruction will be discussed only in Section 3.5, after the concepts of convolution and filtering are presented.7\n\n\n\nBecause xs(t) is obtained from x[n] via a D/S process, it is convenient to rewrite Eq. (1.13) observing that the impulse area xsarea(nTs) coincides with its corresponding amplitude x[n],\nsuch that:\n\n\n\n\n\n\n\n\n\n x(t) = ∑ n=−∞∞x[n]h(t − nT s). \n\n\n(1.14)\n\n\n\n\n\n\nEq. (1.14) indicates that x(t) is obtained by shifting h(t) by t = nTs, scaling this intermediate result by the corresponding amplitude x[n], and then summing up all these parcels.\n\n\n\nAmong many alternatives for choosing the reconstruction function h(t), two important ones are the so-called zero-order holder (ZOH) h(t) = Π((t − 0.5Ts)∕Ts) and sinc reconstruction h(t) = sinc(t∕Ts). ZOH is useful for its simplicity, while sincs achieve a perfect reconstruction. They are discussed in the next paragraphs.\n\n\n\n\n\nReconstruction with zero-order holder (ZOH)\n\n\n\nThe ZOH consists of choosing h(t) with amplitude 1 from t = 0 to Ts. This can be written as h(t) = Π((t − 0.5Ts)∕Ts) (see Section 1.3.6). ZOH reconstruction sustains the amplitude of a given sample x[n0] (which coincides with the area of its corresponding impulse in xs(n0Ts)) during an interval of Ts seconds until the new sample x[n0 + 1] updates this amplitude and so on.\n\n\n\n Example 1.14. Example of ZOH reconstrucion. Eq. (1.14)\nimplemented with ZOH is illustrated in Figure 1.20 for the signal xs(t) in Example 1.13. In this case, Ts = 0.2 s and, therefore, h(t) = Π((t − 0.5Ts)∕Ts) = Π((t − 0.1)∕0.2).\n\n\n\n\n\n\n\n\nFigure 1.20: Example of ZOH reconstruction using the signals of Example 1.13 with Ts = 0.2 s. In this case, x[n] = 0.5δ[n]−2.8δ[n−1]+1.3δ[n−2]+3.5δ[n−3]−1.7δ[n−4]+1.1δ[n−5])+4δ[n−6].\n\n\n\n\n\nFor instance, assuming n0 = 0 in Figure 1.20, the area of the impulse in xs(0) is 0.5 such that x(t) = 0.5 in the interval t ∈ [0,Ts[. Then, in the interval t ∈ [Ts,2Ts[ the amplitude of x(t) is  − 2.8, which is the area of the impulse at xs(Ts). And this “holding” process continues for other samples.    □\n\n\nReconstruction with sincs\n\n\n\n\n\n\nReconstruction with sincs consists of choosing h(t) = sinc(t∕Ts) (see Figure 1.8). It is the ideal reconstruction, as will be discussed along with the sampling theorem when presenting Eq. (3.19).\n\n\n\n Example 1.15. Perfect reconstruction with sinc functions. Figure 1.21 provides an example of Eq. (1.14) using sinc functions. In this case, Ts = 0.2 s and, therefore, h(t) = sinc(t∕0.2).\n\n\n\n\n\n\n\n\nFigure 1.21: Example of D/A of signal xq[n] = δ[n] − 3δ[n − 1] + 3δ[n − 2] (with quantized amplitudes) with D/S using Ts = 0.2 s and reconstruction using sinc functions.\n\n\n\n\n\nThe resulting signal x(t) has an infinite support, extending from t = −∞ to ∞ (Figure 1.21 depicts only the range t ∈ [−0.8,0.8]). It can be also noted that even xq[n] having quantized amplitudes, the reconstruction process creates an analog signal x(t) for which the amplitudes assume an infinite number of different values.\n\n\n\nFigure 1.22 provides more details about the bottom plot in Figure 1.21. Assuming xq[n] was obtained by sampling a continuous-time signal with the sampling theorem satisfied, this D/C conversion exactly recovers the original signal x(t).\n\n\n\nIn this case, the canonical sinc(t) is contracted to sinc(t∕Ts) = sinc(t∕0.2) = sinc(5t), which has zeros at t = ±0.2,±0.4,…. Still as indicated in Eq. (1.14), the amplitude x[n0] at discrete-time n0 multiplies the corresponding contracted and time-shifted sinc  (t−n0Ts Ts ) to create a parcel that is represented by a specific color and dashed line in Figure 1.22.\n\n\n\n\n\n\n\n\nFigure 1.22: Identification of the individual scaled sinc functions (dashed lines) after D/S with Ts = 0.2 s and signal reconstruction of xq[n] = δ[n] − 3δ[n − 1] + 3δ[n − 2] in Figure 1.21.\n\n\n\n\n\nIn this example, the first sinc in Figure 1.22 is centered at t = 0 (corresponding to δ[n]). The second and third sincs are  − 3sinc  (t−0.2 0.2 ) and 3sinc  (t−0.4 0.2 ), respectively. The summation of these three parcels according to Eq. (1.14), results in the analog signal\n\n\n\n\n\n\n\n\n\n x(t) = sinc(t∕0.2) − 3sinc((t − 0.2)∕0.2) + 3sinc((t − 0.4)∕0.2), \n\n\n(1.15)\n\n\n\n\n\n\nwhich is depicted with a continuous (instead of dashed) line.\n\n\n\nThe circles in Figure 1.22 indicate the time instants t = nTs and the × marks indicate the amplitude in the position of each of the three impulses. Note that x(t) has an infinite support (duration) but has zero amplitude at t = nTs, except for n = 0,1 and 2. These are discrete-time values corresponding to the support of x[n].    □\n\n\n1.6.8  Discrete-time to continuous-time (D/C) conversion\n\n\n\nThe discrete-time to continuous-time (D/C) conversion is the inverse of C/D and can be represented as the following block diagram:\n\n\n\n\n\n\n x[n]→ D/S →xs(t)→ RECONSTRUCTION →x(t) \n\n\n\n\n\n\nthat illustrates the D/C conversion is composed by D/S followed by RECONSTRUCTION.\n\n\n\n\n\n1.6.9  Analog to digital (A/D) and digital do analog (D/A) conversions\n\n\n\nThe A/D and D/A processes differ from C/D and D/C, respectively, because the former pair incorporates quantization such that the amplitude values can be represented with a finite number of bits and processed or stored in digital hardware. The distinction between D/C and D/A conversions is that the inputs are, respectively, a discrete-time x[n] and a digital signal xq[n]. For instance, the following block diagram illustrate the signals involved in a D/A process: digital xq[n], quantized-sampled xs,q(t) and analog x(t):\n\n\n\n\n\n\n xq[n]→ D/S →xs,q(t)→ RECONSTRUCTION →x(t). \n\n\n\n\n\n\n Example 1.16. Example of a D/A process. Figure 1.23 provides an example of a complete D/A process.\n\n\n\n\n\n\n\n\nFigure 1.23: Signal reconstruction of a signal with quantized amplitudes using ZOH.\n\n\n\n\n\nFigure 1.23 illustrates D/A assuming RECONSTRUCTION using ZOH, a digital signal xq[n] with amplitudes from the set M = {−3,−1,1,3} and Ts = 0.2 s.   □\n\n\n\nBoth Figure 1.23 and Figure 1.20 illustrate ZOH reconstruction, but in Figure 1.23 the amplitudes of x(t) are limited to the given finite set (quantized amplitudes) used in xq[n].\n\n\n\nOne could denote the ZOH output in Figure 1.23 as xq(t) to emphasize the amplitude is quantized, but most practical reconstruction schemes (other than ZOH) generate an analog signal with amplitudes not belonging to M and it is more general to denote the output of a reconstruction stage as x(t) instead of xq(t).\n\n\n\nHaving all these concepts defined, Figure 1.24 illustrates a complete processing chain of an input analog signal x(t) that generates the output y(t). The core of the processing is the digital signal processing (DSP) block, which can implement, for instance, a digital filter. The sampling frequency Fs is used by both A/D and D/A processes. Note that the sampled signals xs(t) and ys(t) do not actually exist within ADC or DAC chips, but are useful abstract models for the S/D and D/S processes.\n\n\n\n\n\n\n\n\nFigure 1.24: Complete processing chain of an input analog signal x(t) to generate an output y(t) using DSP.\n\n\n\n\n1.6.10  Sampling theorem\n\n\n\nThe sampling theorem  specifies the minimum value that Fs must assume in order to be able to perfectly reconstruct x(t) from xs(t) or the respective x[n]. It is stated here without proof, which is deferred to Chapter 3.\n\n\n\n\n  Theorem 1.  Sampling theorem. Assuming the maximum frequency of a real-valued signal x(t) is fmax, the sampling frequency must obey\n\n\n\n\n\n\n\n\n\n Fs &gt; 2fmax, \n\n\n(1.16)\n\n\n\n\n\n\nin order to guarantee the perfect reconstruction of x(t) from its sampled version xs(t) or the corresponding x[n]. If fmax is given in Hz, Fs is the number of real-valued samples per second, also given in Hz (alternatively, it can be denoted in samples per second, or SPS).   □\n\n\n\n\n\n\n\nSampling is so important that it will be further discussed later with the help of Fourier transforms in Section 3.5. But before delving into a more rigorous mathematical description of sampling, it is useful to understand the expression of a periodic impulse train\n\n\n\n\n\n\n p(t) = ∑ k=−∞∞δ(t − kT s), \n\n\n\n\n\n\nwhere Ts is the sampling interval and k ∈ ℤ. The expression for p(t) and the sifting property of the impulse allows to model the sampled signal as\n\n\n\n\n\n\n\n\n\n xs(t) = x(t)p(t) = ∑ k=−∞∞x(kT s)δ(t − kTs). \n\n\n(1.17)\n\n\n\n\n\n\nThe S/D stage can then convert xs(t) into x[n], which represents x(t).\n\n\n\nIn Eq. (1.16), perfect reconstruction means that given the sample values, specified as xs(t) or x[n], it is possible to recover the original x(t). But in this case, the reconstruction process is not as simple as the ZOH in Figure 1.23. The ideal (“perfect”) reconstruction adopts the infinite-duration sinc function as discussed in Section 1.6.7.0. More specifically, x(t) is obtained from Eq. (1.14) as the following combination of scaled and shifted versions of\n\nh(t) = sinc(t∕Ts):\n\n\n\n\n\n\n\n\n\n x(t) = ∑ n=−∞∞x[n]sinc  (t − nTs Ts ) . \n\n\n(1.18)\n\n\n\n\n\n\nA continuous-time signal x(t) with infinite duration often requires an infinite number of samples to be fully represented. This can be seen by sampling an eternal sinusoidal signal according to the sampling theorem. For instance, x(t) = 3cos ⁡ (20πt) has angular frequency ω = 20π rad/s and frequency 10 Hz. Assuming C/D conversion with Fs = 40 Hz, leads to x[n] = … + 3δ[n + 4] − 3δ[n + 2] + 3δ[n] − 3δ[n − 2] + 3δ[n − 4] − 3δ[n − 6] + …, which corresponds to repeating the amplitude values 3,0,−3,0 from n = −∞ to ∞. Hence, the original signal x(t) can be perfectly reconstructed from x[n] using Eq. (1.18). In this case, the infinite number of sinc parcels in Eq. (1.18) would properly add to perfectly compose the original signal x(t) = 3cos ⁡ (20πt).\n\n\n\nThe following example aims at providing a more concrete description of reconstructing a sinusoid. Instead of trying to use an eternal sinusoid and infinite sinc parcels, an approximation is used for a specific time interval.\n\n\n\n Example 1.17. Example of cosine reconstruction from few samples. This examples adopts oversampling (see Section 1.6.2) to generate a high-resolution discrete-time signal that resembles the original analog signal. Listing 1.8 describes all preparation steps for invoking the function ak_sinc_reconstruction.m.\n\n\n\nListing 1.8: MatlabOctaveCodeSnippets/snip_signals_cosine_reconstruction.m\n\n\n%% Define variables \nmax_n = 8; %n varies from -max_n to max_n \nTs=0.2; %sampling interval (in seconds) \nA=4; %cosine amplitude, in Volts \n5Fs=1/Ts; %sampling frequency (5 Hz) \nfc=Fs/4; %cosine frequency (1.25 Hz) \noversampling_factor = 200; %oversampling factor \ntextra = 1; %one extra time (1 second) for visualizing sincs \n%% Generate signal xn sampled at Fs \n10n=-max_n:max_n; %original discrete-time axis as integers \nt=(-max_n:max_n)*Ts; %original sampled time axis in seconds \nxn=A*cos(2*pi*fc*t); %cosine sampled at Fs \n%% Generate oversampled version of xn \noversampled_Ts = Ts/oversampling_factor; %new value of Ts \n15oversampled_n = n(1)*oversampling_factor:n(end)*oversampling_factor; \noversampled_t = oversampled_n*oversampled_Ts; %time in seconds \noversampled_xn=A*cos(2*pi*fc*oversampled_t); %oversampled cosine \n%% Reconstruct signal from samples stored at xn and compare with \n%% the \"ground truth\" oversampled_xn \n20ak_sinc_reconstruction(n,xn,Ts,oversampled_n,oversampled_xn,textra);\n  \n\n\nThe low-resolution signal x[n], or xn in Listing 1.8, represents a segment with only 17 samples obtained by sampling a segment from t = −1.6 to 1.6 s of the original signal x(t) = Acos ⁡ (2πfct), with A = 4 Volts and fc = 1.25 Hz. Because Fs = 5 Hz is larger than 2fc Hz, the sampling theorem is obeyed and a perfect reconstruction is possible.\n\n\n\n\n\n\n\n\nFigure 1.25: Sampling and sinc-based perfect reconstruction of a cosine as implemented in function ak_sinc_reconstruction.m.\n\n\n\n\n\nThe signal oversampled_xn plays the role of the analog signal x(t) within a computer, using a relatively large oversampling factor of 200. This signal is shown in the top plot of Figure 1.25. The second plot from the top is depicting the sampled signal xs(t), while the third shows the discrete-time signal x[n] = S/D{xs(t)}.\n\n\n\nThe signal oversampled_xn is passed as an input argument to function ak_sinc_reconstruction.m such that it can be compared to the reconstructed signal created within this function, as depicted in the bottom plot of Figure 1.25. The two plots (“original” and “reconstructed”) overlap almost perfectly. But closely observing the bottom plot in Figure 1.25 shows that the reconstruction is better in the time instants closer to the center t = 0 than in the endpoints t = ±1.6 s. The reason for that can be understood by interpreting Figure 1.26 and Figure 1.27.\n\n\n\n\n\n\n\n\nFigure 1.26: Single sinc parcel from Figure 1.27 corresponding to the sinc centered at t = −0.8 s as dashed line and the reconstructed signal as a solid line.\n\n\n\n\n\nFigure 1.26 depicts a single sinc while Figure 1.27 shows all sinc parcels. Figure 1.26 shows the sinc centered at t = −0.8 s (or n = −4) and the reconstructed signal, obtained by using all sinc parcels.\n\n\n\n\n\n\n\n\nFigure 1.27: All individual parcels in dashed lines corresponding to each sinc and the summation as reconstructed signal (solid line) of Listing 1.8.\n\n\n\n\n\nBecause it has several superimposed curves, the reader is invited to obtain Figure 1.27 by executing ak_sinc_reconstruction.m iteratively, step-by-step, to see each individual sinc being plotted. Figure 1.27 can only depict the final result, with all sincs superimposed. Basically, this figure shows that the scripts was created such that only nine sincs effectively contribute to reconstructing the cosine, given that six samples of x[n] have amplitude equals zero. The reconstruction at the endpoints is impaired because the infinite number of sincs in Eq. (1.18) are not used by the script. The missing sincs at the left (t &lt; −1.6) and right (t &gt; 1.6) extrema have more impact at the endpoints than at t = 0.   □\n\n\n\n Example 1.18. Reconstruction of a signal composed by sincs. This example further investigates the reconstruction of the signal described by Eq. (1.15), which is repeated below for convenience:\n\n\n\n\n\n\n x(t) = sinc(t∕0.2) − 3sinc((t − 0.2)∕0.2) + 3sinc((t − 0.4)∕0.2). \n\n\n\n\n\n\nThe signal x(t) is composed by three parcels of the sinc\n\n\n\n\n\n\n z(t) = sinc  ( t 0.2 ). \n\n\n\n\n\n\nFinding the frequencies that compose a signal is the topic of Chapter 2, but it can be anticipated here that the maximum frequency8 of z(t) is bounded by fmax &lt; 2.5 Hz. Shifting z(t) to t = 0.2 and t = 0.4, and then adding these three parcels to obtain Eq. (1.15), does not increase the maximum frequency. Hence, the bound fmax &lt; 2.5 Hz also holds for the signal x(t).\n\n\n\nIn the case of fmax &lt; 2.5 Hz, the sampling theorem states that Fs = 5 Hz guarantees perfect reconstruction. Listing 1.9 adopted Ts = 0.1 s, which corresponds to Fs = 1∕Ts = 10 Hz, which also obeys the theorem.\n\n\n\nListing 1.9: MatlabOctaveCodeSnippets/snip_signals_reconstruction_sinc.m\n\n\n%% Define variables \nTs=0.1; %sampling interval (in seconds) \nFs=1/Ts; %sampling frequency (5 Hz) \noversampling_factor = 200; %oversampling factor \n5textra = 0.5; %one extra time (2 seconds) for visualizing sincs \nsinc_support = 0.2; %support of the sinc in seconds \nn=-8:12; %segment of samples of interest \n%% Generate signal xn sampled at Fs \nt=n*Ts; %discrete-time axis in seconds \n10xn = sinc(t/sinc_support) - 3*sinc((t-0.2)/sinc_support) + ... \n    3*sinc((t-0.4)/sinc_support); %define the sampled signal \n%% Generate oversampled version of xn \noversampled_Ts = Ts/oversampling_factor; %new value of Ts \noversampled_n = n(1)*oversampling_factor:n(end)*oversampling_factor; \n15oversampled_t = oversampled_n*oversampled_Ts; %time in seconds \noversampled_xn = sinc(oversampled_t/sinc_support) - ... \n    3*sinc((oversampled_t-0.2)/sinc_support) + ... \n    3*sinc((oversampled_t-0.4)/sinc_support); %oversampled signal \n%% Reconstruct signal from samples stored at xn and compare with \n20%% the \"ground truth\" oversampled_xn \nak_sinc_reconstruction(n,xn,Ts,oversampled_n,oversampled_xn,textra);\n  \n\n\nFigure 1.28 (bottom plot) illustrates good reconstruction, even when using a segment of limited duration (from t = −0.8 to 1.2 s).\n\n\n\n\n\n\n\n\nFigure 1.28: Sampling and reconstruction of x(t) = sinc(t∕0.2) − 3sinc((t − 0.2)∕0.2) + 3sinc((t − 0.4)∕0.2) using Ts = 0.1 s.\n\n\n\n\n\nFigure 1.29 is similar to Figure 1.27 and shows the sinc parcels used to reconstruct the signal in Figure 1.28.\n\n\n\n\n\n\n\n\nFigure 1.29: Sinc parcels used in the reconstruction of x(t) of Figure 1.28.\n\n\n\n\n\nBoth Figure 1.28 and Figure 1.21 are derived using the same signal x(t), but using Ts = 0.1 and 0.2, respectively. It can be seen from Figure 1.21 that Ts = 0.2 s has, in this case, the property of representing x(t) with only three non-zero samples. The reason to have only three non-zero samples is that the impulses in the train p(t) = ∑ ⁡ k=−∞∞δ(t − 0.2k) used for sampling, coincide with x(t) having an amplitude of zero, with the exception of t = 0,0.2 and 0.4 seconds, as depicted in Figure 1.22. However, if Ts = 0.1 s is adopted, an infinite number of samples is required to represent x(t).    □\n\n\n\n Example 1.19. Failing to reconstruct signals when adopting Fs = 2fmax. The sampling theorem is a strict inequality Fs &gt; 2fmax but some people state it as a non strict inequality, i. e., Fs ≥ 2fmax, which is incorrect.\n\n\n\nOne reason for considering Fs ≥ 2fmax is wrongly interpreting that fmax of the signal described by Eq. (1.15) is fmax = 2.5 Hz. Because in this case Fs = 5 Hz works, it may lead to the incorrect assumption that Fs ≥ 2fmax always works. However, the signal of Eq. (1.15) has infinite frequency components, which extend from zero up to 2.5 Hz, but it does not have a discrete component at 2.5 Hz. Hence, its fmax &lt; 2.5 Hz and Fs = 5 Hz obeys the sampling theorem, and allows perfect reconstruction, as can be seen in Figure 1.21.\n\n\n\nHowever, if a signal has a discrete component at fmax, then choosing Fs = 2fmax does not guarantee reconstruction. A simple example is to consider sampling the signals y(t) = cos ⁡ (2π × 2.5t) and x(t) = cos ⁡ (2π × 2.5t − 0.5π) using\n\nFs = 5 Hz. Both signals have fmax = 2.5 Hz, but the time delay created by the phase 0.5π makes the sampling instants coincide with the zeros of x(t). In this case, xs(t) (and, consequently x[n]) has amplitudes equal to zero, and reconstruction fails. This is indicated in Figure 1.30, created with the script snip_signals_failed_cosine_reconstruction.m. This script can be modified to show that y(t) can be eventually reconstructed, but it is not guaranteed. In summary, one must use Fs &gt; 2fmax to guarantee perfect reconstruction.\n\n\n\n\n\n\n\n\nFigure 1.30: Example of failing to reconstruct the signal x(t) = cos ⁡ (2π2.5t − 0.5π) using Fs = 2fmax = 5 Hz.\n\n\n\n\n\nAs another example, assume a cosine\n\n\n\n\n\n\n\n\n\n x(t) = Acos ⁡   (2πf0t + 𝜃), \n\n\n(1.19)\n\n\n\n\n\n\nwhere A = 1∕cos ⁡ (𝜃). The sampling frequency is (wrongly) chosen as Fs = 2f0 such that x(n) = cos ⁡ (πn) = (−1)n and x(t) cannot be recovered from x[n] (which is the same for any phase 𝜃), as illustrated by Listing 1.10.\n\n\n\nListing 1.10: MatlabOctaveCodeSnippets/snip_signals_sampling_inequality.m\n\n\nFs=20; Ts=1/Fs; %sampling frequency Fs in Hz and period Ts in sec. \nf0=Fs/2; %key thing: Fs should  be greater than 2 f0 \nt=0:Ts:1; %discrete-time axis, from 0 to 1 second \ntheta1=pi/4 %define an arbitrary angle \n5A1=1/cos(theta1); %amplitude \nx1=A1*cos(2*pi*f0*t+theta1); %not obeying sampling theorem \ntheta2=0 %define any value distinct from theta1 \nA2=1/cos(theta2); %amplitude \nx2=A2*cos(2*pi*f0*t+theta2); %not obeying sampling theorem \n10plot(t,x1,'rx',t,x2,'o-b'); title('Cannot distinguish 2 signals!')\n  \n\n\nThis ambiguity and the previous example demonstrate the need for a strict inequality Fs &gt; 2fmax when interpreting the sampling theorem.   □\n\n\n\n Example 1.20. Trying to prove that the sampling theorem is wrong! A very astute student compared Figure 1.11 and Figure 1.21, and observed that two distinct signals could have generated the samples in x[n] = δ[n] − 3δ[n − 1] + 3δ[n − 2]. Hence, she concluded that the sampling theorem was wrong, given that from the samples x[n], one would not be able to distinguish if the original signal corresponds to Eq. (1.7) or Eq. (1.15).\n\n\n\nHer main mistake was not to check the maximum frequencies of the signals given by Eq. (1.7) or Eq. (1.15). Finding the frequencies that compose a signal is the topic of\nChapter 2, but it can be anticipated here that the maximum frequency of Eq. (1.15) is fmax = 2.5 Hz (from Eq. (B.55)) while the signal corresponding to Eq. (1.7) has an infinite maximum frequency (from Eq. (B.54)) and its conversion to x[n] did not obey the sampling theorem!\n\n\n\nListing 1.11 illustrates what happens when one starts with Eq. (1.7), samples this signal (but not obeying the sampling theorem) and then tries to recover Eq. (1.7) using sinc interpolation. The generated Figure 1.31 shows that reconstruction fails miserably.\n\n\n\nListing 1.11: MatlabOctaveCodeSnippets/snip_signals_reconstruction_failure.m\n\n\n%% Define variables \nTs=0.2; %sampling interval (in seconds) \nFs=1/Ts; %sampling frequency (5 Hz) \noversampling_factor = 200; %oversampling factor \n5textra = 1; %one extra time (1 second) for visualizing sincs \npulse_width = 0.2; %support of the rects in seconds \n%% Generate signal xn sampled at Fs \nn=-3:3; \nxn=[0 0 0 1 -3 3 0]; %signal with only three non-zero samples \n10%% Generate oversampled version of xn \noversampled_Ts = Ts/oversampling_factor; %new value of Ts \noversampled_n = n(1)*oversampling_factor:n(end)*oversampling_factor; \noversampled_t = oversampled_n*oversampled_Ts; %time in seconds \noversampled_xn = rectpuls(oversampled_t,pulse_width) - ... \n15    3*rectpuls(oversampled_t-Ts,pulse_width) + ... \n    3*rectpuls(oversampled_t-2*Ts,pulse_width); %define the signal \n%% Reconstruct signal from samples stored at xn and compare with \n%% the \"ground truth\" oversampled_xn \nak_sinc_reconstruction(n,xn,Ts,oversampled_n,oversampled_xn,textra);\n  \n\n\n\n\n\n\n\nFigure 1.31: Reconstruction of signal given by Eq. (1.7) fails because the sampling theorem is not obeyed.\n\n\n\n\n\nFigure 1.32 is similar to Figure 1.27 and shows the sinc parcels used to reconstruct the signal.\n\n\n\n\n\n\n\n\nFigure 1.32: Sinc parcels used in the signal reconstructed as depicted in Figure 1.31.\n\n\n\n\n\nThe main message of this example is the proper interpretation of the sampling theorem: there is a unique continuous-time signal that has fmax &lt; Fs∕2 and is represented by a given sampled signal xs(t), or equivalently, x[n]. In this specific case, Eq. (1.15) is the only signal x(t) that can lead to x[n] = δ[n] − 3δ[n − 1] + 3δ[n − 2].    □\n\n\n\nThe perfect reconstruction guaranteed by the sampling theorem, Eq. (1.16), assumes the use of the ideal sinc interpolation of Eq. (1.18), which is not realizable in practice. Hence, Fs is chosen to be typically 2.5fmax or even larger to alleviate the errors imposed by a non ideal reconstruction. The sampling frequencies adopted for some common signals9 are shown in Table 1.3.\n\n\n\n\n\n\n\nTable 1.3: Typical sampling frequencies.\n\n\n\n\n\n\n\n\n\n\n\n\nSignal \n\n\n\nfmax\n\n\n\n\nExplanation for fmax\n\n\n\n\nFs\n\n\n\n\nTelephone speech \n\n\n3400 Hz \n\n\n\n\nBand [300, 3400] Hz suffices for intelligibility\n\n\n\n8 kHz \n\n\n\n\nAudio (CD format) \n\n\n20 kHz \n\n\n\n\nHumans can hear up to  ∼20 kHz\n\n\n\n44.1 kHz \n\n\n\n\nElectrocardiogram (ECG) \n\n\n\n &lt; 250 Hz \n\n\n\n\nClinical studies\n\n\n\n500 Hz \n\n\n\n\n\n\n\nIn special cases, when x(t) is complex-valued or a “passband” signal (Section 3.5.5), a lower value of Fs can be adopted and still allow perfect reconstruction.\n\n\n1.6.11  Different notations for S/D conversion\n\n\n\nIt is convenient to have two different notations for the S/D conversion: a simpler one and an alternative that better represents cases in which the amplitude of x[n] depends on the sampling interval Ts. We start their discussion with an example.\n\n\n\n Example 1.21. Example of S/D conversion of a signal xs(t). Assume the continuous-time signal x(t) = 4e−2tu(t) should be sampled with a period Ts to create xs(t), which is then transformed into a discrete-time signal x[n] via the S/D operation. The notation S/D{⋅} will denote the S/D process, which in this case leads to:\n\n\n\n x[n]  = S/D  {xs(t)}     = S/D  {∑ n=−∞∞x(nT s)δ(t − nTs)}     = S/D  {∑ n=−∞∞4e−2nTs u(nTs)δ(t − nTs)}     = 4e−2nTs u[n], (1.20)  \n\nThe last step in Eq. (1.20) is based on the fact that S/D{u(nTs)δ(t − nTs)} = u[n].    □\n\n\n\nWhen writing signal expressions, the sampling operation is eventually not made explicit. An alternative notation is discussed in the following example.\n\n\n\n Example 1.22. Simplified notation for the S/D conversion of a signal x(t). This example discusses a simplified notation for the S/D conversion that is sometimes adopted in the literature.\n\n\n\nAssume the continuous-time signal x(t) = 4e−2tu(t) should be transformed to a discrete-time x[n] with a sampling period Ts. This conversion is often denoted as:\n\n\n\n\n\n\n\n\n\n x[n] = x(t)|t=nTs = 4e−2nTs u[n], \n\n\n(1.21)\n\n\n\n\n\n\nwhich can be confusing. One could write x(t)|t=nTs = 4e−2nTsu(nTs) and, comparing to Eq. (1.21), complain that the samples of the continuous-time step function u(nTs) became u[n] (nTs was “substituted” by n), while nTs remained (was not substituted by n) in the exponential e−2nTs.\n\n\n\nThe reason to be careful with the simplified notation x[n] = x(t)|t=nTs is that, when performing a S/D conversion, the occurrences of nTs as part of the independent variable (the argument t of x(t), within (⋅)) are converted to n, as depicted in Figure 1.18. However, the factor nTs remains when it influences the amplitude (dependent variable).\n\n\n\nHence, the notation x[n] = x(t)|t=nTs for the S/D process in Eq. (1.21) is somehow incomplete. It does not rely on impulses and, consequently, it does not make explicit the intermediate step of creating a sampled signal xs(t). However, because the S/D{⋅} notation is cumbersome, the reader should be also familiar with the widely adopted alternative of Eq. (1.21).   □\n\n\n \n\n7 Eq. (1.13) represents the convolution between xs(t) and h(t). Convolution will be discussed in Section 3.4.1 and allows h(t) to be interpreted as the impulse response of a filter used for reconstruction.\n\n \n\n8 This fmax value is obtained from Eq. (B.55) with F = 2.5 Hz, and observing that the spectrum of x(t) extends up to 2.5 Hz but does not have an impulse at this frequency, indicating that there is no frequency component at 2.5 Hz, and fmax &lt; 2.5 Hz.\n\n \n\n9 ECG is discussed, for example, at [url1ecg].\n\n                                                                 &lt;/div&gt;"
  },
  {
    "objectID": "ak_dsp_bookse7.html",
    "href": "ak_dsp_bookse7.html",
    "title": "7  Relating Frequencies in Continuous, Sampled and Discrete-time Signals",
    "section": "",
    "text": "When periodic sampling is used with sampling frequency Fs, the frequencies that show up in x(t) are mapped into frequencies in xs(t) and x[n]. This section discusses such mappings. We start by observing that angular frequencies are denoted as ω and Ω, in continuous and discrete-time, respectively. This notation is important because they have different units and properties.\n\n\n\n\n\n1.7.1  Units of continuous-time and discrete-time angular frequencies\n\n\n\nIn essence, a continuous-time signal (for instance, x(t) = cos ⁡ (ωt)) has radians per second (rad/s) as the unit of the angular frequency ω. Multiplying ω by t, which is given in seconds, leads to an angle in radians. In contrast, the unit of the angular frequency Ω of a discrete-time signal (for instance, x[n] = cos ⁡ (Ωn)) is given in radians. Because n is dimensionless, Ωn is an angle in radians. Similar to the interpretation of discrete-time n as “time”, in spite\nof being an angle, Ω will be interpreted as angular frequency.\n\n\n\nThe different units of ω (rad/s) and Ω (rad) will play a fundamental role in discrete-time signal processing: a function f(ω) of ω can assume distinct values when ω is varied in the range [−∞,∞], while a function f(Ω) is periodic if Ω is an angle. More specifically, When the variable Ω is used to denote a discrete-time angular frequency, any function f(Ω) of Ω will have a period of 2π rad and be typically evaluated only in a range of 2π, such as [0,2π[ or [−π,π[.\n\n\n\nIn other words, whenever Ω is an angular frequency (that is, an angle), the function f(Ω) = f(Ω + 2π),∀ ⁡Ω is periodic and Ω shows up as the argument of cosines or sines. For example, f(Ω) = cos ⁡ (3Ω)∕cos ⁡ (5Ω) and f(Ω) = ej2Ω are possible functions of an angular frequency Ω. In fact, it is a common practice to use the notation f(ejΩ) to indicate that f is a function of an angle Ω (see, e. g. Eq. (2.29)). Therefore, one will never find something like f(ejΩ) = (3 + Ω)∕(5 + Ω) because the periodicity f(Ω) = f(Ω + 2π) is not observed in this case.\n\n\n\n\n\n1.7.2  Mapping frequencies in continuous and discrete-time domains\n\n\n\nIn the following paragraphs, the main goal is not to prove, but to motivate the fundamental equation\n\n\n\n\n\n\n\n\n\n ω = ΩFs, \n\n\n(1.22)\n\n\n\n\n\n\nwhere Fs is the sampling frequency assumed to be in Hertz, ω is the continuous-time angular frequency given in radians per second and Ω is the discrete-time angular frequency given in radians.\n\n\n\nTo better interpret Eq. (1.22), one can apply the S/D conversion to a single sinusoid, as exemplified in the next paragraphs.\n\n\n\n Example 1.23. Example of using the fundamental equation for relating angular frequencies of continuous-time and discrete-time cosines. Assume x(t) = 10cos ⁡ (24πt) is sampled with sampling interval Ts to create the signal xs(t) via the impulse sifting property of Eq. (1.17):\n\n\n\n\n\n\n\n\n\n xs(t) = ∑ n=−∞∞10cos ⁡ (24πnT s)δ(t − nTs). \n\n\n(1.23)\n\n\n\n\n\n\nIn this case, the S/D conversion of xs(t) results in\n\n\n\n\n\n\n x[n] = S/D  {xs(t)} = 10cos ⁡ (24πTsn) = 10cos ⁡   (24π Fs n). \n\n\n\n\n\n\nNote that the original angular frequency ω = 24π radians/s was converted to the angular frequency in discrete-time Ω = 24π∕Fs radians, which corresponds to Ω = ω∕Fs in Eq. (1.22).\n\n\n\nFor example, assuming Fs = 36 Hz, the cosine with angular frequency ω = 24π rad/s (that is equivalent to the linear frequency f = 12 Hz in this case) will be mapped to the angle Ω = 2π∕3 rad. Using Eq. (1.22) in the other direction, a discrete-time angular frequency of Ω = π rad is always mapped to ω = πFs. In the given example, Ω = π rad is mapped into ω = 36π rad/s or, equivalently, f = 18 Hz.   □\n\n\n\nThe previous example can be generalized. In summary, Eq. (1.22) is valid for all pairs of continuous-time and discrete-time signals that are related by a C/D or D/C conversion obtained with periodic sampling.\n\n\n\n Example 1.24. Another example of conversion between discrete and continuous-time domains. Assume a discrete-time signal x[n] = 10cos ⁡ ((2π∕7)n). It is possible to indicate that its angular frequency is Ω = 2π∕7 rad because the signal repeats itself every N = 7 samples. However, because n is dimensionless, there is no information about time in this case. If it is stated that x[n] was obtained via sampling at Fs = 10 Hz, x[n] would be representing a cosine of angular frequency ω = 20π∕7 rad/s. Instead, if Fs = 100 Hz, then ω = 200π∕7 rad/s. As\nindicated in Eq. (1.22), the sampling frequency is needed when mapping the discrete-time angular frequency Ω into the corresponding continuous-time angular frequency ω.    □\n\n\n\nIn summary, Fs in Eq. (1.22) plays the role of a normalization factor that relates angular frequencies of continuous-time signals and their discrete-time counterparts obtained by C/D conversion. It leads to two distinct ways of simulating a continuous-time sinusoid using a discrete-time signal.\n\n\n\n Example 1.25. Two approaches to simulate continuous-time sinusoids. Listing 1.12 indicates how to use Eq. (1.22) to create discrete-time sinusoids. The task is to generate a cosine of 600 Hz using Matlab/Octave. Two distinct approaches are:\n\n\n\nf 1.\n\n\nuse “continuous-time” frequencies f (in Hz) or ω (in rad/s), with discretized time t in seconds, or\n\n\nf 2.\n\n\nuse “discrete-time” frequencies Ω (in rad), obtained via Eq. (1.22), and using a dimensionless “time” n.\n\n\n\n\nThese two approaches are contrasted in Listing 1.12.\n\n\n\nListing 1.12: MatlabOctaveCodeSnippets/snip_signals_sinusoid_generation.m\n\n\nFs=8000; %sampling frequency (Hz) \nTs=1/Fs; %sampling interval (seconds) \nN=20000; %number of desired samples \nf0=600; %cosine frequency (Hz) \n5%%%%First alternative to generate a cosine of 600 Hz \nt=0:Ts:(N-1)*Ts; %discretized continuous-time axis (sec.) \nx1=5*cos(2*pi*f0*t); %amplitude=5 V and frequency = f0 Hz \n%%%%Second alternative: work directly in discrete-time \nw0=2*pi*f0*Ts; %w0 is in rad, convert from rad/s to rad \n10n=0:N-1; %discrete-time axis (do not use Ts anymore) \nx2=5*cos(w0*n); %amplitude=5 V and frequency = w0 rad \nplot(x1-x2); %plot error between two alternative sequences \nsoundsc(x1,Fs) %for fun: playback one of them to listen\n  \n\n\nNote that the sequences x1 and x2 are essentially the same, and there are only small numerical errors. In essence, one can simulate discrete-time signals representing the time evolution either with n (an integer) or t (in seconds), but properly using the corresponding angular frequencies Ω (in radians) or ω (in rad/s), respectively.\n\n\n\nListing 1.12 compared signals created with n = 0,1,2,…, versus\n\nt = nTs = 0,125 × 10−6,250 × 10−6,…, (given that Ts = 1∕Fs = 125 × 10−6 seconds), with the corresponding angular frequencies being Ω = 2π600∕8000 ≈ 0.471 rad or ω = 2π600 ≈ 3769.9 rad/s, respectively.   □\n\n\n\n\n\n1.7.3  Nyquist frequency\n\n\n\nAs indicated by Eq. (1.16), if the sampling theorem is obeyed, the maximum frequency in the original continuous-time signal x(t) is restricted to fmax &lt; Fs∕2 Hz, where Fs∕2 is called the Nyquist frequency.\n\n\n\nUsing Eq. (1.22), one can see that angle Ω = π rad will be mapped into ω = ΩFs = πFs rad/s, which corresponds to the Nyquist frequency Fs∕2 Hz. This is consistent with the fact that π represents the highest frequency in discrete-time processing. This can be observed by plotting x[n] = cos ⁡ (Ωn) and varying Ω until it reaches π rad, which corresponds to a period of N = 2 samples. Increasing Ω from π to 1.5π, for instance, will slow down the signal (observe the angles: 1.5π = −0.5π), and increase the period to N = 4 samples.\n\n\n\n\n\n1.7.4  Frequency normalization in Matlab/Octave\n\n\n\nIt is sometimes inconvenient to show graphs with the abscissa using Ω in radians. For example, a graph in the range [0,π] would have the last abscissa value as approximately\n\n3.14159, which could be annoying. To avoid that, the convention adopted by Matlab/Octave is to use, instead of Ω in rad, a normalized frequency\n\n\n\n\n\n\n\n\n\n fN = Ω∕π. \n\n\n(1.24)\n\n\n\n\n\n\nThis division by π maps the discrete-time angular frequency range [0,π] rad into the range [0,1] of a normalized frequency fN.\n\n\n\nWith Fs being the sampling frequency, a given frequency f in Hz is mapped to fN in Matlab/Octave by substituting Eq. (1.22) in Eq. (1.24), such that\n\n\n\n\n\n\n\n\n\n fN = f Fs∕2, \n\n\n(1.25)\n\n\n\n\n\n\n\ne., f is divided by the Nyquist frequency. For example, if Fs = 10 Hz, a frequency f=4 Hz after the sampling process would be represented as fN = 4∕5 in Matlab/Octave.\n\n\n\n\n\n\n\n\nTable 1.4: The notation and values of the Nyquist frequency in distinct domains.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nf (Hz) \n\n\n\nω (rad/s) \n\n\n\nΩ (rad) \n\n\nMatlab/Octave normalized fN (Ω∕π) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFs∕2\n\n\n\nπFs\n\n\n\nπ\n\n\n1 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                                                                                                                                                                                                                                                                                                      &lt;/div&gt;\n                                                                          \n                                                                          \n\n\n\n\n\n\n\n\nFigure 1.33: Versions of the Nyquist frequency in continuous and discrete-time.\n\n\n\n\n\nTable 1.4 and Figure 1.33 summarize the discussed information about versions of the Nyquist frequency in different domains, including its normalized version in Matlab/Octave. Figure 1.33 is also useful to observe the relation between the frequencies ω, f, Ω and fN."
  },
  {
    "objectID": "ak_dsp_bookse8.html",
    "href": "ak_dsp_bookse8.html",
    "title": "8  An Introduction to Quantization",
    "section": "",
    "text": "1.8  An Introduction to Quantization\n\n\n\nSimilar to sampling, quantization is very important, for example, because computers and other digital systems use a limited number b of bits to represent numbers. In order to convert an analog signal to be processed by a computer, it is therefore necessary to quantize its sampled values.\n\n\n\nQuantization is used not only when ADCs are involved. Whenever it is necessary to perform an operation representing the numbers with less bits than the current representation, this process can be modeled as quantization. For instance, an image with the pixels represented by 8 bits can be turned into a binary image with a quantizer that outputs one bit. Or an audio signal stored as a WAV file can be converted from 24 to 16 bits per sample.\n\n\n\n\n\n1.8.1  Quantization definitions\n\n\n\nA quantizer Q maps input values from a set (eventually with an infinite number of elements) to a smaller set with a finite number of elements. Without loss of generality, one can assume in this text that the quantizer inputs are real numbers. Hence, the quantizer is described as a mapping Q : ℝ → ℳ  from a real number x[n] ∈ ℝ to an element xq[n] of a finite set ℳ  called codebook. The quantization process can be represented pictorially by:\n\n\n\n\n\n\n x[n]→ Q →xq[n] ∈ ℳ . \n\n\n\n\n\n\nThe cardinality of this set is |ℳ | = M and typically M = 2b, where b is the number of bits used to represent each output xq[n]. The higher b, the more accurate the representation tends to be.\n\n\n\nA generic quantizer can then be defined by the quantization levels specified in ℳ  and the corresponding input range that is mapped to each quantization level. These ranges impose a partition of the input space. Most practical quantizers impose a partition that corresponds to rounding the input number to the nearest output level. An alternative to rounding is truncation and, in general, the quantizer mapping is arbitrary.\n\n\n\nThe input/output relation of most quantizers can be depicted as a “stairs” graph, with a non-linear mapping describing the range of input values that are mapped into a given element of the output set ℳ .\n\n\n\n Example 1.26. Example of generic (non-uniform) quantizer. For instance, assuming the codebook is ℳ = {−4,−1,0,3} and a quantizer that uses “rounding” to the nearest output level, the input/output mapping is given by Figure 1.34.\n\n\n\n\n\n\n\n\nFigure 1.34: Input/output mapping for the quantizer specified by ℳ = {−4,−1,0,3}.\n\n\n\n\n\nTable 1.5 lists the input ranges and corresponding quantizer output levels for the example of Figure 1.34.\n\n\n\n\n\n\n\nTable 1.5: Input/output mapping for the quantizer specified by ℳ = {−4,−1,0,3} of Figure 1.34.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInput range \n\n\nOutput level \n\n\n\n\n\n\n\n\n\n\n\n\n\n] −∞,−2.5[\n\n\n\n−4\n\n\n\n\n\n\n\n\n\n\n\n\n\n[−2.5,−0.5[\n\n\n\n−1\n\n\n\n\n\n\n\n\n\n\n\n\n\n[−0.5,1.5[\n\n\n\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n[1.5,∞[\n\n\n\n3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                              &lt;/div&gt;\n                                                                          \n                                                                          \n\n\n\nGiven the adopted rounding, the input thresholds (in this case,  − 2.5,  − 0.5 and 1) indicate when the output changes, and are given by the average between two neighboring output levels. For instance, the threshold  − 2.5 = (−1 − (−4))∕2 is the average between the outputs  − 4 and  − 1.    □\n\n\n\nThe adopted convention is that the input intervals are left-closed and right-open (e. g., [−2.5,−0.5[ in Table 1.5).\n\n\n\nIn most practical quantizers, the distance between consecutive output levels is the same and the quantizer is called uniform. In contrast, the quantizer of Table 1.5 is called non-uniform.\n\n\n\nThe quantization error is\n\n\n\n\n\n\n eq[n] ≜ x[n] − xq[n]. \n\n\n\n\n\n\n Example 1.27. Examples of rounding and truncation. Assume the grades of an exam need to be represented by integer numbers. Using rounding and truncation, an original grade of 8.7 is quantized to 9 and 8, respectively. In this case, the quantization error is  − 0.3 and 0.7, respectively. Assuming the original grade can be any real number x[n] ∈ [0,10], rounding can generate quantization errors in the range eq[n] ∈ [−0.5,0.5[ while truncation generates errors eq[n] ∈ [0,1].   □\n\n\n\nUnless otherwise stated, hereafter we will assume the quantizer implements rounding.\n\n\n1.8.2  Implementation of a generic quantizer\n\n\n\nA quantizer can be implemented as a list of if/else rules implementing a binary search. The quantizer of Table 1.5 can be implemented as in Listing 1.13.\n\n\n\nListing 1.13: MatlabOctaveCodeSnippets/snip_signals_nonuniform_quantization.m\n\n\nx=-5 %define input \nif x &lt; -0.5 \n    if x &lt; -2.5 \n        x_quantized = -4 %output if x in ]-Inf, -2.5[ \n5    else \n        x_quantized = -1 %output if x in [-2.5, -0.5[ \n    end \nelse \n    if x &lt; 1.5 \n10        x_quantized = 0 %output if x in [-0.5, 1.5[ \n    else \n        x_quantized = 3 %output if x in [1.5, Inf[ \n    end \nend\n  \n\n\nAn alternative implementation of quantization is by performing a “nearest-neighbor” search: find the output level xq[n] minimum squared error between the input value x[n] and all output levels (that play the role of “neighbors” of the input value). This is illustrated in Listing 1.14.\n\n\n\nListing 1.14: MatlabOctaveCodeSnippets/snip_signals_nonuniform_quant2.m\n\n\nx=2.3 %define input \noutput_levels = [-4, -1, 0, 3]; \nsquared_errors = (output_levels - x).^2; \n[min_value, min_index] = min(squared_errors); \n5x_quantized = output_levels(min_index) %quantizer output\n  \n\n\n\n\n1.8.3  Uniform quantization\n\n\n\nUnless otherwise stated, this text assumes uniform quantizers with a quantization step or step size Δ.\n\n\n\nIn this case, all the steps in the quantizer “stairs” have both height and width equal to Δ. In contrast, note that the steps in Figure 1.34 are not equal. Uniform quantizers are very useful because they are simpler than the generic non-uniform quantizer.\n\n\n\nA uniform quantizer is defined by the number b of bits and only two other numbers: Δ\nand X^min, where X^min is the minimum value of the quantizer output xq[n] ∈M, where\n\n\n\n\n\n\n\n\n\n M = {Xmin,Xmin + Δ,X^min + 2Δ,…,X^min + (M − 1)Δ}. \n\n\n(1.26)\n\n\n\n\n\n\nFor instance, assuming b = 2 bits, X^min = −4 and Δ = 3, the output levels are M = {−4,−1,2,5}.\n\n\n\nThe value of Δ is also called the least significant bit (LSB) of the ADC, because it represents the value (e. g., in Volts) that corresponds to a variation of a single bit. Later, this is also emphasized in Eq. (B.123) in the context of modeling the representation of real numbers in fixed-point as a quantization process.\n\n\n\n\n\n1.8.4  Granular and overload regions\n\n\n\nThere are three regions of operation of a quantizer: the granular and two overload (or saturation) regions. An overload region is reached when the input x[n] falls outside the quantizer output dynamic range. The granular is between the two overload regions.\n\n\n\nFigure 1.35 depicts the input/output relation of a 3-bits quantizer with Δ = 1. In Figure 1.35, because Δ = 1, operation in the granular region corresponds to the rounding operation round to the nearest integer. For example, round(2.4)=2 and round(2.7)=3. When Δ≠1, the quantization still corresponds to rounding to the nearest X^ ∈ ℳ , but X^ is not\nrestricted to be an integer anymore.\n\n\n\n\n\n\n\n\nFigure 1.35: Input/output relation of a 3-bits quantizer with Δ = 1.\n\n\n\n\n\nFigure 1.36 depicts the input/output relation of a 3-bits quantizer with Δ = 0.5. Close inspection of Figures 1.35 and 1.36 shows that the error eq[n] is in the range [−Δ∕2,Δ∕2] within the granular region, but can grow indefinitely when the input falls in one of the overload regions.\n\n\n\n\n\n\n\n\nFigure 1.36: Input/output relation of a 3-bits quantizer with Δ = 0.5.\n\n\n\n\n1.8.5  Design of uniform quantizers\n\n\n\nThere are several strategies for designing a uniform quantizer. Some are discussed in the following paragraphs.\n\n\n\n\n\nDesigning a uniform quantizer based on input’s statistics\n\n\n\nIdeally, the chosen output levels in ℳ  should match the statistics of the input signal x[n]. A reasonable strategy is to observe the histogram (see Figure 1.46, for an histogram example) of the quantizer input and pick reasonable values to use in ℳ . For example, if the input data follows a Gaussian distribution, the sample mean μ and variance σ2 can be estimated and the dynamic range assumed to be Xmin = μ − 3σ and Xmax = μ + 3σ to have the quantizer covering approximately 99% of the samples.\n\n\n\nIn case the input data has approximately a uniform distribution, the quantizer can be designed based on the dynamic range [Xmin,Xmax] of its input x[n], where Xmin and Xmax are the minimum and maximum amplitude values assumed by x[n].\n\n\n\nOne should notice that outliers (a sample numerically distant from the rest of the data, typically a noisy sample) can influence too much a design strictly based on Xmin and Xmax. This suggests always checking the statistics of x[n] via a histogram.\n\n\n\n\n\nDesigning a uniform quantizer based on input’s dynamic range\n\n\n\nEven if the input signal does not have a uniform distribution, it is sometimes convenient to adopt the suboptimal strategy of taking into account only the input dynamic range\n\n[Xmin,Xmax] of x[n]. Among several possible strategies, a simple one is to choose X^min = Xmin and\n\n\n\n\n\n\n\n\n\n Δ =  |Xmax − Xmin M |. \n\n\n(1.27)\n\n\n\n\n\n\nIn this case, the minimum quantizer output is X^min = Xmin, but the maximum value does not reach Xmax and is given by X^max = Xmax −Δ. The reason is that, as indicated in Eq. (1.26), X^max = X^min + (M − 1)Δ.\n\n\n\nTo design a uniform quantizer in which X^max = Xmax, one can adopt\n\n\n\n\n\n\n\n\n\n Δ =  |Xmax − Xmin M − 1 |. \n\n\n(1.28)\n\n\n\n\n\n\n Example 1.28. Design of a uniform quantizer. For example, assume the quantizer should have b = 2 bits and the input x[n] has a dynamic range given by Xmin = −1 V and Xmax = 3 V. Using Eq. (1.27) leads to Δ = (3 − (−1))∕4 = 1 V and the quantizer output levels are M = {−1,0,1,2}.\nNote that X^max = Xmax −Δ = 3 − 1 = 2. Alternatively, Eq. (1.28) can be used to obtain Δ = (3 − (−1))∕(4 − 1) ≈ 1.33 V and the quantizer output levels would be M = {−1,0.33,1.66,3} with X^max = Xmax = 3.   □\n\n\n\n Example 1.29. Forcing the quantizer to have an output level representing “zero”. One common requirement when designing a quantizer is to reserve one quantization level to represent zero (otherwise, it would output a non-zero value even with no input signal). The levels provided by Eq. (1.28) can be refined by counting the number Mneg of levels representing negative numbers and adjusting X^min such that X^min + ΔMneg = 0. Listing 1.15 illustrates the procedure.\n\n\n\nListing 1.15: MatlabOctaveCodeSnippets/snip_signals_quantizer.m\n\n\nXmin=-1; Xmax=3; %adopted minimum and maximum values \nb=2; %number of bits of the quantizer \nM=2^b; %number of quantization levels \ndelta=abs((Xmax-Xmin)/(M-1)); %quantization step \n5QuantizerLevels=Xmin + (0:M-1)*delta %output values \nisZeroRepresented = find(QuantizerLevels==0); %is 0 there? \nif isempty(isZeroRepresented) %zero is not represented yet \n    Mneg=sum(QuantizerLevels&lt;0); %number of negative \n    Xmin = -Mneg*delta; %update the minimum value \n10    NewQuantizerLevels = Xmin + (0:M-1)*delta %new values \nend\n  \n\n\nConsidering again Example 1.28, Listing 1.15 would convert the original set M = {−1,0.33,1.66,3} into M = {−1.33,0,1.33,2.67}. Notice the new value of X^max is Xmax −Δ but one level is dedicated to represent zero.   □\n\n\n\n Example 1.30. Designing a quantizer for a bipolar input.\n\n\n\nAssume here a bipolar signal x[n], for instance with peak values Xmin = −5 and Xmax = 5). If it can be assumed that Xmax = |Xmin|, Eq. (1.28) simplifies to\n\n\n\n\n\n\n Δ = 2Xmax M − 1 . \n\n\n\n\n\n\nFurthermore, one quantization level can be reserved to represent zero, while M∕2 = 2b−1 and M∕2 − 1 = 2b−1 − 1 levels represent negative and positive values, respectively. Most ADCs adopt this division of quantization levels when operating with bipolar inputs. For example, several commercial 8-bits ADCs can output signed integers from  − 128 to 127, which corresponds to the integer range  − 2b−1 = −28−1 = −128 to 2b−1 − 1 = 27 − 1 = 127. These integer values can be multiplied by the quantization step Δ in Volts to convert the quantizer output into a value in Volts.\n\n\n\nListing 1.16 illustrates a Matlab/Octave function that implements a conventional quantizer with X^min = −2b−1Δ and saturation. Note that before the round function is invoked, the number of quantization levels is obtained by\n\n\n\n\n\n\n\n\n\n xi = x Δ, \n\n\n(1.29)\n\n\n\n\n\n\nand later xq = xi ×Δ.\n\n\n\nListing 1.16: MatlabOctaveFunctions/ak_quantizer.m\n\n\nfunction [x_q,x_i]=ak_quantizer(x,delta,b) \n% function [x_q,x_i]=ak_quantizer(x,delta,b) \n%This function assumes the quantizer allocates 2^(b-1) levels to \n%negative output values, one level to the \"zero\" and 2^(b-1)-1 to \n5%positive values. See ak_quantizer2.m for more flexible allocation. \n%The output x_i will have negative and positive numbers, which \n%correspond to encoding the quantizer's output with two's complement. \nx_i = x / delta; %quantizer levels \nx_i = round(x_i); %nearest integer \n10x_i(x_i &gt; 2^(b-1) - 1) = 2^(b-1) - 1; %impose maximum \nx_i(x_i &lt; -2^(b-1)) = -2^(b-1); %impose minimum \nx_q = x_i * delta;  %quantized and decoded output\n  \n\n\nListing 1.17: MatlabOctaveCodeSnippets/snip_signals_quantizer_use.m\n\n\ndelta=0.5; %quantization step \nb=3; %number of bits, to be used here as range -2^(b-1) to 2^(b-1)-1 \nx=[-5:.01:4]; %define input dynamic range \n[xq,x_integers] = ak_quantizer(x,delta,b); %quantize \n5plot(x,xq), grid  %generate graph\n  \n\n\nThe commands in Listing 1.17 can generate a quantizer input-output graph such as Figure 1.36 using the function ak_quantizer.m.    □\n\n\n\n\n\n1.8.6  Design of optimum non-uniform quantizers\n\n\n\nThe optimum quantizer, which minimizes the quantization error, must be designed strictly according to the input signal statistics (more specifically, the probability density function of x[n]). The uniform quantizer is the optimum only when the input distribution is uniform. For any other distribution, the Lloyd’s algorithm10 can be used to find the optimum set ℳ  of output levels and the quantizer will be non-uniform. For instance, if the input is Gaussian, the optimum quantizer allocates a larger number of quantization levels around the Gaussian mean than in regions far from the mean.\n\n\n\n Example 1.31. Optimum quantizer for a Gaussian input. This example illustrates the design of an optimum quantizer when the input x[n] has a Gaussian distribution with variance σ2 = 10. It is assumed that the number b of bits is three such that the quantizer has M = 23 = 8 output levels. The following code illustrates the generation of x[n] and quantizer designer using Lloyd’s algorithm.\n\n\nclf \nN=1000000; %number of random samples \nb=3; %number of bits \nvariance = 10; \n5x=sqrt(variance)*randn(1,N); %Gaussian samples \nM=2^b; \nnumBins=100; %number of bins \n[partition,codebook] = lloyds(x,M); %design the quantizer\n\n\nThe obtained results are listed in Table 1.6 and Figure 1.37.\n\n\n\n\n\n\n\nTable 1.6: Input/output mapping for a generic quantizer designed for a Gaussian input with variance σ2 = 10.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInput range \n\n\nOutput level \n\n\n\n\n\n\n\n\n\n\n\n\n\n] −∞,−5.5[\n\n\n\n−6.8\n\n\n\n\n\n\n\n\n\n\n\n\n\n[−5.5,−3.3[\n\n\n\n−4.2\n\n\n\n\n\n\n\n\n\n\n\n\n\n[−3.3,−1.6[\n\n\n\n−2.4\n\n\n\n\n\n\n\n\n\n\n\n\n\n[−1.6,0[\n\n\n\n−0.8\n\n\n\n\n\n\n\n\n\n\n\n\n\n[0,1.6[\n\n\n\n0.8\n\n\n\n\n\n\n\n\n\n\n\n\n\n[1.6,3.3[\n\n\n\n2.4\n\n\n\n\n\n\n\n\n\n\n\n\n\n[3.3,5.5[\n\n\n\n4.2\n\n\n\n\n\n\n\n\n\n\n\n\n\n[5.5,∞[\n\n\n\n6.8\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                              &lt;/div&gt;\n                                                                          \n                                                                          \n\n\n\nNote that the two intervals close to 0 have length 1.6 while the interval [3.3,5.5[ has a longer length 5.5 − 3.3 = 2.2. In general, the intervals corresponding to regions with less probability are longer such that more output levels can be concentrated in regions of high probability. This is illustrated in Figure 1.37.\n\n\n\n\n\n\n\n\nFigure 1.37: Theoretical and estimated Gaussian probability density functions with thresholds represented by dashed lines and the output levels indicated with circles in the abscissa.\n\n\n\n\n\nBecause Lloyd’s algorithm has a set of samples as input, this number has to be large enough such that the input distribution is properly represented. In this example, N=1000000 random samples were used.\n\n\n\n\n\n\n\n\nFigure 1.38: Input/output mapping for the quantizer designed with a Gaussian input and outputs given by ℳ = [−6.8,−4.2,−2.4,−0.8,0.8,2.4,4.2,6.8].\n\n\n\n\n\nFigure 1.37 depicts the mapping for the designed non-uniform quantizer.    □\n\n\n\n Example 1.32. Optimum quantizer for a mixture of two Gaussians. This example is similar to Example 1.31, but the input distribution here is a mixture of two Gaussians instead of a single Gaussian. More specifically, the input x[n] has a probability density function given by\n\n\n\n\n\n\n\n\n\n f(x) = 0.8N(−4,0.5) + 0.2N(3,4), \n\n\n(1.30)\n\n\n\n\n\n\nwhere the notation N(μ,σ2) describes a Gaussian with average μ and variance σ2.\n\n\n\n                                   &lt;div class=\"center\" \n\n\n\n\n\n\n\n\n\n\n\n\n(a) Thresholds (dashed lines) and output levels (circles).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Input/output mapping."
  },
  {
    "objectID": "ak_dsp_bookse9.html",
    "href": "ak_dsp_bookse9.html",
    "title": "9  Signal Categorization",
    "section": "",
    "text": "This section discusses important categories of signals and their distinctive properties.\n\n\n\n\n\n1.9.1  Even and odd signals\n\n\n\n\n\n\nA signal x[n] is called even if x[n] = x[−n], and odd if x[n] = −x[−n]. The definitions are also valid for a continuous-time signal and, in general, for any function f(x). An even function f(x) has the property that f(x) = f(−x), while an odd function has the property that f(x) = −f(−x). For instance, the cosine is an even function while the sine is an odd function.\n\n\n\nInterestingly, any function can be decomposed into even fe(x) and odd fo(x) parts, such that f(x) = fe(x) + fo(x). The two component parts can be obtained as\n\n\n\n\n\n\n fe(x) = f(x) + f(−x) 2     and    fo(x) = f(x) − f(−x) 2 . \n\n\n\n\n\n\nSimilarly, any signal x[n] (or x(t)) can be obtained\nas the sum of an even xe[n] and odd xo[n] parcels, which can be found as follows:\n\n\n\n\n\n\n\n\n\n xe[n] = 0.5(x[n] + x[−n]) \n\n\n(1.32)\n\n\n\n\n\n\nand\n\n\n\n\n\n\n\n\n\n xo[n] = 0.5(x[n] − x[−n]), \n\n\n(1.33)\n\n\n\n\n\n\nrespectively, such that x[n] = xe[n] + xo[n].\n\n\n\nFor example, assume the unit step function x(t) = u(t). Its even and odd parts are xe(t) = 0.5,∀ ⁡t and xo(t) = 0.5u(t) − 0.5u(−t), respectively.\n\n\n\nThe function ak_getEvenOddParts.m can be used to obtain the even and odd components of arbitrary finite-duration sequences. Three examples are provided in Figure 1.42, Figure 1.43 and Figure 1.44.\n\n\n\n\n\n\n\n\nFigure 1.42: Even and odd components of a signal x[n] representing a finite-duration segment of the step function u[n]. Note the symmetry properties: xe[n] = xe[−n] and xo[n] = −xo[−n].\n\n\n\n\n\n\n\n\n\n\nFigure 1.43: Even and odd components of a signal x[n] = n2u[n] representing a parabolic function.\n\n\n\n\n\n\n\n\n\n\nFigure 1.44: Even and odd components of a signal x[n] representing a triangle that starts at n = 21 and has its peak with an amplitude x[60] = 40 at n = 60. Note that the peak amplitude of the two components is 20.\n\n\n\n\n1.9.2  Random signals and their generation\n\n\n\nRandom signals are important to represent noise or other non-deterministic signals. Discrete-time random signals of finite duration can be represented by vectors in which the elements are outcomes of random variables (see Appendix B.19). For example, assuming all elements of [2, 0, 2, 3, 3, 2, 3] are outcomes of the same random variable X, one can calculate the average 𝔼[X] ≈ 2.14, the standard deviation σ ≈ 1.07 and other statistical moments of X.\n\n\n\nAlternatively, a vector with random samples may correspond to a realization of a discrete-time random process (see Appendix B.20).\n\n\n\n Example 1.38. Random Gaussian signals: generation, waveform and histogram. It is easy to generate random signals in Matlab/Octave. For example, the command x=randn(1,100) generates 100 samples distributed according to a standard Gaussian (zero-mean and unity-variance) distribution N(0,1), where the notation N(μ,σ2) assumes the second argument is the variance σ2, not the standard deviation σ. These signals can be visualized as a time-series,12 such as in Figure 1.45.\n\n\n\n\n\n\n\n\nFigure 1.45: Waveform representation of a random signal with 100 samples draw from a Gaussian distribution N(0,1).\n\n\n\n\n\n\n\n\n\n\nFigure 1.46: Histogram of the signal in Figure 1.45 with 10 bins.\n\n\n\n\n\nThe time-domain visualization can be complemented by plotting the probability distribution of the random signal. Figure 1.46 illustrates the histogram of the signal depicted in Figure 1.45. The histogram is calculated by dividing the dynamic range in bins and counting the number of samples that belong to each bin. Figure 1.46 was obtained by using 10 bins (the default).   □\n\n\n\nThe histogram indicates the number of occurrences of the input data within the ranges (or bins) indicated in its abscissa. The histogram is also very useful to estimate a probability mass function (PMF) and probability density function (PDF) from available data, which are used for discrete and continuous random variables, respectively.\n\n\n\n Example 1.39. Using a normalized histogram as an estimate of the probability mass function. Even when the elements of the input vector x are real values, the histogram calculation “quantizes” these input values into only B values, which represent the ranges of the histogram bins. In other words, the bin centers can be seen as the result of a quantization process. Besides peforming this “quantization”, the histogram also indicates the number of occurrences of these B values in x.\n\n\n\nThe B center bins of a histogram can be interpreted as the possible distinct values of a discrete random variable. Therefore, normalizing the histogram by the total number N of samples provides an estimate of the PMF of this discrete random variable.\n\n\n\nFor instance, suppose a vector x with N=100 elements. Calculating the histogram and dividing the number of occurrences by N provides an estimate of the PMF of x. In Matlab/Octave, after obtaining the histogram with [occurrences, bin_centers]=hist(x), the PMF can be estimated using stem(bin_centers, occurrences/N).    □\n\n\n\nNormalizing the histogram by the total number N of samples provides an estimate of the PMF of a discrete random variable, with values represented by centers of the histogram bins. When this result is further normalized by the bin width, an estimate of the PDF is obtained, as discussed in the next example.\n\n\n\n Example 1.40. Using a normalized histogram as an estimate of the probability density function. Sometimes we know the input data x is composed of realizations of a continuous random variable with a known PDF. When the task is to superimpose a PDF estimated from data to the actual PDF, one needs to properly normalize the histogram (see Appendix B.13).\n\n\n\nThe function ak_normalize_histogram.m can be used to estimate a PDF from a histogram and was used to obtain Figure 1.47 according to the commands in Listing 1.18.\n\n\n\nListing 1.18: MatlabOctaveCodeSnippets/snip_signals_estimate_pdf.m\n\n\nB=10; %number of bins \nx=randn(1,100); %random numbers ~ G(0,1) \n[n2,x2]=ak_normalize_histogram(x,B);%PDF via normalized histogram \na=-3:0.1:3; %use range of [-3std, 3std] around the mean \n5plot(x2,n2,'o-',a,normpdf(a),'x-') %estimate vs. theoretical PDF\n  \n\n\n\n\n\n\n\nFigure 1.47: PDF estimate from the histogram in Figure 1.46. The histogram values were divided by the product of the total number of samples and bin width. A standard Gaussian PDF is superimposed for the sake of comparison.\n\n\n\n\n\nFigure 1.47 indicates that 100 samples and 10 bins provide only a crude estimation.    □\n\n\n\n Example 1.41. Changing the mean and variance of a random variable. Assume a random variable X has a mean η = 𝔼[X] and N of its realizations compose a vector x. If one wants to create a new random variable Y with 𝔼[Y] = η + 3, this can be done with Y = X + 3, ou using the realizations: y=x+3. This is valid for any constant value κ.\n\n\n\nTo prove it, observe that the expected value is a linear operator (see, Appendix B.19.3), such that when Y = X + κ, one has\n\n\n\n\n\n\n\n\n\n 𝔼[Y] = 𝔼[X + κ] = 𝔼[X] + 𝔼[κ] = η + κ. \n\n\n(1.34)\n\n\n\n\n\n\nSimilarly, if the variance of X is σx2, a random variable Y = κX has variance σy2 = κσx2. The proof is based on Eq. (B.66) and linearity:\n\n\n\n σy2  = 𝔼[(Y − 𝔼[Y])2] = 𝔼[Y2] − 𝔼[Y]2 = 𝔼[(κX)2] − (𝔼[κX])2 = κ(𝔼[X2] − (𝔼[X])2)     = κσx2. (1.35)   \n\nTake now the particular example of the function randn in Matlab/Octave, which generates realizations of a random variable X that is distributed according to the normalized Gaussian N(0,1). Based on Eq. (1.34) and Eq. (1.35), if one creates a new random variable Y = σX + η, the mean and variance of Y are η and σ2, respectively.\n\n\n\nConsidering Matlab/Octave, the command x=sqrt(newVariance)*randn(1,N)+newMean provides Gaussians with arbitrary mean and variance.\n\n\n\nListing 1.19 was used to generate the samples of N(4,0.09) and illustrates how to draw samples from a Gaussian with any given mean and variance from calls to a random number generator that outputs samples from a standard Gaussian N(0,1).\n\n\n\nListing 1.19: MatlabOctaveCodeSnippets/snip_signals_gaussian_rand_gen.m\n\n\nnewMean=4; %new mean \nnewVariance=0.09; %new variance \nN=10000; %number of random samples \nx=sqrt(newVariance)*randn(1,N)+newMean;\n  \n\n\n\n\n\n\n\nFigure 1.48: Comparison of normalized histogram and the correct Gaussian N(4,0.09) when using 10,000 samples and 100 bins. Note the likelihood can be larger than one because it is not a probability.\n\n\n\n\n\nFigure 1.48 was obtained by using 10 thousand samples from a Gaussian N(4,0.09) and using 100 bins for the histogram. Now the Gaussian estimation is relatively good when compared to the one depicted in Figure 1.47.\n\n\n\nIt should be noted that the normalized histogram of a continuous PDF indicates likelihood, not probability. The likelihood function is the PDF viewed as a function of the parameters. Therefore, it is possible to have values larger than one in the ordinate, such as in Figure 1.48.    □\n\n\n\nWhile the previous example assumed a Gaussian PDF, the next one deals with a uniform PDF.\n\n\n\n Example 1.42. Calculating and changing the mean and variance of a uniform probability density function. A random variable X distributed according to a uniform PDF with support [a,b] (range from a to b of values that have non-zero likelihood) has a mean given by\n\n\n\n\n\n\n\n\n\n μ = 𝔼[X] = (a + b)∕2 \n\n\n(1.36)\n\n\n\n\n\n\nand variance:\n\n\n\n\n\n\n\n\n\n σ2 = 𝔼[(X − μ)2] = (b − a)2∕12 \n\n\n(1.37)\n\n\n\n\n\n\nEq. (1.37) can be proved observing that the uniform PDF fX(x) is a constant 1∕(b − a) over the range [a,b] and zero otherwise. Hence, using Eq. (B.68) with the function g(X) = g((X − μ)2) leads to\n\n\n\n\n\n\n\n\n\n σ2 = 𝔼[(X − μ)2] = ∫ −∞∞f X(x)(x − μ)2dx = ∫ ab 1 b − a  (x − (a + b) 2 )2dx = S2 12, \n\n\n(1.38)\n\n\n\n\n\n\nwhere S = b − a is the PDF support.\n\n\n\nWhen using the random number generator rand for uniformly distributed samples, one should notice that the dynamic range is [0,1], i. e., a = 0 and b = 1. Hence, Eq. (1.36) indicates the mean is 0.5 and the variance given by Eq. (1.37) is S2∕12 = 1∕12 ≈ 0.0833.\n\n\n\nDrawing from rand, it is possible to generate N samples uniformly distributed in an arbitrary range [a,b] with the command x = a + (b-a) * rand(N,1).   □\n\n\n1.9.3  Periodic and aperiodic signals\n\n\n\n\n\nPeriodicity in continuous-time\n\n\n\nA signal x(t) (same discussion applies to x[n]) is periodic if a given segment of x(t) is\neternally repeated, such that x(t) = x(t + T) for some T &gt; 0, where T is called the period. For example, if T = 10 seconds and x(t) = x(t + 10), for all values of t.\n\n\n\nIn the example of T = 10, it is easy to check that x(t) = x(t + 20), x(t) = x(t + 30) and so on. In other words, a signal with period T is also periodic in 2T,3T,…. The fundamental period T0 is the smallest value of T for which x(t) = x(t + T0). Note that the definition imposes T0 &gt; 0 and a constant signal x(t) = κ is not considered periodic.\n\n\n\n Example 1.43. Using the LCM and GCD for a periodic signal composed by commensurate frequencies. Two frequencies f1 and f2 are called commensurate if their ratio f1∕f2 can be written as a rational number m∕n, where m,n are non-zero integers. Instead of frequencies, one can use their associated time periods.\n\n\n\nAssume a signal x(t) = ∑ ⁡ i=1Nxi(t) is composed by the sum of N periodic components xi(t), each one with period Ti and frequency fi = 1∕Ti. The set of frequencies {fi} is commensurate if all pairs are commensurate. In this case, the fundamental period T0 of x(t) can be found using the least common multiple (LCM) of the periods {Ti} while the fundamental frequency F0 = 1∕T0 can be found using the greatest common divisor (GCD) of the frequencies {fi}. Assuming both LCM and GCD are defined only for integer numbers, it may be needed to extract a common factor and later reintroduce it. A numerical example helps: let x(t) = cos ⁡ (2πf1t) + cos ⁡ (2πf2t + π∕2) + sin ⁡ (2πf3t) be composed by sinusoids\nwith frequencies f1 = 5∕2, f2 = 1∕6 and f3 = 1∕8 Hz, which corresponds to periods T1 = 0.4, T2 = 6 and T3 = 8 seconds, respectively. To find the LCM, one may need to multiply all periods by 10 and then calculate that LCM(4,60,80) = 240. Dividing this result by the factor 10 leads to T0 = 24 s. This LCM could be obtained in Matlab/Octave with lcm(lcm(4,60),80) given that this function is limited to accepting only two input arguments.    □\n\n\n\n\n\nPeriodicity of a generic discrete-time signal\n\n\n\nA discrete-time signal is periodic if x[n] = x[n + N0] for some integer N0 &gt; 0. Similar to the continuous-time case, the value N0 is called the fundamental period if it corresponds to the minimum number of samples in which the amplitudes repeat.\n\n\n\n\n\nPeriodicity of discrete-time sinusoids\n\n\n\nOne important thing is that the discrete-time counterpart of some periodic analog signals may be non-periodic. The next paragraphs discuss the periodicity of discrete-time sinusoids such as cos ⁡ ((π∕4)n), sin ⁡ (3n) and eπn. For example the signal x(t) = cos ⁡ (3t) is periodic with period T = 2π∕3 s. However, the discrete-time signal x[n] = cos ⁡ (3n) is non-periodic.\n\n\n\nA discrete-time sinusoid such as x[n] = Acos ⁡ (Ωn + ϕ) is periodic only if Ω∕(2π) is a ratio m∕N0 of two integers m and N0 as proved below.13 One can write:\n\n\n\n\n\n\n x[n + N0] = Acos ⁡ (Ω(n + N0) + ϕ) = Acos ⁡ (Ωn + ΩN0 + ϕ). \n\n\n\n\n\n\nIf the parcel ΩN0 in previous expression is a multiple of 2π, then x[n + N0] = x[n],∀ ⁡n. Hence, periodicity requires ΩN0 = 2πm, which leads to the condition\n\n\n\n\n\n\n\n\n\n  m N0 = Ω 2π \n\n\n(1.39)\n\n\n\n\n\n\nfor a discrete-time sinusoid to be periodic.\n\n\n\n Example 1.44. Checking the periodicity of discrete-time sinusoids. For example, x[n] = cos ⁡ (3n) is non-periodic because Ω = 3 and 3∕(2π) cannot be written as a ratio of two integers. In contrast, x[n] = cos ⁡ ((2π∕8)n + 0.2) is periodic with period N0 = 8 (m = 1 in this case). The signal x[n] = cos ⁡ (7πn) is periodic because Ω = 7π and Ω∕(2π) = 7∕2, with m = 7 and N0 = 2.    □\n\n\n\nIf m∕N0 in Eq. (1.39) is an irreducible fraction, then N0 is the fundamental period. Otherwise, N0 may be a multiple of the fundamental period.\n\n\n\n Example 1.45. Finding the fundamental period requires reducing the fraction m∕N0. For instance, the signal x[n] = cos ⁡ ((12π∕28)n) is periodic because Ω = (12π∕28) and Ω∕(2π) = 6∕28, with m = 6 and N0 = 28. However, if one is interested on the fundamental period, it is necessary to reduce the fraction 6∕28 to 3∕14, and obtain the fundamental period as N0 = 14 samples.   □\n\n\n\nIn summary, when contrasting continuous and discrete-time sinusoids, to find the period T of a continuous-time cosine cos ⁡ (ωt + ϕ), one can obtain the term ω that multiplies t and calculate\n\n\n\n\n\n\n T = 2π ω , \n\n\n\n\n\n\nwhich is given in seconds if ω is in rad/s. Hence, a continuous-time sinusoid is always periodic. But a discrete-time cosine cos ⁡ (Ωn + ϕ) may be quasi periodic (i. e., not periodic). If someone tries to simply calculate N0 = 2π Ω , it may end up with a non-integer period. The condition for periodicity is to be able to write 2π∕Ω as a ratio of integers, i. e.\n\n\n\n\n\n\n N0 m = 2π Ω , \n\n\n\n\n\n\nwhere N0 is the period in samples. After turning an N0∕ an irreducible fraction, N0 is the fundamental period.\n\n\n\n Example 1.46. Meaning of m when determining the fundamental period. To understand the role of m, consider the signal cos ⁡ ((3π∕17)n). In this case, 2π Ω = 34∕3 cannot be the period because it is not an integer. However, if one allows for m = 3 times the number of samples specified by 2π Ω , the result is the integer period N0 = m2π Ω = 34 samples. See Application 1.6 for a discussion on finding m and N0 using Matlab/Octave.\n\n\n\nSometimes, it is misleading to guess the period of a discrete-time cosine or sine via the observation of its graph. Figure 1.49 depicts the graph of x[n] = sin ⁡ (0.2n) and was obtained with the following code:\n\n\nM=100, w=0.2; %%num of samples and angular freq. (rad) \nn=0:M-1; %generate abscissa \nxn=sin(w*n); stem(n,xn); %generate and plot a sinusoid\n\n\nIn this case, the signal seems to have a period around 31 samples at a first glance (because 2π∕Ω ≈ 31.4). But, for example, x[n] will never be 0 at the beginning of a cycle for a value of n other than n = 0. Therefore, in spite of resembling a periodic signal, the angular frequency Ω is such that 2π∕Ω = 10π is a irrational number and a cycle of numbers will never repeat. In this case, the signal is called almost or quasi\nperiodic.14\n\n\n\n\n\n\n\n\nFigure 1.49: Graph of the signal x[n] = sin ⁡ (0.2n). Observe carefully that this signal is not periodic. The first non-negative sample of the sine cycle will never exactly repeat its value, as indicated by the ‘x’ marks.\n\n\n\n\n\nIt is useful to visualize a discrete-time sinusoid that is periodic with m &gt; 1. Figure 1.50 depicts the graph of x[n] = sin ⁡ ((3π∕17)n) and illustrates the repetition of m = 3 sine envelopes within a period of N0 = 34 samples.\n\n\n\n\n\n\n\n\nFigure 1.50: Graph of the signal x[n] = sin ⁡ ((3π∕17)n). The signal has period N0 = 34 samples as indicated by the combined marks ‘x’ and ‘o’. This value of N0 corresponds to m = 3 cycles of a sine envelope corresponding to 2π∕Ω = 34∕3 ≈ 11.3.\n\n\n\n\n\nFigure 1.49 and Figure 1.50 illustrate how to distinguish strictly periodic sinusoids from quasi periodic, and help interpreting m in Eq. (1.39).   □\n\n\n1.9.4  Power and energy signals\n\n\n\nIt is important to understand the concepts of power P and energy E of a signal. One reason is that, in some cases, the equation to be used for a specific analysis (autocorrelation, for example) differs depending if P or E are not finite. This section assumes continuous-time but the concepts are also valid for discrete-time signals.\n\n\n\nIf E is the energy dissipated by a signal during a time interval Δt, the average power along Δt is\n\n\n\n\n\n\n P = E Δt. \n\n\n\n\n\n\nIf p(t) = |x(t)|2 is the instantaneous power of x(t), E can be calculated as\n\n\n\n\n\n\n\n\n\n E = ∫ ⟨Δt⟩p(t)dt. \n\n\n(1.40)\n\n\n\n\n\n\nIf the interval Δt is not specified, it is implicitly assumed (by default) the whole time axis ] −∞,∞[ and\n\n\n\n\n\n\n E = ∫ −∞∞p(t)dt. \n\n\n\n\n\n\nIn this case, P is defined as the limit\n\n\n\n\n\n\n\n\n\n P = lim ⁡  Δt→∞  [ 1 Δt∫ −Δt∕2Δt∕2p(t)dt]. \n\n\n(1.41)\n\n\n\n\n\n\nNote that, because the time interval goes to infinite (denominator), P is zero unless the energy E (numerator) also goes to infinite. This situation suggests the definition of power and energy signals, which have finite power and energy, respectively. Their characteristics are\nsummarized in Table 1.8, which also indicates that there is a third category for signals that have neither finite power or energy.\n\n\n\n\n\n\n\nTable 1.8: Total energy E and average power P for two kinds of signal assuming an infinite time interval.\n\n\n\n\n\n\n\n\n\n\n\n\nCategory \n\n\n\nE\n\n\n\nP\n\n\n\n\nExample(s)\n\n\n\n\n\nPower signal \n\n\n\n∞\n\n\nfinite \n\n\n\n\n\ncos ⁡ (ωt) and other periodic signals\n\n\n\n\n\nEnergy signal \n\n\nfinite \n\n\n0 \n\n\n\n\n\ne−tu(t) and t2[u(t)−u(t−5)]\n\n\n\n\n\nNeither \n\n\n\n∞\n\n\n\n∞\n\n\n\n\n\nt\n\n\n\n\n\n\n\n\nThe most common power signals are periodic. In this case, the energy ET  in one period T\n\n\n\n\n\n\n ET  = ∫ ⟨T⟩p(t)dt \n\n\n\n\n\n\ncan be used to easily calculate\n\n\n\n\n\n\n P = ET  T \n\n\n\n\n\n\nbecause what happens in one period is replicated along the whole time axis.\n\n\n\nThe most common energy signals have a finite duration, such as x(t) = t2[u(t) − u(t − 5)]. Assuming the signals have finite amplitude, their energy in a finite time interval cannot be infinite. Note that infinite duration signals, such as x(t) = e−tu(t), can also have a finite energy in case their amplitude decay over time.\n\n\n\nIt is assumed throughout this text that the signals are currents i(t) or voltages v(t) over a resistance R, such that the instantaneous power is\n\n\n\n\n\n\n p(t) = v(t)i(t) = 1 Rv2(t) = i2(t)R. \n\n\n\n\n\n\nBesides, to deal with signals x(t) representing both currents and voltages without bothering about the normalization by R, it is assumed that R = 1 Ohm. Hence, the instantaneous power is p(t) = x2(t) for any real x(t) and, more generally, p(t) = |x(t)|2 in case x(t) is complex-valued.\n\n\n\nThroughout the book, unless stated otherwise, x(t) is assumed to be in Volts, p(t) and P in Watts and E in Joules. A dimensional analysis of p(t) = x2(t) should not be interpreted directly as Watts = Volts2, but Watts = Volts2/Ohm, where the normalization by 1 Ohm is implicit. Two examples are provided in the sequel.\n\n\n\n Example 1.47. Sinusoid power. Sinusoids and cosines can be represented by x(t) = Acos ⁡ (ω0t + 𝜃) and are power signals with average power P = A2 2 . The phase 𝜃 does not influence the power calculation. The proof follows.\n\n\n\nThe angular frequency is ω0 = 2π T  rad/s, where T is the period in seconds.\n\n\n\n\n\n\n ET  = ∫ ⟨T⟩p(t)dt = ∫ ⟨T⟩x2(t)dt = A2 ∫ ⟨T⟩cos ⁡ 2(ω 0t + 𝜃)dt. \n\n\n\n\n\n\nUsing the identity cos ⁡ 2a = 1 2(cos ⁡ (2a) + 1) (see Appendix):\n\n\n\n\n\n\n ET  = A2 2 ∫ ⟨T⟩(cos ⁡ (2ω0t + 2𝜃) + 1)dt = A2T 2 . \n\n\n\n\n\n\nThe first parcel of the integral is zero, independent of 2𝜃 because T corresponds exactly to two periods of the cosine with angular frequency 2ω0, while the second parcel is T. The average power is\n\n\n\n\n\n\n\n\n\n P = ET  T = A2 2 , \n\n\n(1.42)\n\n\n\n\n\n\nwhich is a result valid for any sinusoid or cosine. This discussion assumed continuous-time signals, but Eq. (1.42) is also valid for discrete-time sinusoids.    □\n\n\n\n Example 1.48. Power of a DC signal. A constant signal x(t) = K (i. e., a DC signal) has power P = K2 because the energy at any interval Δt is E = K2Δt.    □\n\n\n\nThe root-mean-square (RMS) value xrms of any signal x(t) is the DC value that corresponds to the same power P of x(t), i. e., xrms2 = P or, equivalently, xrms = P. For example, the RMS value of a cosine x(t) = Acos ⁡ (ω0t + 𝜃) is xrms = A 2 because a DC signal y(t) = A 2 has the same average power as x(t).\n\n\n\nAs discussed in Section B.26.4, δ(t) is a distribution and it is tricky to define the energy or power of a sampled signal, which is the topic of Section 3.5.2.\n\n\n \n\n12 As usually done for signals with many samples, the discrete-time x[n] was depicted as a continuous-time signal with the plot function instead of stem.\n\n \n\n13 When m∕N0 is not a rational number, the discrete-time sinusoid is called almost-periodic  [?,?].\n\n \n\n14 See, e. g., [?,?] to see the importance of almost periodic signals in random processes.\n\n           &lt;/div&gt;"
  },
  {
    "objectID": "ak_dsp_bookse10.html",
    "href": "ak_dsp_bookse10.html",
    "title": "10  Power and Energy in Discrete-Time",
    "section": "",
    "text": "The concepts of power and energy are better defined for continuous-time than for discrete-time signals and vectors. Dealing with the concept of power of a discrete-time signal x[n] requires some caution because n does not have the dimension of time. In contrast to x[n], the notation xi is used when i should not be interpreted as “time” but, for example, as the i-th element of a set.\n\n\n\n\n\n1.10.1  Power and energy of discrete-time signals\n\n\n\nThe following definition of average power will be adopted in this text\n\n\n\n\n\n\n\n\n\n P ≜ lim ⁡  N→∞ 1 2N + 1∑ n=−NN|x[n]|2. \n\n\n(1.43)\n\n\n\n\n\n\nand interpreted as Watts given that x[n] is assumed to be in Volts. This is a sensible definition, as will be discussed in Section 1.11.\n\n\n\nAccordingly, the energy E of a discrete-time signal is\n\n\n\n\n\n\n\n\n\n E ≜∑ n=−∞∞|x[n]|2. \n\n\n(1.44)\n\n\n\n\n\n\nsuch that P = E∕N when the support of x[n] are N samples.\n\n\n\nA similar situation occurs when dealing with vectors.\n\n\n\n\n\n1.10.2  Power and energy of signals represented as vectors\n\n\n\nHere, x[n] denotes the n-th element of a vector x. The order of these elements is assumed to correspond to a time evolution, such that a finite-dimension vector is equivalent to a finite-duration discrete-time sequence. Hence, the energy E of vector x is its squared Euclidean norm:\n\n\n\n\n\n\n\n\n\n E = ∑ n=1N|x[n]|2 = ||x||2, \n\n\n(1.45)\n\n\n\n\n\n\nwhere N is the dimension of x. Accordingly, the power of such finite-dimension vector is\n\n\n\n\n\n\n\n\n\n P = E N = 1 N∑ n=1N|x[n]|2. \n\n\n(1.46)\n\n\n\n\n\n\n\n\n1.10.3  Power and energy of vectors whose elements are not time-ordered\n\n\n\nIn contrast to the previous equations, there are cases in which the vector elements are not indexed according to a time evolution.\n\n\n\nIt should be noticed that the average of the squared norms of several vectors should be interpreted as their average energy, not power. For example, assuming there are M vectors x1,…,xM of dimension N, an average energy is obtained with\n\n\n\n\n\n\n\n\n\n E¯ = 1 M∑ i=1ME i = 1 M∑ i=1M||x i||2, \n\n\n(1.47)\n\n\n\n\n\n\nand interpreted in Joules.\n\n\n\nEq. (1.46) and Eq. (1.47) are similar due to the connection between vectors and finite-duration discrete-time signals. The distinction that allows to observe their results as “power” or “energy” relies on interpreting the index\n\ni as “time” or not. In Eq. (1.46), an average of instantaneous power values |x(i)|2 along the “time” i leads to an estimate of power. But when taking an average E¯ over vectors of energy Ei in Eq. (1.47), the result is (average) energy.\n\n\n\nTo make sure the concepts are clear, applying Eq. (1.46) to each vectors xi and taking the average of their power Pi leads to\n\n\n\n\n\n\n P = 1 M∑ i=1MP i = 1 MN∑ i=1M ∑ j=1N|x i(j)|2 = E¯ N. \n\n\n\n\n\n\n\n\n1.10.4  Power and energy of discrete-time random signals\n\n\n\nIf x[n] represents a random signal, with samples x[n0] corresponding to outcomes of a random variable X, an alternate definition is\n\n\n\n\n\n\n P ≜ 𝔼[|X|2] = 𝔼[|x[n]|2], \n\n\n\n\n\n\nwhere 𝔼[⋅] is the expectation operator.\n\n\n\nThe power of a random signal x[n] (or x(t)) can be decomposed into two parcels as follows:\n\n\n\n\n\n\n\n\n\n P = 𝔼[X2] = σ x2 + μ x2, \n\n\n(1.48)\n\n\n\n\n\n\nwhere the variance σx2 and the squared-mean μx2 correspond to the powers of the AC and DC components of a real x[n], respectively. The proof is derived in Eq. (B.66).\n\n\n\nMost of the signals in telecommunications and other applications have zero mean (μ = 0). In this case, Eq. (1.48) shows that the power P = σx2 coincides with the variance of the random signal and the standard deviation σx with its RMS value."
  },
  {
    "objectID": "ak_dsp_bookse11.html",
    "href": "ak_dsp_bookse11.html",
    "title": "11  Relating Power in Continuous and Discrete-Time",
    "section": "",
    "text": "The goal here is to relate the power of a discrete-time x[n] to the power of a continuous-time x(t) where these signals are related by an A/D or D/A conversion. A possible processing chain relating these signals with their associated power in parenthesis is:\n\n\n\n\n\n\n\n\n\n x(t)(Pc)→ sampling →xs(t)→ S/D →x[n](Pd) = x(nTs). \n\n\n(1.49)\n\n\n\n\n\n\nAnother processing chain of interest is the reconstruction of a continuous-time signal:\n\n\n\n\n\n\n\n\n\n x(nTs) = x[n](Pd)→ D/S →xs(t)→ h(t) →x(t)(Pc) \n\n\n(1.50)\n\n\n\n\n\n\nwhich will be discussed in Section 3.5.7.\n\n\n\nSpecial interest lies on systems that have equivalence between power in discrete and continuous-time, such that\n\n\n\n\n\n\n\n\n\n Pd = Pc, \n\n\n(1.51)\n\n\n\n\n\n\nwhere Pd and Pc are given by Eq. (1.43) and Eq. (1.41), respectively.15\n\n\n\nEq. (1.51) is further discussed in Section 3.5.3, but here it is derived16 using the rectangle method to approximate the integral of p(t) = |x(t)|2 as a sum of rectangles with bases Ts and heights |x[n]|2, as follows:\n\n\n\n Pc  = lim ⁡  N→∞  [ 1 (2N + 1)Ts ∫ −NTsNTs |x(t)|2dt]     ≈ lim ⁡  N→∞  [ 1 (2N + 1)Ts ∑ n=−NNT s|x(nTs)|2]     = lim ⁡  N→∞  [ 1 (2N + 1)∑ n=−NN|x[n]|2]     = Pd. (1.52)  \n\nIn summary, when the sampling theorem is obeyed, the signal processing chains (filtering, amplification, etc.) associated to the A/D and D/A processes are assumed here not to alter the power of x(t) and x[n], such that Eq. (1.51) holds.\n\n\n \n\n15 This is not the same, but similar to the energy-conservation property of Fourier transforms (e. g., Eq. (B.51), or its version for periodic signals Eq. (B.52)).\n\n \n\n16 Eq. (1.51) can also be obtained by assuming the zero-order hold (ZOH) reconstruction of Figure 1.23.\n\n                                                                                                              &lt;/div&gt;"
  },
  {
    "objectID": "ak_dsp_bookse12.html",
    "href": "ak_dsp_bookse12.html",
    "title": "12  Correlation: Finding Trends",
    "section": "",
    "text": "It is often useful to infer whether or not random variables are related to each other via correlation. For example, height and weight are quantities that present positive correlation for the human population. However, happiness and height are uncorrelated. There are measures other than correlation to detect similarities among variables, such as the mutual information, but correlation is simple and yet very useful. Correlation is called a second-order statistics because it considers only pairwise or quadratic dependence. It does not detect non-linear relations nor causation, that is, correlation cannot deduce cause-and-effect relationships. But correlation helps to indicate trends such as one quantity increasing when another one decreases.\n\n\n\nThe correlation C for two complex-valued random variables X and Y is defined as\n\n\n\n\n\n\n C = cor(X,Y) = 𝔼[XY∗], \n\n\n\n\n\n\nwhere ∗ denotes the complex conjugate. The correlation coefficient, which is a normalized version of the covariance (not correlation), is defined as\n\n\n\n\n\n\n ρ = cov(X,Y) σxσy , \n\n\n\n\n\n\nwhere the covariance is given by\n\n\n\n\n\n\n\n\n\n cov(X,Y) = 𝔼[(X − μx)(Y − μy)∗]. \n\n\n(1.53)\n\n\n\n\n\n\nNote that, when one wants to fully identify the statistical moments of two complex random variables, it is convenient to separately identify the interactions among their real and imaginary parts and organize the partial results of Eq. (1.53) as a 2 × 2 matrix. Real-valued random variables are assumed hereafter. When the random variables are real-valued, Eq. (1.53) simplifies to\n\n\n\n\n\n\n cov(X,Y) = 𝔼[XY] − μxμy. \n\n\n\n\n\n\nTwo real-valued random variables are called uncorrelated if, and only if, cov(X,Y) = 0, which is equivalent to\n\n\n\n\n\n\n\n\n\n 𝔼[XY] = μxμy. \n\n\n(1.54)\n\n\n\n\n\n\nAs mentioned, ρ is calculated by extracting the means 𝔼[X] = μx and 𝔼[Y] = μy, i. e., using cov(X,Y) instead of cor(X,Y), and is restricted to  − 1 ≤ ρ ≤ 1 when the random variables are real-valued. Application 1.9 gives an example of using correlation to perform a simple data mining.\n\n\n\n\n\n1.12.1  Autocorrelation function\n\n\n\nAutocorrelation functions are extension of the correlation concept to signals. Table 1.9 summarizes the definitions that will be explored in this text.\n\n\n\n\n\n\n\nTable 1.9: Autocorrelation functions and their respective equation numbers.\n\n\n\n\n\n\n\n\n\n\nType of process or signal \n\n\nEquation \n\n\n\n\nGeneral stochastic process \n\n\n(1.55) \n\n\n\n\nWide-sense stationary stochastic process \n\n\n(1.56) \n\n\n\n\nDeterministic continuous-time energy signal x(t)\n\n\n(1.57) \n\n\n\n\nDeterministic continuous-time power signal x(t)\n\n\n(1.59) \n\n\n\n\nDeterministic discrete-time energy signal x[n] (unbiased estimate) \n\n\n(1.62) \n\n\n\n\nDeterministic discrete-time power signal x[n]\n\n\n(1.63) \n\n\n\n\n\n\n\nThe existence of distinct autocorrelation definitions for power and energy signals illustrate the importance of distinguishing them, as discussed in Section 1.9.4.\n\n\nDefinitions of autocorrelation for random signals\n\n\n\nAn autocorrelation function (ACF) defined for stochastic processes (see Appendix B.20) is\n\n\n\n\n\n\n\n\n\n RX(s,t) = 𝔼[X(s)X∗(t)], \n\n\n(1.55)\n\n\n\n\n\n\nwhich corresponds to the correlation between random variables X(s) and X(t) at two different points s and t in time of the same random process where X∗(t) is the complex conjugate of X(t). The purpose of the ACF is to determine the strength of relationship between amplitudes of the signal occurring at two different time instants.\n\n\n\nFor wide-sense stationary (WSS) processes, the ACF is given by\n\n\n\n\n\n\n\n\n\n RX(τ) = 𝔼[X(t + τ)X∗(t)], \n\n\n(1.56)\n\n\n\n\n\n\nwhere the time difference τ = s − t is called the lag time.\n\n\n\nThe random process formalism is very powerful and allows for representing complicated processes. But in many practical cases only one realization x(t) (or x[n]) of the random process X(t) is available. The alternative to deal with the signal without departing from the random process formalism is to assume that X(t) is ergodic. In this case, the statistical (or ensemble) average represented by 𝔼[⋅] is substituted by averages over time.\n\n\n\nThe definitions of autocorrelation given by Eq. (1.55) and Eq. (1.56) are the most common in signal processing, but others are adopted in fields such as statistics. Choosing one depends on the application and the model adopted for the signals.\n\n\n\nWhen dealing with a simple signal, one can naturally adopt one of the definition of autocorrelation for deterministic signals discussed in the sequel.\n\n\n\n\n\nDefinitions of autocorrelation for deterministic signals\n\n\n\nA definition of ACF tailored to a deterministic (non-random) energy signal x(t), which does not use expected values as Eq. (1.55), is\n\n\n\n\n\n\n\n\n\n RX(τ) = ∫ −∞∞x(t + τ)x∗(t)dt = ∫ −∞∞x∗(t − τ)x(t)dt, \n\n\n(1.57)\n\n\n\n\n\n\nsuch that\n\n\n\n\n\n\n\n\n\n RX(0) = ∫ −∞∞|x(t)|2dt = E, \n\n\n(1.58)\n\n\n\n\n\n\nwhere E is the signal energy. It should be noted that this definition of autocorrelation cannot be used for power signals. Power signals have infinite energy and using Eq. (1.57) for τ = 0 leads to RX(0) = ∞ in the case of power signals, which is uninformative.\n\n\n\nIf x(t) is a deterministic power signal, an useful ACF definition is\n\n\n\n\n\n\n\n\n\n RX(τ) = lim ⁡  T→∞ 1 2T∫ −T T x(t + τ)x∗(t)dt, \n\n\n(1.59)\n\n\n\n\n\n\nsuch that\n\n\n\n\n\n\n\n\n\n RX(0) = lim ⁡  T→∞ 1 2T∫ −T T |x(t)|2dt = P, \n\n\n(1.60)\n\n\n\n\n\n\nwhere P is the signal average power. Eq. (1.59) can also be used when x(t) is a realization of an ergodic stochastic process.\n\n\n\nSimilarly, there are distinct definitions of autocorrelation for deterministic discrete-time signals. For example, when x[n] is a finite-duration real signal with N samples, the (unscaled or not normalized) autocorrelation can be written as\n\n\n\n\n\n\n\n\n\n R^X[i] = ∑ n=iN−1x[n]x[n − i], \n\n\n(1.61)\n\n\n\n\n\n\nwhich has a minus in n − i that does not lead to reflecting the signal.17\n\n\n\nAs an alternative to use expressions more similar to the ones for signals with infinite duration, the (unscaled or not normalized) autocorrelation can be expressed as\n\n\n\n\n\n\n RX[i] = ∑ nx[n + i]x∗[n],i = −(N − 1),…,−1,0,1,…,N − 1 \n\n\n\n\n\n\nand computed assuming a zero value for x[n] when its index is out of range. This corresponds to assuming the signal is extended with enough zeros (zero-padding) to the right and to the left. For example, assuming x[n] = δ[n] + 2δ[n − 1] + 3δ[n − 2], which can be represented by the vector [1,2,3], its autocorrelation would be [3,8,14,8,3],\nfor the lags i = −2,−1,0,1,2, respectively.\n\n\n\n\n\n\n\nTable 1.10: Example of autocorrelation for a real signal [1,2,3] (n = 0,1,2). \n\n\n\n\n\n\n\n\n\n\n\nlag i\n\n\n\n\nvalid products x∗[n]x[n+i]\n\n\n\n\nRX   [i]\n\n\n\n\n\n−2\n\n\n\n\n\nx∗[2]x[0]\n\n\n\n3 \n\n\n\n\n\n−1\n\n\n\n\n\nx∗[1]x[0]+x∗[2]x[1]\n\n\n\n8 \n\n\n\n\n\n0\n\n\n\n\n\nx∗[0]x[0]+x∗[1]x[1]+x∗[2]x[2]\n\n\n\n14 \n\n\n\n\n\n1\n\n\n\n\n\nx∗[0]x[1]+x∗[1]x[2]\n\n\n\n8 \n\n\n\n\n\n2\n\n\n\n\n\nx∗[0]x[2]\n\n\n\n3 \n\n\n\n\n\n\n\nNote in Table 1.10 that the number of products decreases as |i| increases. More specifically, when computing RX[i] there are only N −|i| “valid” products. To cope with that, the normalized (and statistically unbiased) definition is\n\n\n\n\n\n\n\n\n\n RX[i] = 1 N −|i|∑ nx[n + i]x∗[n],i = −(N − 1),…,N − 1. \n\n\n(1.62)\n\n\n\n\n\n\nIn Matlab/Octave, the unbiased estimate for the signal [1,2,3] can be obtained with:\n\n\nx=[1,2,3];xcorr(x,'unbiased')\n\n\nwhich outputs [3,4,4.67,4,3]. The unscaled estimate of Table 1.10 can be obtained with xcorr(x,’none’) or simply xcorr(x) because it is the default.\n\n\n\n\n\n\n\nTable 1.11: Example of calculating the unscaled autocorrelation for a complex-valued signal [1 + j,2,3] (n = 0,1,2), where j = −1. \n\n\n\n\n\n\n\n\n\n\n\nlag i\n\n\n\n\nvalid products x∗[n]x[n+i]\n\n\n\n\nRX   [i]\n\n\n\n\n\n−2\n\n\n\n\n\nx∗[2]x[0]\n\n\n\n3+3j \n\n\n\n\n\n−1\n\n\n\n\n\nx∗[1]x[0]+x∗[2]x[1]\n\n\n\n8+2j \n\n\n\n\n\n0\n\n\n\n\n\nx∗[0]x[0]+x∗[1]x[1]+x∗[2]x[2]\n\n\n\n15 \n\n\n\n\n\n1\n\n\n\n\n\nx∗[0]x[1]+x∗[1]x[2]\n\n\n\n8-2j \n\n\n\n\n\n2\n\n\n\n\n\nx∗[0]x[2]\n\n\n\n3-3j \n\n\n\n\n\n\n\nAnother observation of interest is that for real signals, RX(τ) = RX(−τ). In general, for complex-valued signals, RX(τ) = RX∗(−τ), which is called Hermitian symmetry. Table 1.11 provides an example. It can also be noted that, for a given lag i, the subtraction of the indexes of all parcels in valid products is (n + i) − n = i.\n\n\n\n Example 1.49. Software implementation of autocorrelation. The definitions of RX[i] used a generic summation ∑ ⁡ n over n. To be more concrete, an example of Matlab/Octave code to calculate the unscaled autocorrelation RX[i] is given in Listing 1.20. It can be seen that the property RX(τ) = RX∗(−τ) is used to obtain the autocorrelation values for negative τ.\n\n\n\nListing 1.20: MatlabOctaveCodeSnippets/snip_signals_unscaled_autocorrelation.m\n\n\n%Calculate the unscaled autocorrelation R(i) of x \nx=[1+j 2 3] %define some vector x to test the code \nN=length(x); \nR=zeros(1,N); %space for i=0,1,...N-1 \n5R(1)=sum(abs(x).^2); %R(0) is the energy \nfor i=1:N-1 %for each positive lag \n    temp = 0; %partial value of R \n    for n=1:N-i %vary n over valid products \n        temp = temp + x(n+i)*conj(x(n)); \n10    end \n    R(i+1)=temp; %store final value of R \nend \nR = [conj(fliplr(R(2:end)))] %append complex conjugate\n  \n\n\nThe function xcorr in Matlab/Octave uses a much faster implementation based on the fast Fourier transform (FFT), to be discussed in Chapter 2. When comparing the results of the two methods, the discrepancy is around 10−14, which is a typical order of magnitude for numerical errors when working with Matlab/Octave.    □\n\n\n\nWhen infinite duration power signals can be assumed, it is sensible to define\n\n\n\n\n\n\n\n\n\n RX[i] = lim ⁡  N→∞ 1 2N∑ n=−NNx[n + i]x∗[n]. \n\n\n(1.63)\n\n\n\n\n\nExamples of some signals autocorrelations\n\n\n\n\n\n\nTwo examples of autocorrelation are discussed in the sequel.\n\n\n\n Example 1.50. Autocorrelation of white signal. A signal is called “white” when it has an autocorrelation R[i] = Aδ[i] in discrete-time, or R(τ) = Aδ(τ) in continuous-time, where A ∈ ℝ is an arbitrary value. In other words, a white signal has an autocorrelation that is nonzero only at the origin, which corresponds to having samples that are uncorrelated and, consequently, statistically independent.\n\n\n\nThis nomenclature will be clarified in Chapter 4 but it can be anticipated that such signals have their power uniformly distributed over frequency and the name is inspired by the property of white light, which is composed by a mixture of color wavelengths.\n\n\n\nAs mentioned, the samples of a white signal are independent and, if the are also identically distributed according to a Gaussian PDF, the signal is called white Gaussian noise (WGN). More strictly, a WGN signal can be modeled as a realization of a wide-sense stationary process. In this case, WGN denotes the stochastic process itself. Using Matlab/Octave, a discrete-time realization of WGN can be obtained with function randn, as illustrated in Section 1.9.2.    □\n\n\n\n Example 1.51. Autocorrelation of sinusoid. Using Eq. (1.59), the autocorrelation of a sinusoid x(t) = Asin ⁡ (ωt + ϕ) can be calculated as follows:\n\n\n\n R(τ)  = lim ⁡  T→∞  [ 1 2T∫ −T T Asin ⁡ (ωt + ϕ)Asin ⁡ (ωt + ωτ + ϕ)dt].    \n\nWe now assume a = ωt + ϕ for simplicity and use Eq. (B.4) to expand sin ⁡ (ωt + ϕ + ωτ):\n\n\n\n R(τ)  = lim ⁡  T→∞  [A2 2T∫ −T T sin ⁡ (a)  [sin ⁡ (a)cos ⁡ (ωτ) + sin ⁡ (ωτ)cos ⁡ (a)]dt]     = A2 lim ⁡   T→∞  [cos ⁡ (ωτ) 2T ∫ −T T sin ⁡ 2(a)dt + sin ⁡ (ωτ) 2T ∫ −T T sin ⁡ (a)cos ⁡ (a)dt]     = A2 lim ⁡   T→∞  [cos ⁡ (ωτ) 4T ∫ −T T [1 − cos ⁡ (2a)]dt + sin ⁡ (ωτ) 4T ∫ −T T sin ⁡ (2a)dt]     = A2 lim ⁡   T→∞  [cos ⁡ (ωτ) 2T 1 2∫ −T T dt]     = A2 cos ⁡ (ωτ) 2 . (1.64)  \n\nThe third equality used Eq. (B.9) and Eq. (B.5), and it was simplified because the integrals of both cos ⁡ (2a) and sin ⁡ (2a) are zero given the integration interval is an integer number of their periods. The final result indicates that the autocorrelation of a sinusoid or cosine does not depend on the phase ϕ and is also periodic in τ, with the same period 2π∕ω that the sinusoid has in t. Therefore, the frequencies contained in the realizations of a stationary random process can be investigated via the autocorrelation R(τ) of this process.\n\n\n\n\n\n\n\n\nFigure 1.51: A sinusoid of period N=8 samples and its autocorrelation, which is also periodic each 8 lags. The cosine corresponding to Rx[l] has amplitude A2∕2 = 42∕2 = 8.\n\n\n\n\n\nA simulation with Matlab/Octave can help understanding this result. Figure 1.51 was generated with Listing 1.21.\n\n\n\nListing 1.21: MatlabOctaveCodeSnippets/snip_signals_sinusoid_autocorrelation.m\n\n\nnumSamples = 48; %number of samples \nn=0:numSamples-1; %indices \nN = 8; %sinusoid period \nx=4*sin(2*pi/N*n); %sinusoid (try varying the phase!) \n5[R,l]=xcorr(x,'unbiased'); %calculate autocorrelation \nsubplot(211); stem(n,x); xlabel('n');ylabel('x[n]'); \nsubplot(212); stem(l,R); xlabel('Lag l');ylabel('R_x[l]');\n  \n\n\nIt can be seen that the x and R are a sine and cosine, respectively, of the same frequency in their corresponding domains (n and l, respectively).\n\n\n\n\n\n\n\n\nFigure 1.52: The a) unbiased and b) raw (unscaled) autcorrelations for the sinusoid of Figure 1.51 with a new period of N=15 samples.\n\n\n\n\n\nFigure 1.52 was obtained by changing the sinusoid period from N=8 to N=15 and illustrates the effects of dealing with finite-duration signals. Note that both the unbiased and unscaled versions have diminishing values at the end points.    □\n\n\n\nUsing Eq. (1.63) and a calculation similar to the one used in Eq. (1.64), one can prove that the autocorrelation of x[n] = sin ⁡ (Ωn + ϕ) is RX[i] = cos ⁡ (Ωi)∕2.\n\n\n1.12.2  Cross-correlation\n\n\n\n\n\n\nThe cross-correlation function (also called correlation) is very similar to the ACF but uses two distinct signals, being defined for deterministic energy signals as\n\n\n\n\n\n\n Rxy(τ) ≜∫ −∞∞x(t + τ)y∗(t)dt = ∫ −∞∞x(t)y∗(t − τ)dt. \n\n\n\n\n\n\nNote the adopted convention with respect to the complex conjugate.\n\n\n\nSome important properties of the cross-correlation are:\n\n &lt;ul class='itemize1'&gt;\n &lt;li class='itemize'&gt;&lt;!-- l. 2740 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi mathvariant='italic'&gt;xy&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt; &lt;mo class='MathClass-rel' stretchy='false'&gt;=&lt;/mo&gt; &lt;msubsup&gt;&lt;mrow&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi mathvariant='italic'&gt;xy&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo class='MathClass-bin' stretchy='false'&gt;∗&lt;/mo&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mo class='MathClass-bin' stretchy='false'&gt;−&lt;/mo&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\n (Hermitian symmetry),\n &lt;/li&gt;\n &lt;li class='itemize'&gt;&lt;!-- l. 2741 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi mathvariant='italic'&gt;xy&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt; &lt;mo class='MathClass-rel' stretchy='false'&gt;=&lt;/mo&gt; &lt;msubsup&gt;&lt;mrow&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi mathvariant='italic'&gt;yx&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo class='MathClass-bin' stretchy='false'&gt;∗&lt;/mo&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mo class='MathClass-bin' stretchy='false'&gt;−&lt;/mo&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\n (swapping arguments is also Hermitian),\n &lt;/li&gt;\n &lt;li class='itemize'&gt;&lt;!-- l. 2742 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;mo class='MathClass-rel' stretchy='false'&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi mathvariant='italic'&gt;xy&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mi&gt;τ&lt;/mi&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt;&lt;mo class='MathClass-rel' stretchy='false'&gt;|&lt;/mo&gt;&lt;mo class='MathClass-rel' stretchy='false'&gt;≤&lt;/mo&gt;&lt;msqrt&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi mathvariant='italic'&gt;xx&lt;/mi&gt; &lt;/mrow&gt; &lt;/msub&gt; &lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;R&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi mathvariant='italic'&gt;yy&lt;/mi&gt; &lt;/mrow&gt; &lt;/msub&gt; &lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msqrt&gt;&lt;/mrow&gt;&lt;/math&gt;,\n (maximum is not necessarily at &lt;!-- l. 2742 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;mi&gt;τ&lt;/mi&gt; &lt;mo class='MathClass-rel' stretchy='false'&gt;=&lt;/mo&gt; &lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/math&gt;\n but is bounded).&lt;/li&gt;&lt;/ul&gt;\n                                                                          \n                                                                          \n\n\nIn discrete-time, the deterministic cross-correlation for energy signals is\n\n\n\n\n\n\n Rxy[l] ≜∑ n=−∞∞x[n + l]y∗[n] = ∑ n=−∞∞x[n]y∗[n − l]. \n\n\n\n\n\n\nWhen considering random processes, the two random variables are obtained from distinct processes:\n\n\n\n\n\n\n\n\n\n RXY (s,t) = 𝔼[X(s)Y ∗(t)] \n\n\n(1.65)\n\n\n\n\n\n\nor, assuming stationarity (see Section B.20.0.0) with s = t + τ:\n\n\n\n\n\n\n\n\n\n RXY (τ) = 𝔼[X(t + τ)Y ∗(t)]. \n\n\n(1.66)\n\n\n\n\n\n\nApplication 1.13 illustrates the use of cross-correlation to align two signals.\n\n\n\nBefore concluding this section, it is convenient to recall that the autocorrelation of a signal inherits any periodicity that is present in the signal. Sometimes this periodicity is more evident in the autocorrelation than in the signal waveform. The next example illustrates this point by discussing the autocorrelation of a sinusoid immersed in noise.\n\n\n\n Example 1.52. The power of a sum of signals is the sum of their powers in case they are uncorrelated. Assume that a sinusoid x[n] is contaminated by noise z[n] such that the noisy version of the signal is y[n] = x[n] + z[n]. The signal z[n] is a WGN (see Section 1.12.1.0) that is added to the signal of interest x[n] and, therefore, called additive white Gaussian noise (AWGN). If x[n] and z[n] are uncorrelated, such that Rxz[l] = 𝔼[x[n + l]z[n]] = 0,∀ ⁡l, the autocorrelation Ry[l] of y[n] is given by \n\n\n   Ry[l] = 𝔼[y[n + l]y[n]] = 𝔼[(x[n + l] + z[n + l])(x[n] + z[n])]    = Rx[l] + Rzx[l] + Rxz[l] + Rz[l]     = Rx[l] + Rz[l].     \n\n\n\n\n\n\n\n\n\nFigure 1.53: Sinusoid of amplitude 4 V immersed in AWGN of power 25 W. The bottom graph is a zoom showing the first 100 samples.\n\n\n\n\n\n\n\n\n\n\nFigure 1.54: Bottom graph: autocorrelation of the sine plus noise in Figure 1.53, top: autocorrelation of the sine and middle: autocorrelation of the noise.\n\n\n\n\n\nListing 1.22 illustrates a practical use of this result. A sine with amplitude 4 V and power 42∕2 = 8 W is contaminated by AWGN with power of 25 W. All signals are represented by 4,000 samples, such that the estimates are relatively accurate.\n\n\n\nListing 1.22: MatlabOctaveCodeSnippets/snip_signals_noisy_sinusoid.m\n\n\n%Example of sinusoid plus noise \nA=4; %sinusoid amplitude \nnoisePower=25; %noise power \nf=2; %frequency in Hz \n5n=0:3999; %\"time\", using many samples to get good estimate \nFs=20; %sampling frequency in Hz \nx=A*sin(2*pi*f/Fs*n); %generate discrete-time sine \nrandn('state', 0); %Set randn to its default initial state \nz=sqrt(noisePower)*randn(size(x)); %generate noise \n10clf, subplot(211), plot(x+z); %plot noise \nmaxSample=100; %determine the zoom range \nsubplot(212),stem(x(1:maxSample)+z(1:maxSample)),pause; %zoom \nmaxLags = 20; %maximum lag for xcorr calculation \n[Rx,lags]=xcorr(x,maxLags,'unbiased'); %signal only \n15[Rz,lags]=xcorr(z,maxLags,'unbiased'); %noise only \n[Ry,lags]=xcorr(x+z,maxLags,'unbiased');%noisy signal \nsubplot(311), stem(lags,Rx); ylabel('R_x[l]'); \nsubplot(312),stem(lags,Rz);ylabel('R_z[l]'); \nsubplot(313),stem(lags,Ry);xlabel('Lag l');ylabel('R_y[l]');\n  \n\n\nThe signal-to-noise ratio (SNR) is a common metric consisting of the ratio between the signal and noise power values and often denoted in dB (seem Appendix B.24). Figure 1.53 illustrates the fact that it is hard to visualize the sinusoid because of the relatively low signal-to-noise ratio SNR dB = 10log ⁡ 10(8∕25) ≈−5 dB. The waveform does not seem to indicate periodicity.\n\n\n\nFigure 1.54 shows the autocorrelations of y[n] and its two parcels. For the lag l = 0, the estimated values are Rx[0] = 8, Rz[0] = 24.92 and Ry[0] = 33.17. The theoretical values are 8 W (the sine power), 25 W (noise power) and 8 + 25 = 33 W (sum of the parcels), respectively. The bottom graph clearly exhibits periodicity and the noise disturbs only the value of Ry[l] at l = 0. In summary, two assumptions can simplify the analysis of random signals: that the ACF of the noise is approximately an impulse at the origin (l = 0) and that the signal and noise are uncorrelated.  □\n\n\n\n Example 1.53. The AWGN channel. Thermal noise is ubiquitous and WGN is often present in telecommunication and other signal processing applications. WGN was briefly introduced in Examples 1.50, 1.52 and Application 1.10. In telecommunications, distinct systems that share the property of having WGN added at the receiver are called AWGN channel models.\n\n\n\nFigure 1.55 illustrates a continuous-time AWGN, where the received signal r(t) = s(t) + ν(t) is simply the sum of the transmitted signal s(t) and WGN ν(t).\n\n\n\n\n\n\n\n\nFigure 1.55: Continuous-time version of the AWGN channel model.\n\n\n\n\n\nBecause s(t) and WGN ν(t) are often assumed to be uncorrelated, as discussed in Example 1.52, the power of r(t) is the sum of the powers of s(t) and ν(t).    □\n\n\n\nHaving learned about correlation (cross-correlation, etc.), it is possible to study a linear model for quantization, which circumvents dealing with the quantizer non-linearity.\n\n\n \n\n17 This operation should not be confused with convolution, which is discussed in Chapter 3.\n\n    &lt;/div&gt;"
  },
  {
    "objectID": "ak_dsp_bookse13.html",
    "href": "ak_dsp_bookse13.html",
    "title": "13  A Linear Model for Quantization",
    "section": "",
    "text": "As discussed, the quantization process is non-linear and the quantization error values depend on the quantizer input. But the following linear model for the quantization error has proved to be a good approximation in several applications.\n\n\n\nIn this model, the error Eq = X −Xq is assumed to be a uniformly distributed random variable with support [−Δ∕2,Δ∕2] and zero-mean. This is sometimes called quantization noise. Two assumptions are important:\n\n\n\nf 1.\n\n\nThe signal has enough variation to span all levels of the quantizer stair. The model fails, for example, if the input signal is a constant (DC) value.\n\n\nf 2.\n\n\nThere is no correlation between the error Eq and the signal to be quantized X.\n\n\n\n\nThe main assumption is that, for a quantizer with a step of Δ, the quantization noise Eq is modeled by assuming it is a random variable Eq with uniform PDF and support [−Δ∕2,Δ∕2]. Because it is zero-mean, the power of 𝔼[Eq2] is solely the variance of the uniform PDF given by Eq. (1.38), which in this case is\n\n\n\n\n\n\n\n\n\n σ2 = Δ2∕12, \n\n\n(1.67)\n\n\n\n\n\n\nas indicated by Eq. (1.38).\n\n\n\nAssuming that\n\n\n\n\n\n\n Δ = X^max −X^min 2b , \n\n\n\n\n\n\nthe power Pn of the quantization noise is\n\n\n\n\n\n\n\n\n\n Pn = σ2 = Δ2 12 = (X^max −X^min)2 22b12 , \n\n\n(1.68)\n\n\n\n\n\n\nIt is a good idea to practice using the model by calculating the quantization SNR for sinusoids and cosines, uniformly and normally distributed random signals. The quantization SNR is obtained by using the quantization noise power in the denominator of the SNR expression (the numerator is the signal power, as usual). The following example illustrates the result for quantizing a Gaussian input signal.\n\n\n\n Example 1.54. Quantization SNR of a Gaussian signal and the 6 dB per bit rule of thumb. If the signal to be quantized x(t) has samples distributed according to a Gaussian N(3,4) with mean μ = 3 V and variance σ2 = 4 W, a reasonable alternative (see Eq. (1.28)) is to adopt X^min = μ − 3σ and X^max = μ + 3σ. In this case and using Eq. (1.28),\n\n\n\n\n\n\n Δ = X^max −X^min M − 1 = 3 + 6 − (3 − 6) 2b − 1 = 12 2b − 1, \n\n\n\n\n\n\nwhere b is the number of bits of the quantizer. Using the linear model of quantization:\n\n\n\n\n\n\n SNR = Ps Pn = 32 + 4 Δ2∕12 = 13 × 12 122∕(2b − 1)2 = 13(2b − 1)2 12 ≈ 1.083(2b − 1)2, \n\n\n\n\n\n\nwhere Ps and Pn are the signal and noise power, respectively. The SNRdB is\n\n\n\n 10log ⁡ 10SNR  = 10[log ⁡ 101.0833 + 2log ⁡ 10(2b − 1)]     ≈ 0.3463 + 20blog ⁡ 102     ≈ 0.3463 + 6.021b,   \n\n\nwhere the last step assumed that 2b » 1.\n\n\n\nThe result SNRdB ≈ 6b + cte. is a well-known “rule of thumb”. The constant (cte.) may vary, but the SNR dB typically increases by 6 dB for each extra bit in the quantizer. It is common to use this approximation to suggest, for example, that an ADC of 12 bits has approximately quantization SNR dB = 6 × 12 = 72 dB, while an ADC of 16 bits has SNRdB = 6 × 16 = 96 dB.   □\n\n\n\nOther applications of the main results in this chapter are discussed in the next section."
  },
  {
    "objectID": "ak_dsp_bookse14.html",
    "href": "ak_dsp_bookse14.html",
    "title": "14  Applications",
    "section": "",
    "text": "Application 1.1. Recording with a sound board. It is relatively easy to record sound using a microcomputer. However, most softwares that capture sound are not very useful for studying DSP, because they assume the user is not interested in “low-level” details such as the number of bits per sample. But there many alternatives that do provide this kind of information. Two free and open-source (FOSS) softwares for manipulation sound files are Audacity [url1aud] and Sox [url1sox]. While Sox is very useful for converting among file formats and working from a command line, Audacity is adopted here because it has a graphical user interface (GUI) that allows, for example, to monitor recording and avoid saturating the ADC, which would distort the sound due to clipping of its amplitudes.\n\n\n\nFigure 1.56 shows a short segment of audio recorded with the default options of sampling frequency Fs = 44.1 kHz and number b = 32 bits per sample in floating-point, as indicated by the letter A in the figure. The menu Edit - Preferences - Quality of Audacity allows to change these values. Another option to change Fs is the “Project rate” in letter B of Figure 1.56. The level meters indicated with letter C are activated during recording and playback and, in this case, suggest that the signal amplitude was clipped due to ADC saturation. Alternatively, this can be visualized using menu View - Show Clipping. Each time a new recording starts (by clicking the button indicated by letter D), the audio track has the Fs and b imposed by the current default options. Most sound boards have two channels and can record in stereo but here it is assumed that only one channel is of interest and the files are mono.\n\n\n\n\n\n\n\n\nFigure 1.56: Example of sound recorded at Fs = 44.1 kHz with the Audacity sound editor.\n\n\n\n\n\nAudacity can save a signal in a variety of file formats, such as MP3, using the menu Export. Our goal is to later read the saved file in another software (Matlab/Octave, etc.), so MP3 should be avoided and of special interest here are “wav” (actually the WAVE format, an instance of Microsoft’s RIFF file format) and raw (header-less).\n\n\n\nThe “wav” format is just a wrapper for many codecs. In other words, within a “wav” file one can find uncompressed data requiring hundreds of kilobits to represent each second (kbps) of audio as well highly compressed data requesting less than five kbps. Unless the file size should be minimized, for increased portability it is better to use an uncompressed “PCM” format. Due to its adoption in digital communications, the result of A/D conversion is sometimes called pulse-coded modulation (PCM). Hence, PCM can be seen as a codec but its output is equivalent to a signal simply sampled at Fs and quantized (or encoded) with b bits/sample. If the adopted quantizer is uniform (see Eq. (1.26)), the PCM is called linear. The linear PCM is the best format with respect to portability but there are also two popular non-linear PCMs.\n\n\n\nBecause the probability distribution of long segments of speech signals is approximately Laplacian, not uniform, the quantizer used in digital telephony is non-uniform. These quantizers are based on non-linear curves (approximately logarithmic) called A-law and μ-law. Figure 1.57 shows some options when the user chooses Export - Other compressed files (in “type”) and then Options.\n\n\n\n\n\n\n\n\nFigure 1.57: Some Audacity options for saving an uncompressed WAVE file. The two non-linear PCMs are indicated.\n\n\n\n\n\nHence, use Audacity to record some sound with Fs = 8 kHz and export it as a file named myvoice.wav in the “WAV (Microsoft) signed 16 bits” format. After that, read it with Matlab/Octave using:\n\n\n[x,Fs,b]=wavread('myvoice.wav');\n\n\nIt can be observed that Fs=8000 and b=16, as recorded. Note that by default the wavread function outputs samples in floating-point and normalizes them to be within the range [−1,1]. If the actual integer values are of interest, Matlab allows to use\n\n\n[x2,Fs,b]=wavread('myvoice.wav','native');\n\n\nUsing the last two commands and comparing min(x), max(x) and min(x2), max(x2), in the case of a specific audio file, the (native) integer values  − 8488 (min) and 5877 (max) were normalized to  − 0.2590 and 0.1794, respectively, when not using the option ’native’. The normalization consists in dividing the native integer values by 2b−1, which takes in account that these values are originally within the range [−2b−1,2b − 1]. For example, in this case b = 16 and 5877∕215 ≈ 0.1794.\n\n\n\nIn case the file had used A-law non-linear PCM, Matlab would give the error message:Data compression format (CCITT a-law) is not supported.and Octave:error: wavread: sample format 0x6 is not supported\n\n\n\nNow, it is suggested to get more familiar with headerless files using Audacity to save a sound file as “raw”. It may be useful to check Appendix C.2 for more details on how the information is organized in binary files. After recording in Audacity, choose Export - Other compressed files (in “type”) as in Figure 1.57, but this time select the header “RAW (header-less)” instead of “WAV (Microsoft)”. For the encoding, select “Signed 16 bit-PCM”, as before, and name the file ’myvoice_raw.wav’. In this case, it would be wiser to use another file extension and name it ’myvoice.raw’, for example. But the purpose of using “wav” is to make clear that the extension by itself cannot guarantee a file format is the expected one.\n\n\n\nIn this particular example, the file sizes are 29,228 and 29,184 for the WAVE and raw formats, respectively. In fact, in spite of a WAVE possibly having a sophisticated structure with several sections (chunks), most of them have a single chunk and one header consisting of the first 44 bytes,\nwhich is the difference between the two sizes given that both have the same 29184∕2 = 14592 samples of 2 bytes each.\n\n\n\nUsing the command wavread for the raw file would generate error messages in Matlab/Octave. Based on Appendix C.2.4.0, the following code properly reads the samples:\n\n\nfp=fopen('myvoice_raw.wav','rb'); %open for reading in binary \nx=fread(fp,Inf,'int16'); %read all samples as signed 16-bits \nfclose(fp);  %close the file\n\n\nAs a sanity check, one can read the samples of the WAVE file, skip its header and compare with the result of wavread with Listing 1.23 on Matlab.\n\n\n\nListing 1.23: MatlabOctaveCodeSnippets/snip_signals_wavread.m\n\n\nfp=fopen('myvoice.wav','rb'); %open for reading in binary \nx=fread(fp,Inf,'int16'); %read all samples as signed 16-bits \nfclose(fp);  %close the file \nx(1:22)=[]; %eliminate the 44-bytes header \n5[x2,Fs,b]=readwav('myvoice.wav','r'); \nx2=double(x2); %convert integer to double for easier manipulation \nmax(abs(x-x2)) %result is 0, indicating they are identical\n  \n\n\nThe advantage of using WAVE is that the header informs Fs, b, whether its mono or stereo, etc. Also, the WAVE format takes care of endianness (see Appendix C.2.3). Not using wavread, write code in Matlab/Octave to open a WAVE file (with only 1 chunk) and extract Fs, b and the samples as integers. This code can be used by Octave users to mimic the option native in Matlab’s wavread. It may be useful to read Appendix C.2.5 and use the companion code laps_dump.c, which can be compiled with most C compilers. A short description of the WAVE header is provided at [url1wav].    □\n\n\n\n Application 1.2. Recording sound with Matlab. This application discusses how to record sound directly with Matlab, which has several functions to deal with sound recording and playback. You can check soundsc, audiorecorder, wavplay and wavrecord, for example. Some functions work only on Windows.\n\n\n\nOctave has functions such as record and sound and its support to sound is more natural on Linux. There are solutions such as [url1rec] to record and play sound on Octave running on Windows, but the installation is not trivial.\n\n\n\nThe following code was used in Matlab to record 5 seconds of one (mono) channel sound at a sampling rate of 11,025 Hz, using 16 bits to represent each sample:\n\n\nr=audiorecorder(11025,16,1); %create audiorecorder object \nrecord(r,5); %record 5 seconds and store inside object r\n\n\nOne can use play(r) to listen the recorded sound or y = getaudiodata(r, ’int16’) to obtain the samples from the audiorecorder object. However, if one of these commands immediately\nfollows record(r,5), the error can be generated:??? Cannot retrieve audio data while recording is in progress.This means the software was still recording when it tried to execute the second command. An alternative is to use recordblocking as in Listing 1.24.\n\n\n\nListing 1.24: MatlabOnly/snip_signals_recordblocking.m\n\n\nr=audiorecorder(11025,16,1); %create audiorecorder object \nrecordblocking(r,5); %record 5 seconds and store inside r \nplay(r) %playback the sound \ny = getaudiodata(r, 'int16'); %extract samples as int16 \n5plot(y); %show the graph\n  \n\n\nNote that y in the above example is an array with elements of the type int16, i. e., 2 bytes per sample. This saves storage space when compared to the conventional real numbers stored in double (8 bytes) each, but limits the manipulations. For example, the command soundsc(y,11025) generates an error message if y is int16. In such cases, a conversion as y=double(y) can be used before invoking soundsc (use whos y to check that the storage has quadruplicated).\n\n\n\nTo write y to a 16-bits per sample WAV file and read it back, use Listing 1.25 but the command double(z)./double(y) shows that the normalization used by wavwrite made z approximately three times y. The Voicebox toolbox ([url1voi]) has functions readwav and writewav that are smarter than Matlab’s with respect to avoiding the normalization.\n\n\n\nListing 1.25: MatlabOnly/snip_signals_wavwrite.m\n\n\nyd=double(y); %convert from int16 (y from getaudiodata) \nyd=yd/max(abs(yd)); %normalize \nwritewav(yd,11025,'somename.wav','16r') %write as 16-bits \nz=readwav('somename.wav','r'); %avoid normalization\n  \n\n\nSome sound boards allow full-duplex operation, i. e., recording and playing at the same time. Typically the sampling frequency must be the same for both operations. On Windows one can try the wavplay function with the option “async” as exemplified in Listing 1.26.\n\n\n\nListing 1.26: MatlabOnly/snip_signals_realtimeLoopback.m\n\n\nFs = 11025;   %define sampling rate (Hz) \nfc = 1500; %cosine frequency (Hz) \nrecordingDuration = 1; %duration of recording, in seconds \nr=audiorecorder(Fs,16,1); \n5while 1 %infinite loop, stop with CTRL+C \n    recordblocking(r,recordingDuration); \n    inputSignal  = getaudiodata(r); \n    p=audioplayer(r); \n    subplot(211), plot(inputSignal); %graph in time domain \n10    subplot(212), pwelch(double(inputSignal)); %in frequency domain \n    drawnow %Force the graphics to update immediately inside the loop \nend\n  \n\n\nListing 1.26 shows the acquired signal (from the ADC) in both time and frequency domains. In this code, the call to wavplay is non-blocking but samples are lost in the sense that inputSignal is not a perfect cosine. Using a loopback cable, as in Application 1.4, allows to evaluate the system.\n\n\n\n\n\n\n\n\nFigure 1.58: Cosine obtained with Listing 1.26 and a loopback cable connecting the soundboard DAC and ADC.\n\n\n\n\n\nFigure 1.58 was obtained with a loopback. Note from the top plot that approximately 300 samples are a transient and after that one can see the cosine at fc = 1500 Hz, which is mapped to 1500∕(Fs∕2) ≈ 0.27 (Fs = 11025 Hz) in the normalized axis of the bottom plot. In order to get this kind of system running, it is important to reduce the volume (DAC gain) to avoid saturation of the signals.\n\n\n\nAs an exercise, digitize signals at different sampling frequencies and plot them with the axes properly normalized. Another interesting exploration is to obtain a sound signal inputSignal, digitized at a given rate (e. g., Fs = 11,025 Hz) and represented by int16. Convert it to double with x=double(inputSignal) in order to easily manipulate the signal and describe what is the result of each of the commands in Listing 1.27.\n\n\n\nListing 1.27: MatlabOnly/snip_signals_digitize_signals.m\n\n\nfs=22050; %sampling frequency \nr = audiorecorder(fs, 16, 1);%create audiorecorder object \nrecordblocking(r,5);%record 5 s and store inside object r \ny = getaudiodata(r,'int16'); %retrieve samples as int16 \n5x = double(y); %convert from int16 to double \nsoundsc(x,fs); %play at the sampling frequency \nsoundsc(x,round(fs/2));%play at half of the sampling freq. \nsoundsc(x,2*fs); %play at twice the sampling frequency \nw=x(1:2:end); %keep only half of the samples \n10soundsc(w,fs); %play at the original sampling frequency \nz=zeros(2*length(x),1); %vector with twice the size of x \nz(1:2:end)=x;%copy x into odd elements of z (even are 0) \nsoundsc(z,fs); %play at the original sampling frequency\n  \n\n\nWhat should be the sampling frequency for vectors w and z in Listing 1.27 to properly listen the audio?   □\n\n\n\n Application 1.3. Real time sound processing with Matlab’s DSP System Toolbox. Matlab’s DSP System Toolbox has extended support to interfacing with the sound board. Listing 1.28 provides a simple example that illustrates recording audio.\n\n\n\nListing 1.28: MatlabOnly/snip_signals_realtimeWithDspSystem.m\n\n\nexampleNumber=1; %choose 1 (spectrum analyzer) or 2 (digital filter) \nFs = 8000;   %define sampling rate (Hz) \n%create an audio recorder object: \nmicrophone = dsp.AudioRecorder('NumChannels',1,'SampleRate',Fs); \n5if exampleNumber==1 \n    specAnalyzer = dsp.SpectrumAnalyzer; %spectrum analyzer object \nelse \n    [B,A]=butter(4,0.05); %4-th order lowpass Butterworth filter \n    filterMemory=[]; %initialize the filter's memory \n10    speaker = dsp.AudioPlayer('SampleRate',Fs); %create audio player \nend \ndisp('Infinite loop, stop with CTRL+C...'); \nwhile 1 %infinite loop, stop with CTRL+C \n    audio = step(microphone); %record audio \n15    if exampleNumber==1 %spectrum analyzer \n        step(specAnalyzer,audio); %observe audio in frequency domain \n    else %perform digital filtering \n        [output,filterMemory]=filter(B,A,audio,filterMemory); \n        step(speaker, output); %send filtered audio to player \n20    end \nend\n  \n\n\nListing 1.28 indicates how to plot the signal in frequency domain or to perform digital filtering. The code may drop samples depending on the computer’s speed. Matlab’s documentation inform how to control the queue and buffer lengths, and also obtain the number of overruns.   □\n\n\n\n Application 1.4. Estimating latency using the sound board, Audacity and a loopback cable. The goal here is to practice dealing with digital and analog signals, interfacing Audacity and Matlab/Octave. An audio cable and a single computer (with the sound system working) is all that is needed for many interesting experiments. The user is invited to construct or purchase a cable with the proper connectors for his/her computer. In most cases, a “3.5 mm male to 3.5 mm male audio cable” is the required one, as indicated in Figure 1.59. A single channel (mono) cable may suffice but stereo cables have almost the same cost and can be used in more elaborated experiments.18\n\n\n\n\n\n\n\n\nFigure 1.59: Setup for loopback of the sound system using an audio cable, which is connected to the microphone input (or, alternatively, to the line in) and to the speaker output (or to the line out).\n\n\n\n\n\nThe task is to estimate the latency or channel delay, which is the time interval between the signal is transmitted and its arrival at a receiver after passing through a channel. In this specific case, the channel is composed of the sound board hardware (buffers, etc.) and the used device drivers (low-level software that interfaces with the hardware) and application software (Audacity in this case). In sound processing, latency is especially important when overdubbing, i. e., recording a track while playing back others. A detailed description of testing the latency with Audacity can be found at [url1lat].\n\n\n\n\n\n\n\n\nFigure 1.60: Example of options provided by Windows and the sound board. All the enhancements for both recording and playback devices should be disabled.\n\n\n\n\n\nTo have a better control of the sound board, it is important to disable all special effects and enhancements for both recording and playback devices, such as automatic gain control for the input ADC signal. Figure 1.60 provides screenshots from Windows but users of other operating systems should be able to find how to choose the best sound options.\n\n\n\nAfter making sure the best configuration for your sound system was chosen, the task now is to generate some samples of a periodic train of impulses. Instead of impulses, the Audacity menu “Generate - Click Track” provides a dialog window with other options. But here the suggestion is to use Matlab/Octave and create a signal with N discrete-time impulses δ[n]. Note that the analog signal corresponding to δ[n] will never be the theoretical continuous-time δ(t). For example, assuming zero-order reconstruction (see Figure 1.23), the amplitude of δ[n] would be held constant during the whole sampling interval Ts. Aware of this limitation, Listing 1.29 generates a train of N discrete-time impulses and saves it to a WAVE file.\n\n\n\nListing 1.29: MatlabOctaveCodeSnippets/snip_signals_inpulse_train.m\n\n\nFs = 44100; %sampling frequency \nTs = 1/Fs; %sampling period \nTimpulses = 0.25; %interval between impules in seconds \nL=floor(Timpulses/Ts); %number of samples between impulses \n5N = 4; %number of impulses \nimpulseTrain=zeros(N*L,1); %allocate space with zeros \nb=16; %number of bits per sample \namplitude = 2^(b-1)-1; %impulse amplitude, max signed int \nimpulseTrain(1:L:end)=amplitude; %generate impulses \n10writewav(impulseTrain,Fs,'impulses.wav','16r') %save WAVE RIFF\n  \n\n\n\n\n\n\n\nFigure 1.61: Audacity window after reading in the ’impulses.wav’ file.\n\n\n\n\n\nOpening the generated file with Audacity should lead to Figure 1.61. Note the amplitudes have been normalized and the first impulse barely appears. In this case, as indicated by letter F in Figure 1.61, the selection region starts approximately at 1 s. The interface is friendly and the letters C and D indicate how to switch between zooming the signal and enabling the cursor, respectively. After a segment is selected, letter E indicates how to easily zoom it to fit the selection. Instead of seconds (in letter F), it is sometimes convenient to use “samples”. Using the play button indicated with letter A plays the file.\n\n\n\n\n\n\n\n\nFigure 1.62: Audacity window after simultaneously recording and playing ’impulses.wav’ with a loopback.\n\n\n\n\n\nAt this point, a feature of Audacity that is useful for overdubbing can be used to simultaneously activate the DAC and ADC: when recording, Audacity also plays all the signals that are “open” (in this case, the “impulses” signal). With the audio cable connected in loopback, start recording (and playback) simply using button (letter B), stopping it after a second. The final situation should be similar to Figure 1.62.\n\n\n\nFrom the code used to generate ’impulses.wav’ it can be seen that the impulses are separated by Fs∕4 = 11025 samples (the first one is at n = 1, the second at n = 11026 and so on). This information was used to impose the start selection (letter F in Figure 1.61) at sample 11,026 in Figure 1.62 (it is irrelevant here, but recall that the first index in Matlab/Octave is 1 but 0 in Audacity). The end of the selection was located approximately at the start of the second impulse of the recorded signal (bottom plot, identified as “Audio Track”). In this case, the number of samples of this selection indicate that the latency was approximately 2102 × Ts ≈ 47.66 ms.\n\n\n\n\n\n\n\n\nFigure 1.63: Zoom of the response to the second impulse in Figure 1.61.\n\n\n\n\n\nAt this point it may be useful to export the recorded signal as a WAVE file to be read in Matlab/Octave. First, you can close the window with the “impulses” signal (otherwise Audacity will ask if the two files should be merged) and use the “Export” menu. Assuming the output file name was impulseResponses.wav, the command h=wavread(’impulseResponses.wav’) can be used to generate the zoom of the second impulse response in Figure 1.63. The concept of impulse response is very important, as discussed in Chapter 3.\n\n\n\nBecause the maximum absolute amplitude occurs at n = 13037 in Figure 1.63 and the corresponding impulse is located at n = 11026, another estimate of the latency is (13037 − 11026)Ts ≈ 45.6 ms. A detail is that for creating Figure 1.63, the ‘native’ option of Matlab’s wavread was used to avoid normalization and, consequently, the minimum signal value is  − 2b−1 = −32768 not  − 1.    □\n\n\n\n Application 1.5. PC sound board quantizer. Given a system with an ADC, typically one has to know beforehand or conduct measurements to obtain the quantizer step size Δ. This is the case when using a personal computer (PC) sound board. For a sound board, the value of Δ depends if the signal was acquired using the microphone input or the line-in input of the sound board. The microphone interface is designed for signals with peak value of Xmax = 10 to 100 mV while the peak for the line-in is typically 0.2 to 2 V. Note that the voltage ranges of line inputs and microphones vary from card to card. See more details in [url1pcs]. For the sake of this discussion, one can assume a dynamic range of [−100,100] mV and a ADC of 8 bits per sample, such that Δ = 200∕(28 − 1) ≈ 0.78 mV. The following example illustrates how to approximately recover the analog signal for visualization purposes. Assume the digital dynamic range is [0, 255] and the digital samples are D = [13,126,3,34,254]. If one simply uses stem(D), there is no information about time and amplitude. Listing 1.30 shows the necessary normalizations to visualize the abscissa in seconds and the ordinate in Volts, which in this case corresponds to A=1000*[-89.70, -1.56, -97.50, -73.32, 98.28].\n\n\n\nListing 1.30: MatlabOctaveCodeSnippets/snip_signals_amplitude_normalization.m\n\n\nD=[13 126 3 34 254]; %signal as 8-bits unsigned [0, 255] \nn=[0:4]; %sample instants in the digital domain \nFs=8000; %sampling frequency \ndelta=0.78e-3; %step size in Volts \n5A=(D-128)*delta; %subtract offset=128 and normalize by delta \nTs=1/Fs; %sampling interval in seconds \ntime=n*Ts; %normalize abscissa \nstem(time,A); %compare with stem(n,D) \nxlabel('time (s)'); ylabel('amplitude (V)');\n                                                                              \n                                                                              \n  \n\n\nNow assume a PC computer with a sound board that uses a 16 bits ADC and supports at its input a dynamic range of  − 185 to 185 mV. The quantizer is similar to the one depicted in Figure 1.36, but the quantization step should be Δ = 2 × 185 × 10−3∕216 ≈ 5.6 μV. It is assumed here that Δ = 5.6 μV and the quantizer is uniform from  − 215Δ to (215 − 1)Δ. In this case, the M = 65,536 = 216 levels are organized as 32,767 positive levels, 32,768 negative levels and one level representing zero. The assumed coding scheme is the offset code of Table 1.7 with 32,768 as the offset. Hence, the smallest value  − 215Δ is mapped to the 16-bits codeword “0000 0000 0000 0000”, (−215 + 1)Δ to “0000 0000 0000 0001” and so on, with (215 − 1)Δ being coded as “1111 1111 1111 1111”.\n\n\n\nIf at a specific time t0 the ADC input is x(t0) = x = 0.003 V, the ADC output is xi = 536, which corresponds to xq = 0.0030016 V and leads to a quantization error e = x − xq ≈−1.6 × 10−6 V. These results can be obtained in Matlab/Octave with\n\n\ndelta=5.6e-6, b=16 %define parameters for the quantizer \nformat long %see numbers with many decimals \nx=3e-3; [xq,xi]=ak_quantizer(x,delta,b), error=x-xq\n\n\nBased on similar reasoning, calculate the outputs xi of the quantizer, their respective xq values and the quantization error for x ∈{−300,−100,0,20,180} mV.\n\n\n\nIf you have access to an oscilloscope and a function generator, try to estimate the value of Δ of your sound board, paying attention to the fact that some software/hardware combination use automatic gain control (AGC). You probably need to disable AGC to better control the acquisition.\n\n\n\nIt is not trivial, but if you want to learn more about your sound board, try to evaluate its performance according to the procedure described at [url1bau].    □\n\n\n\n Application 1.6. Using rat in Matlab/Octave to find the period of discrete-time sinusoids. The Matlab/Octave function rat for rational fraction approximation can be used for finding m and N.\nBut care must be exercised because rat approximates the input argument within a given tolerance. The code below illustrates how this function can be used to obtain m and N:\n\n\nw=3*pi/5 %define some angular frequency (rad) \n[m,N]=rat(w/(2*pi)) %find m and N\n\n\nIn this case, the result is m=3, N=10, as expected. However, note that w=0.2,[m,N]=rat(w/(2pi)) returns m=113, N=3550, which is not precise (recall that if Ω = 0.2 the sinusoid is non-periodic). Modifying the previous command to use a smaller tolerance w=0.2,[m,N]=rat(w/(2pi),1e-300) gives much larger values for m,N, which clearly indicates that the user must be aware that rat uses approximations. Make sure you can generate discrete-time sinusoids with distinct values of m and N and understand the roles played by these two values.    □\n\n\n\n Application 1.7. Power of the sum of two signals. Assume a signal z[n] = x[n] + y[n] is generated by summing two real signals (similar result can be obtained for complex-valued signals) x[n] and y[n] with power Px and Py. The question is: What is the condition for having Pz = Px + Py?\n\n\n\nAssuming the two signals are random and using expected values (a similar result would hold for deterministic signals):\n\n\n\n\n\n\n\n\n\n Pz = 𝔼[Z2] = 𝔼[(X + Y)2] = P x + Py + 2𝔼[XY]. \n\n\n(1.69)\n\n\n\n\n\n\nIf X and Y are uncorrelated, i. e., 𝔼[XY] = 𝔼[X]𝔼[Y]\nand at least one signal is zero-mean, Eq. (1.69) simplifies to\n\n\n\n\n\n\n\n\n\n Pz = Px + Py. \n\n\n(1.70)\n\n\n\n\n\n\nThis is a useful result for analyzing communication channels that model the noise as additive. These models assume the noise is uncorrelated to the transmitted signal and Eq. (1.70) applies.   □\n\n\n\n Application 1.8. Estimate the PDF of speech signals. Via a normalized histogram, estimate the PDF of a speech signal with a long duration. After this estimation, you should convince yourself that uniform quantizers are not adequate for speech signals. In fact, when using a non-linear quantizer based on the A-law or μ-law, it is possible to use only 8 bits to achieve the subjective quality of a linear PCM with 12 bits. Observe whether or not your histogram approaches a Laplacian density, as suggested by previous research in speech coding.    □\n\n\n\n Application 1.9. A simple application of correlation analysis. A company produces three distinct beauty creams: A, B and C. The task is the analysis of correlation in three databases, one for each product. The contents of each database can be represented by two vectors x and y, with 1,000 elements each. Vector x informs the age of the consumer and y the number of his/her purchases of the respective cream (A, B or C) during one year, respectively. Figure 1.64 depicts scatter plots corresponding to each product.\n\n\n\n\n\n\n\n\nFigure 1.64: Scatter plot of customer age versus purchased units for three products. These two variables present positive correlation for product B, negative for C and are uncorrelated for product A.\n\n\n\n\n\nThe empirical (the one calculated from the available data) covariance matrices and means were approximately the following: Ca =  [  4.08  −0.002       −0.002   0.98  ]and μa = [30.0,6.05], Cb =  [  4.00  0.99       0.99   1.00  ]and μb = [30.1,12.0], Cc =  [  4.16  −1.46       −1.46   2.02  ]and μc = [30.13,7.99]. The correlation coefficients are ρa = −0.0011, ρb = 0.4924 and ρc = −0.5031.\n\n\n\nThe plots and correlation coefficients indicate that when age increases, the sales of product B also increases (positive correlation). In contrast, the negative correlation of ρb indicates that the sales of product C decreases among older people. The sales of product A seem uncorrelated with age. The script figs_signals_correlationcoeff.m allows to study how the data and figures were created. Your task is to learn how to generate two-dimensional Gaussians with arbitrary covariance matrices.\n\n\n\nNote that the correlation analysis was performed observing each product sales individually. You can assume the existence of a unique database, where each entry has four fields: age, sales of A, B and C. What kind of analysis do you foresee? For example, one could try a marketing campaign that combines two product if their sales are correlated. Or even use data mining tools to extract association rules that indicate how to organize the marketing.    □\n\n\n\n Application 1.10. Playing with the autocorrelation function of white noise and sinusoids. Using randn in Matlab/Octave, generate a vector corresponding to a realization of a WGN process (see Example 1.50): x=randn(1,1000). Check whether or not it is Gaussian by estimating the FDP (use hist). Plot its autocorrelation with the proper axes. Generate a new signal that is uniformly distributed: y=rand(1,1000)-0.5; and plot the same graphs as for the Gaussian signal. What does it happen with the autocorrelation if you add a DC level (add a constant to x and y)? And what if you multiply by a number (a “gain”)? Generate a cosine T=0.01; t=0:T:10-T; z=cos(2pi10*t); of 10 Hz with a sampling frequency of 100 Hz. Take a look at the autocorrelation for lags from m = −30,…,30 with [c,lags]=xcorr(z,30,’biased’);plot(lags,c). Compare this last plot with a zoom of the cosine: plot(z(1:30)). Note that they have the same period. In fact, an autocorrelation R of x incorporates all the periodicity that is found in x as indicated by Eq. (1.64). Make sure you can use Eq. (1.64) to predict the plots you obtain with xcorr when the signal is a sinusoid.    □\n\n\n\n Application 1.11. Using autocorrelation to estimate the cycle of sunspot activity. The international sunspot number (also known as the Wolfer number) is a quantity that simultaneously measures the number and size of sunspots. A sunspot is a region on the Sun’s surface that is visible as dark spots. The number of sunspots correlates with the intensity of solar radiation: more sunspots means a brighter sun. This number has been collected and tabulated by researchers for around 300 years. They have found that sunspot activity is cyclical and reaches its maximum around every 9.5 to 11 years (in average, 10.4883 years).19 The autocorrelation can provide such estimate as indicated by the script below. Note that we are not interested in R(0), which is always the maximum value of R(τ). The lag of the largest absolute value of R(τ) other than R(0) indicates the signal fundamental period. Because theoretically no other value can be larger than R(0), the task of automatically finding the second peak (not the second largest sample), which is the one of interest, is not trivial. The code snippet below simply (not automatically) indicates the position of the second peak for the sunspot data.\n\n\n\nListing 1.31: MatlabOctaveCodeSnippets/snip_signals_peak_detection.m\n\n\nload sunspot.dat; %the data file \nyear=sunspot(:,1); %first column \nwolfer=sunspot(:,2); %second column \n%plot(year,wolfer); title('Sunspot Data') %plot raw data \n5x=wolfer-mean(wolfer); %remove mean \n[R,lag]=xcorr(x); %calculate autocorrelation \nplot(lag,R); hold on; \nindex=find(lag==11); %we know the 2nd peak is lag=11 \nplot(lag(index),R(index),'r.', 'MarkerSize',25); \n10text(lag(index)+10,R(index),['2nd peak at lag=11']);\n  \n\n\nFigure 1.65 shows the graph generated by the companion script figs_signals_correlation.m. It complements the previous code snippet, showing how to extract the second peak automatically (this can be useful in other applications of the ACF). Your task is to study this code and get prepared to work with “pitch” estimation in Application 1.12.\n\n\n\n\n\n\n\n\nFigure 1.65: Autocorrelation of the sunspot data.\n\n\n\n\n\nAn important aspect of the sunspot task is the interpretation of R(τ). As discussed, when the autocorrelation has a peak, it is an indication of high similarity, i. e., periodicity. In the sunspot application, the interval between two lags was one year. If the ACF is obtained from a signal sampled at Fs Hz, this interval between lags is the sampling period Ts = 1∕Fs and it is relatively easy to normalize the lag axis. The next example illustrates the procedure.    □\n\n\n\n Application 1.12. Using autocorrelation to estimate the “pitch”. This application studies a procedure to record speech, estimate the average fundamental frequency F0 (also erroneously but commonly called pitch) via autocorrelation and play a sinusoid with a frequency proportional to F0.\n\n\n\nOne can estimate the fundamental frequency of a speech signal by looking for a peak in the delay interval corresponding to the normal pitch range in speech.20 The following script illustrates the procedure.\n\n\n\nListing 1.32: MatlabOctaveCodeSnippets/snip_signals_fundamental_frequency.m\n\n\nFs=44100; %sampling frequency \nTs=1/Fs; %sampling interval \nminF0Frequency=80; %minimum F0 frequency in Hz \nmaxF0Frequency=300; %minimum F0 frequency in Hz \n5minF0Period = 1/minF0Frequency; %correponding F0 (sec) \nmaxF0Period = 1/maxF0Frequency; %correponding F0 (sec) \nNbegin=round(maxF0Period/Ts);%number of lags for max freq. \nNend=round(minF0Period/Ts); %number of lags for min freq. \nif 0 %record sound or test with 300 Hz cosine \n10    r = audiorecorder(Fs, 16, 1);%object audiorecorder \n    disp('Started recording. Say a vowel a, e, i, o or u') \n    recordblocking(r,2);%record 2 s and store in object r \n    disp('finished recording'); \n    y=double(getaudiodata(r, 'int16'));%get recorded data \n15else %test with a cosine \n    y=cos(2*pi*300*[0:2*Fs-1]*Ts); %300 Hz, duration 2 secs \nend \nsubplot(211); plot(Ts*[0:length(y)-1],y); \nxlabel('time (s)'); ylabel('Signal y(t)') \n20[R,lags]=xcorr(y,Nend,'biased'); %ACF with max lag Nend \nsubplot(212); %autocorrelation with normalized abscissa \nplot(lags*Ts,R); xlabel('lag (s)'); \nylabel('Autocorrelation of y(t)') \nfirstIndex = find(lags==Nbegin); %find index of lag \n25Rpartial = R(firstIndex:end); %just the region of interest \n[Rmax, relative_index_max]=max(Rpartial); \n%Rpartial was just part of R, so recalculate the index: \nindex_max = firstIndex - 1 + relative_index_max; \nlag_max = lags(index_max); %get lag corresponding to index \n30hold on; %show the point: \nplot(lag_max*Ts,Rmax,'xr','markersize',20); \nF0 = 1/(lag_max*Ts); %estimated F0 frequency (Hz) \nfprintf('Rmax=%g lag_max=%g T=%g (s) Freq.=%g Hz\\n',... \n    Rmax,lag_max,lag_max*Ts,F0); \n35t=0:Ts:2; soundsc(cos(2*pi*3*F0*t),Fs); %play freq. 3*F0\n  \n\n\nFigure 1.66 was generated using the previous script with the signal y(t) consisting of a cosine of 300 Hz instead of digitized speech (simply change the logical condition of the “if”).\n\n\n\n\n\n\n\n\nFigure 1.66: Autocorrelation of a cosine of 300 Hz. The autocorrelation is also periodic. The period in terms of lags is 1/300 s.\n\n\n\n\n\nThe code outputs the following results:Rmax=0.49913 lag_max=147 T=0.00333333 (sec) Frequency=300 HzNote that the autocorrelation was normalized xcorr(y,Nend,’biased’), which led to R(0) ≈ 0.5 Watts, coinciding with the sinusoid power A2∕2, where A = 1 V is the sinusoid amplitude.\n\n\n\nAs commonly done, in spite of dealing with discrete-time signals, the graphs assume the signals are approximating a continuous-time signal and ACF. Hence, the abscissa is t, not n.\n\n\n\nSome PC sound boards heavily attenuate the signals around 100 Hz. Therefore, the last command multiplies the estimated F0 by 3, to provide a more audible tone. Modify the last line of the code to use F0 instead of 3F0 and observe the result of varying F0 with your own voice. Then try to improve the code to create your own F0 estimation algorithm. Find on the Web a Matlab/Octave F0 (or pitch) estimation algorithm to be the baseline (there is one at [url1pit]) and compare it with your code. Use the same input files for a fair comparison and superimpose the pitch tracks to spectrograms to better compare them. If you enjoy speech processing, try to get familiar with Praat [url1pra] and similar softwares and compare their F0 estimations with yours.    □\n\n\n\n Application 1.13. Using cross-correlation for synchronization of two signals or time-alignment. Assume a discrete-time signal x[n] is transmitted through a communication channel and the receiver obtains a delayed and distorted version y[n]. The task is to estimate the delay imposed by the channel. The transmitter does not “stamp” the time when the transmission starts, but uses a predefined preamble sequence p[n] that is known by the receiver. The receiver will then guess the beginning of the transmitted message by searching for the preamble sequence in y[n] via cross-correlation. Before trying an example that pretends to be realistic, some simple manipulations can clarify the procedure.\n\n\n\nAssume we want to align the signal x[n] = δ[n] + 2δ[n − 1] + 3δ[n − 2] with y[n] = 3δ[n] + 2δ[n − 1] + δ[n − 2] + δ[n − 3] + 2δ[n − 4] + 2δ[n − 5]. Intuitively, the signal x[n − 3] is a good match to y[n]. Alternatively, y[n + 3] matches x[n].\nThe Matlab/Octave command xcorr(x,y) for cross-correlation can help finding the best lag L such that x[n + L] matches y[n]. The procedure is illustrated below:\n\n\n\nListing 1.33: MatlabOctaveCodeSnippets/snip_signals_cross_correlation.m\n\n\nx=1:3; %some signal \ny=[(3:-1:1) x]; %the other signal \n[c,lags]=xcorr(x,y); %find cross-correlation \nmax(c) %show the maximum cross-correlation value \n5L = lags(find(c==max(c))) %lag for max cross-correlation \nstem(lags,c); %plot \nxlabel('lag (samples)'); ylabel('cross-correlation')\n  \n\n\nThe result is L = −3. If the order is swapped to xcorr(y,x) as below, the result is L = 3.\n\n\n[c,lags]=xcorr(y,x); \nmax(c) %show the maximum cross-correlation value \nmaxlag=lags(find(c==max(c))) %max cross-correlation y,x lag\n\n\nIt should be noticed that the cross-correlation is far from perfect with respect to capturing similarity between waveforms. For example, if y[n] is changed to y=[(4:-1:1) x], the previous commands would indicate the best lag as L = 1. The reader is invited to play with simple signals and find more evidence of this limitation. As a rule of thumb, the cross-correlation will work well if one of the signals is a delayed version of the other, without significant distortion. However, in situations such as reverberant rooms where one of the signals is composed by a sum of multi-path (with distinct delays) versions of the other signal, more sophisticated techniques should be used.\n\n\n\nAnother aspect is that, in some applications, the best similarity measure is the absolute value of the cross-correlation (i. e., L = lags(find(abs(c)==max(abs(c)))) instead of L = lags(find(c==max(c)))). For example, this is the case when x[n] can be compared either to y[n] or  − y[n].\n\n\n\nListing 1.34 illustrates the delay estimation between two signals x[n] and y[n]. The vector y, representing y[n], is obtained by delaying x and adding Gaussian noise to have a given SNR.\n\n\n\nListing 1.34: MatlabOctaveCodeSnippets/snip_signals_time_delay.m\n\n\nFs=8000; %sampling frequency \nTs=1/Fs; %sampling interval \nN=1.5*Fs; %1.5 seconds \nt=[0:N-1]*Ts; \n5if 1 \n    x = rand(1,N)-0.5; %zero mean uniformly distributed \nelse \n    x = cos(2*pi*100*t); %cosine \nend \n10delayInSamples=2000; \ntimeDelay = delayInSamples*Ts %delay in seconds \ny=[zeros(1,delayInSamples) x(1:end-delayInSamples)]; \nSNRdb=10; %specified SNR \nsignalPower=mean(x.^2); \n15noisePower=signalPower/(10^(SNRdb/10)); \nnoise=sqrt(noisePower)*randn(size(y)); \ny=y+noise; \nsubplot(211); plot(t,x,t,y); \n[c,lags]=xcorr(x,y); %find crosscorrelation \n20subplot(212); plot(lags*Ts,c); \n%find the lag for maximum absolute crosscorrelation: \nL = lags(find(abs(c)==max(abs(c)))); \nestimatedTimeDelay = L*Ts\n  \n\n\nFigure 1.67 illustrates the result of running the previous code. The random signals have zero mean and uniformly distributed samples. The estimated delay via xcorr(x,y) was -0.25 s. The negative value indicates that x is advanced with respect to y. The command xcorr(y,x) would lead to a positive delay of 0.25 s.\n\n\n\n\n\n\n\n\nFigure 1.67: First graph shows signals x(t) and y(t − 0.25) contaminated by AWGN at an SNR of 10 dB. The signal y(t) can be identified by a smaller amplitude in the beginning, where its samples are due to noise only. The second graph shows their cross-correlation RXY (τ) indicating the delay of -0.25 s. \n\n\n\n\n\nAfter running the code as it is, observe what happens if x=rand(1,N), i. e., use a signal with a mean different than zero (0.5, in this case). In this case, the correlation is affected in a way that the peak indicating the delay is less pronounced. Another test is to use a cosine (modify the if) with delayInSamples assuming a small value with respect to the total length N of the vectors. The estimation can fail, indicating the delay to be zero. Another parameter to play with is the SNR. Use values smaller than 10 dB to visualize how the correlation can be useful even with negative SNR.\n\n\n\nIt is important to address another issue: comparing vectors of different length. Assume two signals x[n] and y[n] should be aligned in time and then compared sample-by-sample, for example to calculate the error x[n] − y[n]. There is a small problem for Matlab/Octave if the vectors have a different length. Assuming that xcorr(x,y) indicated the best lag is positive (L &gt; 0), an useful post-processing for comparing x[n] and y[n] is to delete samples of x[n]. If L is negative, the first samples of y[n] can be deleted. Listing 1.35 illustrates the operation and makes sure that the vectors representing the signals have the same length.\n\n\n\nListing 1.35: MatlabOctaveCodeSnippets/snip_signals_time_aligment.m\n\n\nx=[1 -2 3 4 5 -1]; %some signal \ny=[3 1 -2 -2 1 -4 -3 -5 -10]; %the other signal \n[c,lags]=xcorr(x,y); %find crosscorrelation \nL = lags(find(abs(c)==max(abs(c)))); %lag for the maximum \n5if L&gt;0 \n  x(1:L)=[]; %delete first L samples from x \nelse \n  y(1:-L)=[]; %delete first L samples from y \nend \n10if length(x) &gt; length(y) %make sure lengths are the same \n  x=x(1:length(y)); \nelse \n  y=y(1:length(x)); \nend \n15plot(x-y); title('error between aligned x and y');\n  \n\n\nElaborate and execute the following experiment: record two utterances of the same word, storing the first in a vector x and the second in y. Align the two signals via cross-correlation and calculate the mean-squared error (MSE) between them (for the MSE calculation it may be necessary to have vectors of the same length, as discussed).    □\n\n\n \n\n18 Note that some computers do not have a sound input connector (microphone or line in) and an external (e. g., USB-based) sound board would be required.\n\n \n\n19 See, e. g., [url1sun] for more information. Note also that Matlab has a script sunspots.m that works in the frequency domain.\n\n \n\n20 Adult male speakers have a pitch range typically in the range [110, 130] Hz, while females have pitch in [200, 230] Hz. You may get an idea if you like Hollywood: it has been said that Sean Connery, Mel Gibson, Barbra Streisand and Julia Roberts have an average pitch of 158, 108, 228 and 171 Hz, respectively.\n\n                                                                                                              &lt;/div&gt;"
  },
  {
    "objectID": "ak_dsp_bookse15.html",
    "href": "ak_dsp_bookse15.html",
    "title": "15  Comments and Further Reading",
    "section": "",
    "text": "Signal processing books sometimes target two distinct audiences. The most introductory treatments are typically adopted for “Signals and Systems” courses, while more advanced textbooks target “Digital Signal Processing” courses. This division can be observed in the two classic books [?] and [?]. It is also used in the Schaum’s Outline Series (well-known for the many fully solved problems).\n\n\n\nRegarding the name digital or discrete-time: the jargon is such that people call digital signal processing (DSP) many operations that should be considered discrete-time signal processing given the amplitude is not quantized.21\n\n\n\nThere are many great books on DSP and three of them are (always check for new editions): [?,?,?]. Youtube has several channels with videolectures about signal processing such as the ones by Prof. Barry Van Veen [url1you].\n\n\n\nWhen reading DSP and telecommunication books, keep in mind that the taxonomy of signals adopted here is not the only one used. For example, books such as  [?,?] define digital signals as the ones with quantized amplitudes. Their alternative definition considers that xq(t), a continuous-time signal, is digital because of its quantized amplitudes.\n\n\n\nTopics such as sampling and quantization have been widely investigated. The sampling theorem is part of a broad area. Only periodic sampling is discussed here. There are many other sampling theorems, addressing issues such as non-uniform sampling and non-bandlimited signals. See, e. g.,  [?,?]. In  [?] a whole chapter is dedicated to sampling. As a side note: the sampling theorem is related to the work by Harry Nyquist and, therefore, called Nyquist theorem by some authors. However, the credits for the theory related to the sampling theorem should also go to C. Shannon, V. Kotelnikov, E. Whittaker and others. See, for example,  [?,?], for historical information.\n\n\n\nAs discussed in [?], to be more pedagogical, signal processing textbooks typically do not adopt rigorous mathematical treatment of continuous-time impulses. This was the approach adopted in this text.\n\n\n\nQuantization is also part of a vast area known as source coding. A classical and good book is  [?]. A more modern treatment can be found in  [?].\n\n\n\nRegarding quantization, it is important to warn the experienced reader that in this text, unless otherwise stated (e. g., when discussing PAM decoding), it is assumed the quantizers are uniform and mid-tread. Non-uniform and mid-riser quantizers (see  [?]) are not discussed.\n\n\n\nSome authors prefer to reserve the unit Hertz (Hz) to only represent the oscillation rate of signals, electrical fields, etc., sticking with its historical use. With this motivation, they avoid, for example, to denote the sampling frequency Fs of an ADC in Hz and use samples per second (SPS) instead. In this text, Hz is “overloaded” to be a unit that can account for everything repeating a given number of times per second and Fs is reported in Hz.\n\n\n \n\n21 This is a possible reason for the change in the book title “Digital signal processing”  [?] to “Discrete-time signal processing”  [?].\n\n                                                                           &lt;/div&gt;"
  },
  {
    "objectID": "ak_dsp_bookse16.html",
    "href": "ak_dsp_bookse16.html",
    "title": "16  Review Exercises",
    "section": "",
    "text": "If you are familiar with the material, you probably want to skip this section and check the exercises in Section 1.17.\n\n\n\n1.1. What is the amplitude of a sinusoid x(t) with 30 dBm of power? And the amplitude of another sinusoid y(t) with 30 dB more power than x(t)? Is it correct to say that a signal has 30 dB of power? You can find the expression for the power of a sinusoid in Example 1.47. And you may want to read Appendix B.24 for a discussion about dB. 1.2. One has a discrete-time sinusoid with amplitude A = 10 Volts and wants to generate discrete-time AWGN with power P such that the SNR equals 30 dB. What is the value of P in Watts? What are the Matlab/Octave commands to generate both signals? 1.3. To review the alternatives to represent complex numbers, let a = 4ej0 and b = 3ejπ∕2 be two complex numbers and calculate: c = ab, d = a + b, |c| and ∠c. Note that the following two forms of representing complex numbers in the polar notation are equivalent: mejΘ and m∠Θ. 1.4. Calculate by hand and plot the signal x[n] = Cean for n = 0,1,…,3, where C = 3 + j4 and a = 2 + jπ. Note that any complex-valued signal x[n]\ncarries information about two real-valued signals and, describing x[n] with graphs requires one for its real component and another for its imaginary component. Alternatively, the two graphs can describe the magnitude and phase of x[n]. 1.5. Describe in details the header and summarize the contents (list all the information you could extract from the header such as number of samples, sampling frequency, etc.) of the following companion files: tidigits_saa7.wav, timit_testdr8mpam0sx199.raw and timit_testdr8mpam0sx199.wav. These are files that store speech waveforms (TIDIGITS and TIMIT are famous corpora distributed by LDC – [url1ldc]). Choose one of these files and analyze the corresponding signal via two plots: the waveform and the histogram of amplitudes, both with the abscissa and ordinate properly labeled. 1.6. Using your favorite programming language, list the code for reading raw (without header) files storing two kinds of data: a) floats (4 bytes) in little-endian format and b) shorts (2 bytes) in big-endian format. If you like the C language, you may find useful the companion code laps_dump.c. 1.7. Assume the random vector\n\n\n\n\n\n\n X = [3;2;3;2;2;0;1;0;0;3;0;2;3;2;2;3;3;0;3;2;0]. \n\n\n\n\n\n\n\nCalculate its histogram. b) Estimate its probability mass function (PMF). c) Calculate the moments: mean 𝔼[X], variance 𝔼[(X − μx)2] and 𝔼[X2]. You may find useful the discussion in Section 1.9.2 and Appendix B.19.3. 1.8. For calculating the variance using its definition 𝔼[(X − μx)2] one has to go twice over the data samples. The first loop obtains the mean μx and the second loop calculates 𝔼[(X − μx)2]. Show a code that uses only one pass over the data by adopting the expression\n\n\n𝔼[(X − μx)2] = 𝔼[X2] − μx2 discussed in Appendix B.19.3. 1.9. a) Generate a realization (one waveform) of a Gaussian random process with 100 i.i.d. (independent and identically distributed) samples of a Gaussian with mean 4 and variance 3, that is, N(4,3). b) Generate a waveform with independent but not identically distributed samples: the samples with odd indexes are draw from N(4,3), while the samples with even indexes are draw from a uniform distribution U(5,7) with support [5,7]. You can check Section 1.9.2. 1.10. If the task is the generation of a vector of independent and identically distributed (i. i. d.) samples of random variables, one can simply generate each one independently and then organize them in a single vector. But if the task is to generate a vector with a given correlation among its elements, then a more sophisticated approach is required. Here, we practice the generation of two-dimensional Gaussian random vectors drawn from a PDF fX 1,X 2(x1,x2) with mean μ = (2,3)T  and a given covariance matrix. Use the Matlab/Octave’s function mvnrnd or, if you do not have Matlab’s Statistics Toolbox installed, use the companion octave_mvnrnd instead. An example follows:\n\n\n\nListing 1.36: MatlabOctaveCodeSnippets/snip_signals_2Drandom.m\n\n\nN = 100000; %number of 2-d vectors \nmu=[2 3]; %mean \nC=[1 0.5 ; 0.5 10]; %covariance matrix \nr = octave_mvnrnd(mu,C,N); %octave_mvnrnd or mvnrnd \n5numbiny = 30; numbinx = 30; %number of bins for histogram \nCest=cov(r) %check estimated covariance matrix (should be close to C) \nmu_est=mean(r) %estimated mean \nR=C+mu'*mu %theoretical correlation matrix \nRest=Cest + mu_est'*mu_est %estimated correlation matrix \n10[n,xaxis,yaxis]=ak_hist2d(r(:,1),r(:,2),numbinx,numbiny); %histogram \nmesh(xaxis,yaxis,n); pause %plot histogram \ncontour(xaxis,yaxis,n); xlabel('x1'); ylabel('x2'); %and its countour\n\n\n\nThe task is to generate realizations of two-dimensional Gaussian random variables, and compare their estimated probability mass functions (PMFs) for the following covariance matrices: a) C = σ2I =  [  σ2  0        0  σ2   ] , σ2 = 0.5. b) C =  [  σ12  0        0  σ22   ] , σ12 = 0.5 and σ22 = 4. c) C =  [  σ12  σ12        σ21  σ22   ] , σ12 = σ21 = 0.5\nand σ12 = σ22 = 1. Can you observe the interrelation between the correlations (σ12 and σ21, which are always equal because a covariance matrix is symmetric) and variances (σ12 and σ22) with the shape of the respective PMFs?"
  },
  {
    "objectID": "ak_dsp_bookse17.html",
    "href": "ak_dsp_bookse17.html",
    "title": "17  Exercises",
    "section": "",
    "text": "Unless specified otherwise, assume that the signals are in volts and were obtained over a resistor of 1 Ohm. Always indicate the units in your answers and graphs.\n\n\n\n1.1. Both continuous-time x(t) and the discrete-time x[n] signals consist of pulses with a limited duration (finite support). Their amplitudes are equal to three for 0 ≤ t ≤ 4 and 0 ≤ n ≤ 4, respectively, and zero otherwise. a) What are the values of x(t) when t = 1.5, 3 and 6? b) What are the values of x[n] when n = 1.5, 3 and 6? 1.2. An ADC uses b = 8 bits per sample and sampling frequency Fs = 100 Msps. a) What is the respective bit rate in bits per second? b) How many megabytes are needed to store 1 hour of a signal digitized with this ADC and stored in a binary “raw” format (without compression or a file header)? 1.3. Using an Internet browser, practice choosing commercial ADCs and DACs. Some companies of interest are Analog Devices, Maxim and Texas Instruments. Use their “parametric search” tools. Assume you have to choose chips for three projects with distinct requirements: 1) the fastest ADC and DAC with at least 16 bits per sample, 2) Low cost chips with 8 bits per sample to work with Fs up to 10 kHz and 3) high precision chips to work with Fs around 100 Hz in medical applications. In your comparison, indicate at least resolution, speed, price, power consumption, supply current and if the data bus is serial or parallel and inform the interface (e. g., SPI). Extra parameters you may include are full scale range (FSR), total harmonic distortion (THD), effective number of bits (ENOB) and\noffset error. 1.4. Get familiar with digitizing systems and boards, which are sometimes called DAQ (data acquisition) boards. Calculate the storage space and transfer rate for digitized signals (visit [url1bww] for extra information). a) Calculate the total space in megabytes (MB) for storing 30 minutes of a signal sampled at the maximum rate of the following data transfer technologies:\n\n &lt;ul class='itemize1'&gt;\n &lt;li class='itemize'&gt;PCI: 2133 Mbit/s (266.7 MB/s)\n &lt;/li&gt;\n &lt;li class='itemize'&gt;Serial ATA (SATA-300): 3000 Mbit/s (375 MB/s)\n &lt;/li&gt;\n &lt;li class='itemize'&gt;USB 2.0: 480 Mbit/s (60 MB/s)\n &lt;/li&gt;\n &lt;li class='itemize'&gt;Serial RS-232 (max): 0.2304 Mbit/s (0.0288 MB/s)&lt;/li&gt;&lt;/ul&gt;\n\n\n\nAssume you need to use a 16-bits A/D process to achieve the desired SNR, what is the maximum sampling rate that needs to be supported for each interface above? c) Describe in high-level a digitizer system to sustain a sampling rate of 40 MHz and store 3 hours of a signal. Choose the data transfer technology, total hard disk space, etc.). d) Considering you must use USB 2.0: what is the maximum sampling rate the system could achieve in this case? e) Evaluate a Signatec [url1sig] waveform recording product and indicate what is the maximum throughput that Signatec offers (indicate sampling rate and number of bits per sample) for recording some hours of signal into a hard disk. Note that when operating at maximum sampling rate, most acquisition boards and digital oscilloscopes store the ADC samples in a limited amount of onboard RAM, which is typically capable of storing only few seconds of signal. The discussed recording system must take into account the data transfer from onboard RAM to hard disk. 1.5. The analog signal x(t) = 2t V is digitized using a 2-bits ADC with sampling frequency Fs = 100 Hz to create a digital signal xq[n]. Assuming the first sample is obtained at t = 0 s, inform: a) the values of the first four samples (just use here sampling, without quantization) and b) the values of xq[n] for n = 0,…,3 given that the ADC uses rounding and its output levels are\n\n\n{−2Δ,−Δ,0,Δ}, with Δ = 0.025 V. 1.6. Note the several distinct meanings of the word digital depending on the context: an FPGA chip implements digital logic, the signal analyzer has both analog and digital inputs, etc. We want to discuss whether the waveform at a microprocessor data bus pin is a digital signal or not. But according to the definitions adopted in this text, not the nomenclature used in a text dealing with digital electronics. For that, we will assume two cases of a waveform observed with an analog oscilloscope: a) the amplitudes assume only two values: 0 and 5 V, and b) the waveform corresponds to non-ideal pulses, with e. g. non-zero rise and fall times. In these two cases, what are the signal categories according to Table 1.1)? 1.7. Draw the graph of x[n] = n2(u[n − 3] − u[n − 7]). 1.8. Let [3,4,5] be the amplitudes of the three samples (n = 0, 1 and 2, respectively) of a discrete-time x[n] obtained by sampling a speech signal. a) Describe the result of a D/C conversion of x[n] to a sampled signal xs(t) with sampling period Ts = 4 s and draw the graph of xs(t) specifying the abscissa in seconds. b) What are the amplitude values of xs(t) at t = 1.5 and 8 s? 1.9. a) Manually (do not use a computer) draw the graph of the sampled signal xs(t) = ∑ ⁡ k=0∞(−1)kδ(t − k) indicating values at both abscissa and ordinate. 1.10. Given x(t) as depicted in Figure 1.68, clearly draw the graphs of a) x(−t − 3), b) x(4t) and c) x(t∕2).\n\n\n \n\nFigure 1.68: Example of continuous-time signal.\n\n\n\n\n1.11. a) Manually draw the graph of\n\n\nx[n] = δ[n + 2] + δ[n + 1] + δ[n] + δ[n − 1] + δ[n − 2] + 0.4δ[n + 3]\n\n. b) Do the same for the signals:\n\n\ny[n] = x[n + 1]\n\n,\n\n\nz[n] = x[n + 1]δ[n − 1]\n\nand\n\n\nw[n] = 3x[n2]\n\n. c) Convert\n\n\nx[n]\n\nto a sampled signal\n\n\nxs(t)\n\nadopting\n\n\nFs = 2\n\nkHz and draw the graph of\n\n\nxs(t)\n\n. 1.12. Carefully draw the signals: a)\n\n\nz(t) = rect  (t−3Ts Ts )\n\n, b)\n\n\nx(t) = sinc  ( t Ts )\n\nand c)\n\n\ny(t) = 2x(t + Ts) − x(t) + 2x(t − Ts) = 2sinc  (t+Ts Ts ) −sinc  ( t Ts ) + 2sinc  (t−Ts Ts )\n\n, with\n\n\nTs = 2\n\n s. You may use a computer to help with c). 1.13. An analog signal\n\n\nx(t)\n\nwas periodically sampled with sampling period\n\n\nTs = 2\n\n s to obtain the sampled signal\n\n\nxs(t) = 2δ(t + 4) − 5δ(t − 2) + 7δ(t − 6)\n\n. The sampling theorem was obeyed. a) What is the analytic expression of\n\n\nx(t)\n\n? b) Is this the only possible answer? For example, is\n\n\n2rect(t + 4) − 5rect(t − 2) + 7rect(t − 6)\n\na possible expression for\n\n\nx(t)\n\ngiven that\n\n\nTs = 2\n\n s and the sampling theorem was obeyed? Why? 1.14. Four frequencies generated by an ideal piano (in Hz) are:\n\n\nA7 = 3520\n\n,\n\n\nA6 = 1760\n\n,\n\n\nA5 = 880\n\nand\n\n\nA4 = 440\n\n. a) When a piano song is digitized with sampling frequency\n\n\nFs = 44100\n\n Hz, what are the digital angular frequencies in radians corresponding to A7 and A4? b) what should be the values of W1 and W2 such that the Matlab/Octave commands below\n\n\nFs=44100;Ts=1/Fs;N=3/Ts;n=0:N-1;x=cos(W1*n)+cos(W2*n);soundsc(x)\n\n\ngenerate 3 seconds of a sum of two cosines, corresponding to\nthe frequencies A6 and A5, respectively, still assuming the given Fs? c) Repeat a) and b) for Fs = 2 kHz. d) Adopting Fs = 2 kHz brings any trouble? Explain you answer. 1.15. An b = 8-bits ADC uses two’s complement to output binary numbers that correspond to integers in the range [−128,127]. The ADC’s quantizer is uniform with a step size Δ = 10 mV. a) For the following input values x[n] = [0,1,−1.28,1.28,2], indicate the output xb[n] = Q~c{x[n]} in binary, its corresponding value as an integer xd[n], the decoded output xq[n] and the quantization error. b) Repeat the procedure assuming the ADC uses a numbering scheme based on an offset 2b−1 = 128. 1.16. A sinusoid x[n] = sin ⁡ ((π∕3)n) was quantized with b = 3 bits per sample using a step size Δ = 0.25 and X^min = −1. This quantization generated the digital signal xq[n]. The corresponding binary values xb[n] = Q~c{x[n]} were stored in a file using a numbering scheme with offset 2b−1 = 4, but this file was wrongly interpreted with a routine that generated yq[n] assuming a two’s complement scheme. Inform the 6 values of a period of the original x[n], the properly quantized xq[n] and the erroneously interpreted yq[n]. 1.17. The goal is to design a uniform scalar quantizer for a discrete-time signal x[n]. It is given that x[n] is approximately uniformly distributed with mean equal to 2 and standard deviation equal to 3. a) Describe a 3-bits quantizer that minimizes the quantization error, explaining your design decisions. b) Calculate the signal to noise ratio (in dB) for the case of a 8 bits quantizer operating with the same input signal. 1.18. A 3-bits uniform quantizer has a step size\n\nΔ = 2 V. The input is x and the output xq. Its minimum output value is xq = −8. Assume four input values [20,0.7,−5.4,6.1] V and inform: a) the graph (“stairs”) showing xq × x for this quantizer, b) the quantization error eq for each of these input values and c) the power in Watts of eq considering only the four corresponding samples. 1.19. Assuming the input signal of a quantizer can be modeled by a Gaussian FDP with variance 4 W and mean 2 V. Design a 2-bits uniform quantizer that tries to minimize the quantization error. a) Draw the output versus input for this quantizer and b) calculate the quantization SNR dB assuming only errors in the granular region and that the linear model for quantization is valid. 1.20. Consider you want to quantize a sinusoid with peak amplitude that is equal to 10 Volts using b = 10 bits. Find general expressions for the quantization SNR in terms of b, using both linear and dB scales. If instead of a sinusoid your signal (to be quantized) had a Gaussian PDF with mean μ = 3 and standard deviation σ = 2, find similar expressions for the SNR assuming the signal dynamic range is 6σ around the mean. 1.21. Use an approach similar to Example 1.54, but assume the input signal is uniformly distributed, to prove the rule of thumb that says each extra bit of an ADC increases in 6 dB the quantization SNR. 1.22. A data acquisition system uses an ADC of 12 bits. To improve its resolution, a signal conditioning board is developed, based on an automatic gain control (AGC) circuit with programmable gain. This gain can assume 16 distinct values and is represented by 4 bits. Assuming the overall system now represents a sample with 12 + 4 = 16 bits, what is the expected improvement in the quantization SNR in dB due to the adoption of the AGC?\n1.23. An eight-bits ADC uses two’s complement and the Q2.5 format. The input signal is always within the quantizer’s granular region (no saturation). The quantizer is uniform, uses rounding and its output levels match the signal dynamic range given by [−3,3] V. a) What is the expected mean and variance of the quantization error assuming the linear model for quantization is valid? b) What is the estimated SQNR? c) What is the new SNR if Q3.4 is adopted and what could be an advantage of Q3.4 over Q2.5? 1.24. Represent the four numbers x[n] = [0.02,0,8.7,−2.5] using b = 8 bits and two’s complement. a) Compare the quantized values xq[n] when the following formats are adopted: Q4.3, Q3.4 and Q0.7. b) What are the corresponding dynamic ranges and step sizes for each of the three formats? 1.25. An embedded system generates the following thousand numbers xd=linspace(-1e30,1e30,1000) with Matlab/Octave (or import numpy; xd=numpy.linspace(-1e30,1e30,1000) with Python) and stores them in ROM using IEEE 754 in double precision. To reduce this amount of memory, an engineer considered the adoption of single precision, obtained with xf=single(xd) in Matlab/Octave (or xf=numpy.float32(xd) in Python). a) Calculate the required ROM size in bytes in both cases. b) The maximum absolute value of the error xd-xf. c) Is this error acceptable in your opinion? 1.26. a) Assuming xd=1e200 is a double (IEEE 754 in double precision), explain the result of converting it to 32 bits using xf=single(xd) in Matlab/Octave (you can do the same analysis in Python, if you prefer). b) Explain the discrepancy between the results of single(0.3)-0.2-0.1 and double(0.3)-0.2-0.1 in Matlab/Octave or between import numpy; print(numpy.float32(0.3)-0.2-0.1) and print(numpy.float64(0.3)-0.2-0.1) in Python. 1.27. Depict in plots the even and odd parts of x[n] = δ[n + 1] + 2δ[n] + 4δ[n − 1]. 1.28. You do not know much about a signal x[n], but it is given that x[0] = 3. Can x[n] be an odd signal? Why? 1.29. For the following signals, calculate the energy\n\nE∞ and power P∞. Do they have a finite total energy or a finite average power? a) x(t) = 5cos ⁡ (2π300t + π∕4), b) x(t) = 3e−2tu(t) and c) x(t) = t. 1.30. What can be said about the total energy and average power of any periodic signal? 1.31. a) The set M = {−7,−5,−3,−1,1,3,5,7} describes the output values of a given quantizer, in Volts. Assuming all elements have the same probability, what is the average energy in Joules of these values? b) Consider the same values compose a discrete-time sequence x[n] = [−7,−5,−3,−1,1,3,5,7]. What is the average power in Watts of x[n] over its support of eight non-zero samples? c) Given that the equation for both cases a) and b) is the same, discuss why the results are expressed in distinct units (Joules and Watts). 1.32. For each of the following signals, determine whether or not it is periodic and in positive case, its fundamental period N0: a) x[n] = u[n] + u[−n − 1], b) x[n] = cos ⁡ (π 8 n), c) x[n] = cos ⁡ ( 11 128n + π∕3) and d) x[n] = cos ⁡ ((π∕4)n2)? 1.33. a) Classify as energy or power signals: x(t) = cos ⁡ (2π100t), y(t) = 4cos ⁡ (2π100t + 0.2π) and z(t) = exp ⁡ (−t)u(t), where t is given in seconds. b) Calculate their autocorrelation R(τ), using the proper definition for power and energy signals, and indicate the unit of τ. 1.34. A signal x(t) = 10cos ⁡ (6πt) V is sampled to obtain N = 100 samples at t = 0,Ts,2Ts,…,99Ts s, where Ts = 1∕8 s. After C/D conversion, these samples compose the finite-duration sequence\n\nx[n]. What are the power values (in Watts) Pc of x(t) and Pd of x[n], assuming for x[n] only the interval n = 0 to 99? 1.35. The following commands were used to estimate the autocorrelation of a cosine: N=16; n=0:5N-1; x=cos(2pi/N*n); R=xcorr(x,’biased’); However, the result did not match the theoretical expression. Can you explain the reason? How could you obtain the proper result? Compare this result with Application 1.12. Why in this case autocorrelation seems to be periodic and match the theoretical result? 1.36. A sinc function sinc(τ) (see Section B.12) centered in τ = 0 can be an autocorrelation? What if it was centered in τ = 3 seconds? What is the interpretation to the fact that the autocorrelation achieves its maximum at τ = 0? What is the interpretation for sinc(0) (the autocorrelation at origin) if the adopted definition were: a) for energy signals and b) for random or power signals? 1.37. A signal x[n] was obtained using the randn function in Matlab/Octave, such that it has zero mean and unity variance. What is the power of this signal? How x[n] can be transformed in a signal y[n] with mean equal to four and variance equal to nine? What is the Matlab/Octave command to generate 100 samples of y[n] using randn? What is the average power of y[n]? Plot the following graphs: autocorrelation and probability density function of both x[n] and y[n]. 1.38. Prove that for the AWGN channel with noise ν(t) uncorrelated with the input signal s(t), the output of the received signal r(t) = s(t) + ν(t) is simply the sum of the power of s(t)\nand the power of ν(t). 1.39. Assuming x[n] = 3δ[n] − 2δ[n − 1] + δ[n − 2] and y[n] = (1 + j)δ[n] + (3 − j4)δ[n − 1], calculate the unbiased autocorrelations of x[n] and y[n], and their crosscorrelation. 1.40. The autocorrelation of a sinusoid x(t) = Asin ⁡ (ωt + ϕ) is R(τ) = 50cos ⁡ (30τ). What can be said about its amplitude A, angular frequency ω and phase ϕ? 1.41. In case you have access to the required equipment, estimate and describe in details the quantizer used by the sound system of some personal computer (describe the “stairs”: dynamic range and step size). Try to model the DC offset. 1.42. Learn how to manipulate wav files obtained from an audio CD and evaluate their histograms. Were the signals properly digitized? All (or most) quantizer levels were used? In case you find a CDA file, note that these files (of just 44 bytes) are not the actual audio files. They are just pointers to the audio data (similar to shortcut files). In order to copy the files, you need to use a rip software such as, e. g. [url1rip]."
  },
  {
    "objectID": "ak_dsp_bookch2.html",
    "href": "ak_dsp_bookch2.html",
    "title": "Transforms and Signal Representation",
    "section": "",
    "text": "2.1  To Learn in This Chapter  2.2  Linear Transform  2.2.1  Matrix multiplication corresponds to a linear transform  2.2.2  Basis: standard, orthogonal and orthonormal  2.3  Inner Products to Obtain the Transform Coefficients  2.4  Block Transforms  2.4.1  DCT transform  2.4.2  DFT transform  2.4.3  Haar transform  2.4.4  Unitary matrices lead to energy conservation  2.4.5  Orthogonal but not unitary also allows easy inversion  2.5  Fourier Transforms and Series  2.5.1  Fourier series for continuous-time signals  2.5.2  Discrete-time Fourier series (DTFS)  2.5.3  Continuous-time Fourier transform using frequency in Hertz  2.5.4  Continuous-time Fourier transform using frequency in rad/s  2.5.5  Discrete-time Fourier transform (DTFT)  2.6  Relating discrete and analog frequencies  2.7  Summary of equations for DFT / FFT Usage  2.8  Laplace Transform  2.9  Z Transform  2.9.1  Some pairs and properties of the Z-transform  2.9.2  Z-transform region of convergence  2.10  Applications  2.11  Comments and Further Reading  2.12  Review Exercises  2.13  Exercises"
  },
  {
    "objectID": "ak_dsp_bookse18.html",
    "href": "ak_dsp_bookse18.html",
    "title": "18  To Learn in This Chapter",
    "section": "",
    "text": "Apply basic concepts of linear algebra such as inner products and projections to better understand transforms\n\n\nUse inner products to efficiently obtain the transform coefficients when the basis functions are orthogonal\n\n\nInterpret transforms (Fourier, Z, Laplace) as obtaining coefficients given by the inner product between the signal to be transformed and the corresponding basis function\n\n\nObtain by inspection the Fourier series coefficients of periodic signals composed by harmonic sinusoids\n\n\n\n\nTransforms are a very important tool in several applications. The continuous-time Fourier transform, for example, provides an alternative “view” X(f) of a signal x(t). Sometimes this extra view is essential for efficiently solving a problem. Few examples of transforms and applications can be found in Table 2.1.\n\n\n\n\n\n\n\nTable 2.1: Examples of transforms and applications.\n\n\n\n\n\n\n\n\n\n\n\n\nTransform\n\n\n\n\n\nExample of application\n\n\n\n\n\n\n\nFourier\n\n\n\n\n\nvisualize a signal in frequency “domain” as a sum or integral of sinusoidal components\n\n\n\n\n\n\n\nZ\n\n\n\n\n\nanalyze discrete-time systems by transforming difference equations into polynomials\n\n\n\n\n\n\n\nDiscrete cosine transform (DCT)\n\n\n\n\n\nimage coding, where the image details are represented by high-frequency DCT coefficients, which can be discarded without significant loss of perceptual quality"
  },
  {
    "objectID": "ak_dsp_bookse19.html",
    "href": "ak_dsp_bookse19.html",
    "title": "19  Linear Transform",
    "section": "",
    "text": "The goal in this section is to analyze a signal (x(t) or x[n]) using a linear transform. The following steps will be discussed:\n\n\n\nf 1.\n\n\nWe will choose the linear transform.\n\n\nf 2.\n\n\nThe linear transform will represent the signal using its basis functions, which are often orthogonal among themselves.\n\n\nf 3.\n\n\nThe transform operation corresponds to finding the values called transform coefficients, which multiplied by the corresponding basis functions (one coefficient per basis functions) reconstruct the original signal.\n\n\n\n\nTo fully understand linear transforms, we will discuss these concepts using vectors and linear algebra. Later we will generalize from vectors to signals. We start this study of linear transform, associating it to a simple matrix multiplication.\n\n\n\n\n\n2.2.1  Matrix multiplication corresponds to a linear transform\n\n\n\nIn linear algebra, any linear transformation1 (or transform) can be represented by a matrix A. The linear transform operation is given by\n\n\n\n\n\n\n\n\n\n y = Ax, \n\n\n(2.1)\n\n\n\n\n\n\nwhere x and y are the input and output column vectors, respectively.\n\n\n\n Example 2.1. Example of linear transform. The matrix\n\n\n\n\n\n\n\n\n\n A =  [   cos ⁡ (𝜃)  sin ⁡ (𝜃)   − sin ⁡  (𝜃)    cos ⁡  (𝜃)   ] \n\n\n(2.2)\n\n\n\n\n\n\nimplements a transform that corresponds to a clockwise rotation of the input vector by an angle 𝜃.\n\n\n\n\n\n\n\n\nFigure 2.1: Rotation of a vector x by an angle 𝜃 = π∕2 radians using y = Ax with A given by Eq. (2.2).\n\n\n\n\n\nFigure 2.1 illustrates the rotation for a vector x = [4,8]T  by an angle 𝜃 = π∕2 radians, i. e., A = [0,1;−1,0], resulting in y = Ax = [8,−4]T .    □\n\n\n2.2.2  Basis: standard, orthogonal and orthonormal\n\n\n\nAnother important concept in linear transforms, which has origin in linear algebra, is the concept of basis. The linear combination of basis vectors allow to create any possible vector in the corresponding vector space. Many basis are orthogonal or orthonormal. Figure 2.1 indicates a pair of orthonormal2 vectors i¯ = [1,0] and j¯ = [0,1] that span ℝ2. The vectors i¯ and j¯ form a standard basis and allow to easily represent any vector y ∈ ℝ2, such as y = 8i¯ − 4j¯. It is useful to get a geometric interpretation by studying basis vectors, and later generalize the concepts to basis functions that represent discrete or continuous-time signals.\n\n\n\nWhen the basis vectors are organized as the columns of a matrix A, the elements of the input vector x indicate the coefficients of a linear combination of the basis functions that lead to y. For example, in the case of the standard basis:\n\n\n\n\n\n\n y =  [   8   − 4  ] =  [   1  0   0   1  ]  [   8   − 4  ] = Ax, \n\n\n\n\n\n\nwhich is a trivial relation because A is the identity matrix. More interesting transforms are used in practice.\n\n\n\n Example 2.2. Interpreting the given example as a linear transform. Assume one is dealing with computer graphics and wants to rotate vectors. Eq. (2.2) of Example 2.1 with 𝜃 = π∕2 radians leads to (see Figure 2.1):\n\n\n\n\n\n\n y =  [   8   − 4  ] =  [   cos ⁡ (π∕2)  sin ⁡ (π∕2)   − sin ⁡  (π∕2)    cos ⁡  (π∕2)   ]  [   4   8  ] =  [   0  1   − 1   0  ]  [   4   8  ] = Ax. \n\n\n\n\n\n\nThis can be interpreted as a transform as follows:\n\n\n\nf 1.\n\n\nWe chose A with 𝜃 = π∕2 as the linear transform.\n\n\nf 2.\n\n\nIn this case, the elements of y are interpreted as the transform coefficients, while x is interpreted as the original vector.\n\n\n\n\nThe direct transform operation corresponds to finding coefficients y. In the inverse operation, the coefficients y would be the input values, and x could be found using x = A−1y. In practice, we avoid the task of inverting a matrix and often choose A with special properties. For instance, because the columns of\n\nA are orthonormal, its inverse A−1 = AH is equal to its Hermitian (and the basis vectors are the rows of A). Tricks to avoid inverting matrices will be further explored, alongside with the adoption of inner products.   □\n\n\n \n\n1 See [url2tra].\n\n \n\n2 Vectors that are orthogonal and have norm equal to one.\n\n                                                &lt;/div&gt;"
  },
  {
    "objectID": "ak_dsp_bookse20.html",
    "href": "ak_dsp_bookse20.html",
    "title": "20  Inner Products to Obtain the Transform Coefficients",
    "section": "",
    "text": "Most linear transforms are designed such that “transforming” the original signal corresponds to the calculation of inner products. We can calculate an inner product between a pair of vectors, but also between a pair of functions, and a pair of signals. We start by discussing the inner product between vectors, to make useful analogies with the inner product between signals in the context of linear transforms.\n\n\n\nAn inner product is a generalization of the dot product x ⋅y between two vectors x and y of N elements or, equivalently, equal-length sequences (with N samples). The dot product is defined for vectors with real-valued elements as (the notation of an inner product ⟨x,y⟩ will be used hereafter instead of x ⋅y):\n\n\n\n\n\n\n\n\n\n ⟨x,y⟩≜∥x∥∥y∥cos ⁡ (𝜃), \n\n\n(2.3)\n\n\n\n\n\n\nwhere 𝜃 is the angle between y and x, which is restricted to the range 0 ≤ 𝜃 ≤ 180 degrees. Alternatively, this inner product can also be calculated as\n\n\n\n\n\n\n\n\n\n ⟨x,y⟩ = ∑ i=1Nx i yi = x1y1 + x2y2 + ⋯ + xNyN, \n\n\n(2.4)\n\n\n\n\n\n\nwhere xi and yi are the i-th elements of x and y, respectively.\n\n\n\nAnother aspect of the inner product ⟨x,y⟩ is that it is proportional to the norm of the projection of x on y and vice versa. Repeating Eq. (B.36) below, for convenience\n\n\n\n\n\n\n\n\n\n ∥pxy∥ = |⟨x,y⟩| ∥y∥ , \n\n\n(2.5)\n\n\n\n\n\n\nand interpreting y as a basis vector of unitary norm (∥y∥ = 1, one has ∥pxy∥ = |⟨x,y⟩|. A large magnitude of ⟨x,y⟩ corresponds to a basis vector y that represents well (a reasonable part of the energy) of the (signal) x. On the other hand, ⟨x,y⟩ = 0 means that the vectors are orthogonal.\n\n\n\n Example 2.3. Calculating the angle between vectors using their inner product. For example, consider x = [1,1,1] and y = [1,3,2]. Their inner product is\ncalculated from Eq. (2.4) as ⟨x,y⟩ = 6, such that cos ⁡ (𝜃) = ⟨x,y⟩∕(∥x∥∥y∥) ≈ 6∕(1.732 × 3.742) = 0.926 and consequently, 𝜃 = 0.387 radians.\n\n\n\nAs another example, if x = [3,−1,0] and y = [1,3,2], their inner product is ⟨x,y⟩ = 0. In this case, from Eq. (2.3) and observing that the norms of both vectors are non-zero, the inner product is zero necessarily because cos ⁡ (𝜃) = 0 and consequently, 𝜃 = π∕2. Hence, when vectors are perpendicular (𝜃 = π∕2), their inner product is zero.   □\n\n\n\nOrthogonal is a generalization of perpendicular. In general, when two vectors are orthogonal their inner product is zero.\n\n\n\nThere are many distinct definitions of inner products. But an operation must obey specific properties such as linearity to be “valid” as an inner product and, consequently, define an inner product space, where concepts such as norm and orthogonality are natural extensions of the ones with geometric interpretations provided by Eq. (2.3). This geometric interpretation is highly beneficial when interpreting the inner products used in transforms. Table 2.2 illustrates alternative definitions of inner products that are discussed in the sequel.\n\n\n\n\n\n\n\nTable 2.2: Examples of inner product definitions.\n\n\n\n\n\n\n\n\n\n\n\n\n\nEquation\n\n\n\n\n\nUsed for\n\n\n\nNumber \n\n\n\n\n\n\n\n∑ ⁡ i=1Nxi yi\n\n\n\n\n\nfinite-length real-valued vectors or sequences\n\n\n\n(2.4) \n\n\n\n\n\n\n\n∑ ⁡ n=−∞∞x[n] y∗[n]\n\n\n\n\n\ninfinite-length complex-valued vectors or sequences\n\n\n\n(2.6) \n\n\n\n\n\n\n\n∫ −∞∞x(t)y∗(t)dt\n\n\n\n\n\ncontinuous-time complex-valued signals\n\n\n\n(2.7) \n\n\n\n\n\n\n\n∫ ⟨T⟩x(t)y∗(t)dt\n\n\n\n\n\ncontinuous-time complex-valued signals with duration T\n\n\n\n(2.8) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs indicated in Table 2.2, it is also possible to define an inner product for infinite duration signals. For example, consider the complex-valued signal y[n] = ejΩn, where n = −∞,…,−1,0,1,…,∞. It is a common operation in Fourier transform to calculate inner products among such signals using the definition\n\n\n\n\n\n\n\n\n\n ⟨x[n],y[n]⟩≜∑ n=−∞∞x[n]y∗[n]. \n\n\n(2.6)\n\n\n\n\n\n\nNote that when applied to complex-valued signals, the inner product is defined using a complex conjugation.\n\n\n\nWhen dealing with continuous-time signals, a convenient definition is obtained by changing the summation by an integral:\n\n\n\n\n\n\n\n\n\n ⟨x(t),y(t)⟩ = ∫ −∞∞x(t)y∗(t)dt. \n\n\n(2.7)\n\n\n\n\n\n\nThe inner product of finite-duration signals with support T is simplified to\n\n\n\n\n\n\n\n\n\n ⟨x(t),y(t)⟩ = ∫ ⟨T⟩x(t)y∗(t)dt, \n\n\n(2.8)\n\n\n\n\n\n\nwhere ⟨T⟩ denotes a range of T such as [0,T] or [−T∕2,T∕2].\n\n\n\nWhen using inner products between continuous-time signals, it is possible to make useful analogies to vectors in the Euclidean space and benefit from geometrical interpretations. For example, similar to vectors, the squared norm ⟨x(t),x(t)⟩ = ∥x(t)∥2 is the signal energy.3 But the most important lesson in this context is that orthogonal signals share similar properties with orthogonal vectors.4\n\n\n\n Example 2.4. Example of a pair of orthogonal signals. The signals x(t) = 1 and y(t) = 2u(t) − 1 can be interpreted as orthogonal because ⟨x(t),y(t)⟩ = 0. This inner product can be obtained by noting that x(t)y(t) is  − 1 for t &lt; 0 and 1 for t ≥ 0, leading to a zero integral when using Eq. (2.7).   □\n\n\n\nAt this point, the reader may eventually benefit from Appendices B.14.2 and B.14.3, which provide a review of linear algebra applied to transforms. The goal of these appendices is to interpret, for example, the Fourier transform\n\n\n\n\n\n\n X(ω) = ∫ −∞∞x(t)e−jωtdt = ⟨x(t),ejωt⟩ \n\n\n\n\n\n\nas the inner product ⟨x(t),ejωt⟩ between the signal x(t) and the basis function ejωt.\n\n\n\nAs discussed in Appendix B.14.3, if a signal x(t) can be represented as a linear combination x(t) = ∑ ⁡ d=1Dmdφd(t), where the D functions φd(t) compose a set {φj(t)} of orthonormal basis functions, then the values (or coefficients) md can be recovered using inner products:\n\n\n\n\n\n\n\n\n\n md = ⟨x(t),φd(t)⟩. \n\n\n(2.9)\n\n\n\n\n\n\nThis result is discussed in Appendix B.14.3 via examples with vectors.\n\n\n\nBefore dealing with infinite duration signals, the next section discusses transforms that operate on blocks of samples.\n\n\n \n\n3 Unless otherwise stated, ∥⋅∥2 denotes the Euclidean or L2 norm.\n\n \n\n4 As mentioned, orthogonal is a generalization of perpendicular.\n\n                                         &lt;/div&gt;"
  },
  {
    "objectID": "ak_dsp_bookse21.html",
    "href": "ak_dsp_bookse21.html",
    "title": "21  Block Transforms",
    "section": "",
    "text": "Generic block processing was presented in Section 1.4. This section discusses block transforms as a specific block-oriented signal processing, including DCT, DFT, and Haar transforms, which together with the KLT (or PCA) already presented, are among the most used transforms.\n\n\n\nIn many signal processing tasks, it is useful to have two related transformations called a transform pair. One transform is the inverse of the other, with the inverse undoing the forward transform. When dealing with block transforms,5 one simply uses linear algebra (as detailed in Appendix B.14), and both transforms are simple matrix multiplications. The pair of transforms is defined by a pair of matrices. The matrices can be rectangular, as in lapped transforms,6 but most block transforms are defined by a pair of N × N square matrices A and A−1 and the jargon N-point transform indicates their dimension. The inverse matrix A−1 is assumed to exist and can “undo” the transformation A (and vice-versa). Here, the forward (or direct) transformation is denoted as\n\n\n\n\n\n\n X = Ax, \n\n\n\n\n\n\nwhile the inverse transformation is denoted here as\n\n\n\n\n\n\n\n\n\n x = A−1X. \n\n\n(2.10)\n\n\n\n\n\n\nThe vector X is called the transform of x and the elements of X are called coefficients. The columns of A−1 are called the basis functions (or basis vectors). In this text, vectors are represented by bold lower case letters, but the vector with coefficients will be denoted by capital letters to be consistent with the jargon (unfortunately, vectors of coefficients such as X can be confused with matrices, but the context will distinguish them).\n\n\n\nA matrix B with orthonormal columns is called unitary. The rows of a unitary matrix are also orthonormal. Unitary matrices are widely used in transforms because their inverse is simply the conjugate transpose as indicated in:\n\n\n\n\n\n\n B−1 = BH =  (B∗)T , \n\n\n\n\n\n\nwhere H denotes the Hermitian (conjugate transposition).\n\n\n\n Example 2.5. Observing that the inverse of a unitary real matrix is its transpose. In order to get insight on why B−1 = BH for a unitary\n\nB, consider the elements of B are real numbers, such that BH = BT . The result of the product BT B = I is the identity matrix (that is B−1 = BT ), because the inner product between the rows of BT  (columns of B) with the columns of B is one when they coincide (main diagonal of I) and zero otherwise (due to their orthogonality). In other words, the inner products of columns of B with themselves is the identity, given they are orthonormal. In case A is complex-valued, one has A−1 = AH via a similar reasoning.   □\n\n\n\nConsidering Eq. (2.10), if the basis functions (columns of A−1) are orthonormal, then A−1 = AH. Consequently, the rows of the direct transform A are the complex-conjugate of the basis functions. Two important facts are:\n\n &lt;ul class='itemize1'&gt;\n &lt;li class='itemize'&gt;&lt;span class='ec-lmri-10x-x-109'&gt;Forward&lt;/span&gt;: the &lt;!-- l. 303 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;-th\n coefficient of the forward transform &lt;!-- l. 303 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;mstyle class='text'&gt;&lt;mtext class='textbf' mathvariant='bold'&gt;X&lt;/mtext&gt;&lt;/mstyle&gt; &lt;mo class='MathClass-rel' stretchy='false'&gt;=&lt;/mo&gt; &lt;mstyle class='text'&gt;&lt;mtext class='textbf' mathvariant='bold'&gt;A&lt;/mtext&gt;&lt;/mstyle&gt;&lt;mstyle class='text'&gt;&lt;mtext class='textbf' mathvariant='bold'&gt;x&lt;/mtext&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/math&gt;\n is obtained by performing the inner product between &lt;!-- l. 303 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mstyle class='text'&gt;&lt;mtext class='textbf' mathvariant='bold'&gt;x&lt;/mtext&gt;&lt;/mstyle&gt;&lt;/math&gt;\n and the complex conjugate of the &lt;!-- l. 303 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;-th\n basis  function.  The  larger  is  this  coefficient  magnitude,  the  better  the\n &lt;!-- l. 303 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;-th\n basis function represents signal &lt;!-- l. 303 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mstyle class='text'&gt;&lt;mtext class='textbf' mathvariant='bold'&gt;x&lt;/mtext&gt;&lt;/mstyle&gt;&lt;/math&gt;.\n &lt;/li&gt;\n &lt;li class='itemize'&gt;&lt;span class='ec-lmri-10x-x-109'&gt;Inverse&lt;/span&gt;: in the inverse transform &lt;!-- l. 304 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;mstyle class='text'&gt;&lt;mtext class='textbf' mathvariant='bold'&gt;x&lt;/mtext&gt;&lt;/mstyle&gt; &lt;mo class='MathClass-rel' stretchy='false'&gt;=&lt;/mo&gt;&lt;msup&gt;&lt;mrow&gt; &lt;mstyle class='text'&gt;&lt;mtext class='textbf' mathvariant='bold'&gt;A&lt;/mtext&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo class='MathClass-bin' stretchy='false'&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mstyle class='text'&gt;&lt;mtext class='textbf' mathvariant='bold'&gt;X&lt;/mtext&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/math&gt;,\n the column vector &lt;!-- l. 304 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mstyle class='text'&gt;&lt;mtext class='textbf' mathvariant='bold'&gt;x&lt;/mtext&gt;&lt;/mstyle&gt;&lt;/math&gt;\n is   obtained   by   the   linear   combination   of   the   basis   functions:   the\n &lt;!-- l. 304 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;-th\n element (coefficient) in &lt;!-- l. 304 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mstyle class='text'&gt;&lt;mtext class='textbf' mathvariant='bold'&gt;X&lt;/mtext&gt;&lt;/mstyle&gt;&lt;/math&gt;\n multiplies the &lt;!-- l. 304 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;/math&gt;-th\n column (basis function) of &lt;!-- l. 304 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;msup&gt;&lt;mrow&gt;&lt;mstyle class='text'&gt;&lt;mtext class='textbf' mathvariant='bold'&gt;A&lt;/mtext&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo class='MathClass-bin' stretchy='false'&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;/math&gt;\n in a linear combination that generates &lt;!-- l. 304 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mstyle class='text'&gt;&lt;mtext class='textbf' mathvariant='bold'&gt;x&lt;/mtext&gt;&lt;/mstyle&gt;&lt;/math&gt;.&lt;/li&gt;&lt;/ul&gt;\n\n\nThe next paragraphs present the DCT transform, which can be used for both frequency analysis and coding.\n\n\n\n\n\n2.4.1  DCT transform\n\n\n\nAn example of a unitary matrix transform very useful for coding is the discrete cosine transform (DCT). When N = 4, the corresponding matrices are\n\n\n\n\n\n\n A = 1 2  [   1  1  1  1   1.307   0.541   − 0.541   − 1.307   1  − 1  − 1  1   0.541   − 1.307   1.307   − 0.541   ] \n\n\n\n\n\n\nand\n\n\n\n\n\n\n A−1 = AH = AT  = 1 2  [   1  1.307  1  0.541   1   0.541   − 1   − 1.307   1  − 0.541  − 1  1.307   1   − 1.307   1   − 0.541   ] \n\n\n\n\n\n\nfor the direct and inverse transforms, respectively.\n\n\n\nConsidering that the first element is a0,0 (first index in equations is zero, not one as in Matlab/Octave), an element an,k of the N-point inverse DCT matrix A−1 = {an,k} can be obtained by\n\n\n\n\n\n\n an,k = wk cos ⁡   (π(2n + 1)k 2N ), \n\n\n\n\n\n\nwhere wk is a scaling factor that enforces the basis vectors to have unit norm, i. e., wk = 1 N for k = 0 and wk =    2 N for k = 1,2,…,N − 1.\n\n\n\n Example 2.6. DCT calculation in Matlab/Octave. Listing 2.1 illustrates how the DCT matrices can be obtained in Matlab/Octave.\n\n\n\nListing 2.1: MatlabOctaveFunctions/ak_dctmtx.m\n\n\nfunction [A, Ai] = ak_dctmtx(N) \n% function [A, Ai] = ak_dctmtx(N) \n%Calculate the DCT-II matrix of dimension N x N. \n%A and Ai are the direct and inverse transform matrices, respectively \n5Ai=zeros(N,N); %pre-allocate space \nscalingFactor = sqrt(2/N); %make base functions to have norm = 1 \nfor n=0:N-1 %a loop helps to clarify obtaining Ai (inverse) matrix \n    for k=0:N-1 %first array element is 1, so use A(n+1,k+1): \n       Ai(n+1,k+1)=scalingFactor*cos((pi*(2*n+1)*k)/(2*N)); \n10    end \nend \nAi(1:N,1)=Ai(1:N,1)/sqrt(2); %different scaling factor, k=0 \n%unitary transform, so the direct is the Hermitian: \nA = Ai'; %command ' is Hermitian. Matrix is real so, just transpose\n  \n\n\nThe matrices obtained with ak_dctmtx.m can be used to perform the transformations but this has only pedagogical value. There are algorithms for computing the DCT that are faster than a plain matrix multiplication. Check the functions dct and idct in Matlab/Octave.    □\n\n\n\n Example 2.7. The DCT basis functions are cosines of distinct frequencies. Figure 2.2 shows four basis functions of a 32-points DCT transform.\n\n\n\n\n\n\n\n\nFigure 2.2: The first three (k = 0,1,2) and the last (k = 31) basis functions for a 32-points DCT. Note that the frequency increases with k.\n\n\n\n\n\nFigure 2.2 indicates that, in order to represent signals composed by “low frequencies”, DCT coefficients of low order (small values of k can be used), while higher order coefficients are more useful for signals composed by “high frequencies”. For example, the commands:\n\n\nN=32;k=3;n=0:N-1;x=7*cos(k*(pi*(2*n+1)/(2*N)));stem(x); X=dct(x)\n\n\nreturn a vector X with all elements equal to zero but X(4)=28, which corresponds to k = 3 (recall the first index in Matlab/Octave is 1, not 0). Using a larger k will increase the frequency and the order of the corresponding DCT coefficient.    □\n\n\n\n Example 2.8. Example of a DCT transformation. For example, assuming a 4-points DCT and x = [1,2,3,4]T , the forward transform can be obtained in this case with\n\n\n\n X =   [   X(0)    X(1)    X(2)    X(3)   ] = Ax    ≈ 1 2  [   1  1  1  1   1.307   0.541   − 0.541   1.307   1  − 1  − 1  1   0.541   − 1.307   1.307   − 0.541   ]  [   1   2   3   4  ]    ≈  [   5   − 2.230   0   − 0.158  ].    \n\nIn this case,\n\n\n\n\n\n\n X(0)  =  ⟨x,A(:,0))⟩  =  ⟨[1,2,3,4]T ,0.5[1,1,1,1]T ⟩  = 5, \n\n\n\n\n\n\nwhere A(:,0) represents the first (0-th) column of matrix A. Similarly, X(2) is given by\n\n\n\n\n\n\n X(2)  =  ⟨x,A(:,2))⟩  =  ⟨[1,2,3,4]T ,0.5[1,−1,−1,1]T ⟩  = 0, \n\n\n\n\n\n\nand so on.\n\n\n\nThe previous expressions provide intuition on the direct transform. In the inverse transform, when reconstructing x, the coefficient X(k) is the scaling factor that multiplies the k-th basis function in the linear combination x = A−1X. Still considering the 4-points DCT, the inverse corresponds to\n\n\n\n   [   x(0)    x(1)    x(2)    x(3)   ]  = 1 2  [   1  1.307  1  0.541   1   0.541   − 1   − 1.307   1  − 0.541  − 1  1.307   1   − 1.307   1   − 0.541   ]  [   5   − 2.230   0   − 0.158  ]     = 1 2  {5  [   1   1   1   1  ] − 2.230  [   1.307   0.541   − 0.541   − 1.307   ]        +0  [   1   − 1   − 1   1  ] − 0.158  [   0.541   − 1.307   1.307   − 0.541  ]}.    \n\nNote that X(2) = 0 and, consequently, the basis function [1,−1,−1,1]T  is not used to reconstruct x. The reason is that this specific basis function is orthogonal to x and does not contribute to its construction.   □\n\n\n\nAlternatively, the matrix multiplication can be described by a transform equation. For example, the DCT coefficients can be calculated by\n\n\n\n\n\n\n X[k] =    2 N∑ n=0N−1x[n]cos ⁡   (π(2n + 1)k 2N ),k = 1,… ⁡ ,N − 1 \n\n\n\n\n\n\nand\n\n\n\n\n\n\n X[0] = 1 N∑ n=0N−1x[n]. \n\n\n\n\n\n\nAs mentioned, the k-th element (or coefficient) X(k) of X can be calculated as the inner product of x with the complex conjugate of the k-th basis function. This can be done because the DCT basis functions are orthogonal among themselves, as discussed in Section B.14.3. The factors  2∕N and 1∕N are used to have basis vectors with unitary norms. The k-th basis vector is a cosine with frequency (kπ)∕N and phase (kπ)∕(2N).\n\n\n\nSection 2.10 discusses examples of DCT applications, including coding (signal compression). One advantage of adopting block transforms in coding applications is the distinct importance of coefficients. In the original domain, all samples (or pixels in image processing) have the same importance, but in the transform domain, coefficients typically have distinct importance. Hence, the coding scheme can concentrate on representing the most important coefficients and even discard the non-important ones. Another application of DCTs is in frequency analysis (finding the most relevant frequencies that compose a signal). But, in this application, the DFT is more widely adopted than the DCT.\n\n\n2.4.2  DFT transform\n\n\n\nAs the DCT, the discrete Fourier transform (DFT) is a very useful tool to accomplish frequency analysis, where the goal is to estimate the coefficients for basis functions that are distinguished by the their frequencies. The DFT is related to the discrete-time Fourier series, which also uses cosines cos ⁡ (2πnk N ) and sines sin ⁡ (2πnk N ), k = 0,1,…,N − 1, as basis functions, and will be discussed in this chapter. While the DCT uses cosines and its matrices are real, the DFT uses complex exponentials as basis functions.\n\n\n\nUsing Euler’s formula, Eq. (B.1), complex numbers provide a more concise representation of sines and cosines and the k-th DFT basis function is given by\n\n\n\n\n\n\n\n\n\n  1 Nej2πnk N  = 1 N  (cos ⁡ (2πnk N ) + jsin ⁡ (2πnk N )), \n\n\n(2.11)\n\n\n\n\n\n\nwhere n = 0,1,…,N − 1 expresses time evolution, as for the DCT. The value k determines the frequency of the basis function.\n\n\n\nAn element an,k of the N-point inverse DFT matrix A−1 = {an,k} is then\n\n\n\n\n\n\n an,k = 1 Nej2πnk N . \n\n\n\n\n\n\nThe DFT scaling factor 1∕N must be changed to 1∕N if one wants to have basis functions with unitary norm. In this case the transform is called unitary DFT.\n\n\n\nFor convenience, the twiddle factor WN is defined as\n\n\n\n\n\n\n\n\n\n WN = e−j2π N  \n\n\n(2.12)\n\n\n\n\n\n\nsuch that the k-th basis is (1∕N)  (WN) −nk for the conventional DFT and (1∕N)  (WN) −nk for the unitary DFT. Twiddle means to lightly turn over or around and is used because the complex number WN has unitary magnitude and changes only the angle of a complex number that is multiplied by it. Each element of the inverse DFT matrix A−1 = {an,k} is\n\n\n\n\n\n\n an,k = 1 N  (WN) −nk. \n\n\n\n\n\n\nFigure 2.3 illustrates the complex numbers WN as vectors for different values of N. Because |WN| = 1, the twiddle factor is located on the unit circle of the complex plane and effectively informs an angle. For example, the three angles used by a DFT of N = 3 points are 0, 120 and 240 degrees, while a 4-points DFT uses 0, 90, 180 and 270.\n\n\n\n                                   &lt;div class='center'&gt;\n\n\n\n\n                               &lt;div class='subfigure'&gt;&lt;table&gt;&lt;tr&gt;&lt;td style='text-align:left'&gt;&lt;img alt='PIC' src='FiguresNonScript/circledivided3.png' /&gt;\n\n(a) N = 3"
  },
  {
    "objectID": "ak_dsp_bookse22.html",
    "href": "ak_dsp_bookse22.html",
    "title": "22  Fourier Transforms and Series",
    "section": "",
    "text": "2.5  Fourier Transforms and Series\n\n\n\nThe transforms in this section adopt eternal sinusoids as basis functions and complement the DFT, which uses finite-length basis functions. For mathematical convenience, complex exponentials are also used given that they conveniently represent sinusoids.\n\n\n\nUsing sinusoids as basis functions is very useful in many applications. Depending on the type of signal to be analyzed, there are four pairs of analysis and synthesis transform equations that adopt eternal sinusoids as basis functions. These four pairs can be collectively called Fourier analysis tools and are discussed in the sequel.\n\n\n\nThe reason for not having only one transform pair when dealing with Fourier analysis is that two properties of the signal to be analyzed must be taken in account: whether the signal is continuous or discrete in time, and periodic or non-periodic. Covering all possible combinations, Table 2.3 lists the four pairs of equations to conduct Fourier analysis with eternal sinusoids (later we will discuss the DFT, which is used for finite-duration signals).\n\n\n\n\n\n\n\nTable 2.3: The four pair os equations for Fourier analysis with eternal sinusoids and the description of their spectra: ck, X(f) (or X(ω)), X[k] and X(ejΩ). For periodic continuous and discrete-time signals the periods are T0 and N0, respectively, with fundamental (angular) frequencies ω0 = 2π∕T0 rad/s and Ω0 = 2π∕N0 rad. For continuous-time signals, one can alternatively use the linear frequency f instead of ω = 2πf, such that f0 = 1∕T0 is the fundamental frequency in Hz.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContinuous-time\n\n\n\n\n\nDiscrete-time\n\n\n\n\n\n\n\n\n\nFourier series\n\n\n\n\n\nDiscrete-time Fourier series (DTFS)\n\n\n\n\n\n\n\nPeriodic\n\n\n\n\n\n\nck = 1 T0  ∫ ⟨T0⟩x(t)e−j2πkf0tdt\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx(t) = ∑ ⁡ k=−∞∞ckej2πkf0t\n\n\n\n\n\n\nX[k] = 1 N0  ∑ ⁡ n=⟨N0⟩x[n]e−jkΩ0n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nor ck = 1 T0  ∫ ⟨T0⟩x(t)e−jkω0tdt\n\n\n\n\n\n\nx[n] = ∑ ⁡ k=⟨N0⟩X[k]ejkΩ0n\n\n\n\n\n\n\n\n\n\n\nx(t) = ∑ ⁡ k=−∞∞ckejkω0t\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFourier transform\n\n\n\n\n\nDiscrete-time Fourier transform (DTFT)\n\n\n\n\n\n\n\nNon-periodic\n\n\n\n\n\n\nX(f) = ∫ −∞∞x(t)e−j2πftdt\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx(t) = ∫ −∞∞X(f)ej2πftdf\n\n\n\n\n\n\nX(ejΩ) = ∑ ⁡ n=−∞∞x[n]e−jΩn\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nor X(ω) = ∫ −∞∞x(t)e−jωtdt\n\n\n\n\n\n\nx[n] = 1 2π ∫ ⟨2π⟩X(ejΩ)ejΩndΩ\n\n\n\n\n\n\n\n\n\n\nx(t) = 1 2π ∫ −∞∞X(ω)ejωtdω\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIt can be seen from Table 2.3 that the terminology series is used when the signal to be analyzed is periodic. In this case the spectrum is discrete in frequency and represented by coefficients ck or X[k]. Obtaining a Fourier series is a special case of the general procedure of representing a function (in this case, a periodic signal) via a series expansion such as Taylor’s, Laurent’s, etc. In contrast, the spectrum of non-periodic signals is continuous in frequency and the tools to analyze non-periodic signals are called transforms. Note this nomenclature is not 100% consistent with block transforms in the sense that the DFT, which has a discrete spectrum, is called “transform”.\n\n\n\nAs highlighted in Table 2.4, in Fourier analysis, there is an interesting and maybe not evident duality between the time and frequency domains: periodicity in one domain leads to a discrete function in the other domain, while non-periodicity leads to a continuous function. For example, Table 2.4 shows that the spectrum of a discrete-time signal x[n] is always periodic.\n\n\n\n\n\n\n\nTable 2.4: Duality of periodicity and discreteness in Fourier analysis. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContinuous-time x(t)\n\n\n\n\n\nDiscrete-time x[n]\n\n\n\n\n\n\n\nPeriodic\n\n    &lt;/td&gt;&lt;td  style=\"text-align:left;\" id=\"TBL-81-2-2\"  \nclass=“td11”&gt; \n\nFourier series\n\n\n\n\n\nDiscrete-time Fourier series (DTFS)\n\n\n\n\n\n\n\n\n\nBasis: ejω0kt\n\n\n\n\n\nBasis: ej 2π N0 kn\n\n\n\n\n\n\n\n\n\n\nck non-periodic, k discrete\n\n\n\n\n\n\nX[k] periodic, k discrete\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon-periodic\n\n\n\n\n\nFourier transform\n\n\n\n\n\nDiscrete-time Fourier transform (DTFT)\n\n\n\n\n\n\n\n\n\nBasis: ejωt\n\n\n\n\n\nBasis: ejΩn\n\n\n\n\n\n\n\n\n\n\nX(ω) non-periodic, ω continuous\n\n\n\n\n\n\nX(ejΩ) periodic, Ω continuous\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2.4 and Table 2.3 indicate that the DTFS is the only pair that has discrete signals in both domains, time and frequency. This indicates that the DTFS is related to the DFT. In fact, the DFT and DTFS are mathematically equivalent in case N0 = N, with the exception of the normalization factor 1∕N in Eq. (2.11). But their interpretation has a remarkable distinction: The basis functions of the block transform DFT in Eq. (2.11) are finite-duration sequences or, equivalently, vectors, with dimension N, while the basis functions of the DTFS are infinite-duration complex exponentials ej 2π N0kn  of period N0. Hence, the DTFS and DFT are more naturally interpreted and used for periodic and finite-duration signals, respectively.\n\n\n\nAnother reason for keeping their names different is that the DFT (via FFT algorithms) is often used in computers, digital oscilloscopes, spectrum analyzers, etc., to analyze non-periodic and even continuous-time signals that were digitized. Hence, there are aspects that must be studied to apply DFT in these cases, such as the relation between an original spectrum X(ω) of a continuous-time signal and X[k], the one obtained by the DFT of its discrete-time version. Therefore, in spite of the operations in DTFS and DFT being mathematically equivalent apart from a normalization constant, it is convenient to restrict the discussion of DTFS to the analysis of periodic and discrete-time signals and call DFT the tool for finite-duration signals, which in practice is used for analyzing any digitized signal.\n\n\n\nBesides their relation to the DFT, there are many other similarities and relations within the four Fourier pairs in Table 2.4 themselves. For example, transforms are meant for non-periodic signals but, as discussed in Appendix B.26.2, impulses can be used to also represent periodic signals via a transform (instead of a series). The next sections will discuss some of these relations.\n\n\n\nOne point that will not be explored in this text is the important aspect of convergence. Similar to the fact that a vector outside the span of a given basis set cannot be perfectly represented by the given basis vectors, there are signals that cannot be represented by Fourier transforms or series. In other words, the transform/series may not converge to a perfect representation even when using an infinite number of basis functions. A related aspect is the well-known Gibbs phenomenon: when the signal x(t) has discontinuities (such as u(t) at t = 0), the Fourier representation has to use an infinite number of basis functions. Any truncation of this number (i. e., using a finite number of basis functions) leads to ripples in the reconstructed signal. Given the adopted emphasis in the engineering application of transforms, this text assumes the signals are well-behaved and the transforms and series properly converge.\n\n\n\nComplementing Table 2.3, Table 2.5 indicates the assumed units when the signals in time domain are given in Volts and is useful to observe the difference for continuous and discrete spectra.\n\n\n\n\n\n\n\nTable 2.5: Units for each pair of Fourier equations in Table 2.3.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nContinuous-time\n\n\n\n\n\nDiscrete-time\n\n\n\n\n\n\n\n\n\nx(t) in Volts\n\n\n\n\n\nx[n] in Volts\n\n\n\n\n\nPeriodic \n\n\n\n\nFourier series\n\n\n\n\n\nDiscrete-time Fourier series (DTFS)\n\n\n\n\n\n\n\n\n\n\nck (Volts)\n\n\n\n\n\n\nX[k] (Volts)\n\n\n\n\n\n\n\n\n\nFourier transform\n\n\n\n\n\nDiscrete-time Fourier transform (DTFT)\n\n\n\n\n\nNon-periodic \n\n\n\n\n\nX(f) (Volts/Hz)\n\n\n\n\n\n\nX(ejΩ) (Volts/(normalized frequency))\n\n\n\n\n\n\n\n\n\n\n\n2.5.1  Fourier series for continuous-time signals\n\n\n\nThe Fourier series for a continuous-time signal uses an infinite number of complex harmonic sinusoids ejkω0t,k ∈ ℤ, as basis functions. These functions allow to represent any periodic signal x(t) with period T0 (i. e., x(t) = x(t + T0),∀ ⁡t), where ω0 = 2π T0 = 2πf0 (rad/s). The frequency f0 in Hz (or ω0 in rad/s) is called the fundamental frequency.\n\n\n\nThe following result is useful for proving Fourier pairs.\n\n\n\n Example 2.12. Eternal (infinite-duration) sinusoids at different frequencies are orthogonal. If ω1≠ω2, then\n\n\n\n\n\n\n ∫ −∞∞A 1 sin ⁡ (ω1t + ϕ1)A2 sin ⁡ (ω2t + ϕ2)dt = 0. \n\n\n\n\n\n\nProof: To simplify notation, let ω1t + ϕ1 = α and ω2t + ϕ2 = β. From Eq. (B.11):\n\n\n\n\n\n\n A1 sin ⁡ (α)A2 sin ⁡ (β) = 1 2[cos ⁡ (α − β) − cos ⁡ (α + β)]. \n\n\n\n\n\n\nThe integral from  −∞ to ∞ of any sinusoid is zero, therefore:\n\n\n\n\n\n\n ∫ −∞∞[A 1 sin ⁡ (α)A2 sin ⁡ (β)]dt = 1 2[∫ −∞∞cos ⁡ (α − β)dt −∫ −∞∞cos ⁡ (α + β)dt] = 0, \n\n\n\n\n\n\nunless α = β (in this case cos ⁡ (α − β) = 1).   □\n\n\n\n Example 2.13. Cosine and sine at the same frequency are orthogonal. Similar to Example 2.12, it can be shown that:\n\n\n\n\n\n\n ∫ −∞∞A 1 cos ⁡ (ω0t)A2 sin ⁡ (ω0t)dt = A1A2∕2∫ −∞∞sin ⁡ (2ω 0t)dt = 0, \n\n\n\n\n\n\nwhich used Eq. (B.5).   □\n\n\n\nThe following result is useful when the integration interval is the fundamental period T0 or a multiple of T0.\n\n\n\n Example 2.14. Harmonic sinusoids are orthogonal when the inner product (integral) is over a time interval multiple of the fundamental period. If ω0 = 2π∕T0 is the fundamental frequency in rad/s and k≠m, then\n\n\n\n\n\n\n ∫ &lt;T0&gt;A1 sin ⁡ (kω0t + ϕ1)A2 sin ⁡ (mω0t + ϕ2)dt = 0. \n\n\n\n\n\n\nProof: Note that the sinusoids are assumed here to be eternal, but the result is also valid in case they have a finite-duration coinciding with the time duration of the integral. From Eq. (B.11):\n\n\n\n ∫ &lt;T0&gt;A1 sin ⁡ (kω0t + ϕ1)A2 sin ⁡ (mω0t + ϕ2)dt =  1 2  [∫ &lt;T0&gt; cos ⁡ ((k − m)ω0t + ϕ1 − ϕ2)dt −∫ &lt;T0&gt; cos ⁡ ((k + m)ω0t + ϕ1 + ϕ2)dt] = 0.  \n\nThe cosine with angular frequency (k + m)ω0 of the second parcel has a period T = T0∕(k + m) and its integral over T0 is zero, because T0 is an interval corresponding to an integer number of periods T.\nThe same reasoning can be applied to the cosine with angular frequency (k − m)ω0 with period T0∕|k − m|, which is especially easy to observe when k &gt; m. If k &lt; m, because cos ⁡ (x) = cos ⁡ (−x), the cosine argument of the first parcel can be changed to (m − k)ω0t + ϕ2 − ϕ1 to help concluding that this integral over T0 is also zero.   □\n\n\n\nExample 2.14 proves that the Fourier series basis functions are orthogonal. They are not orthonormal and the energy over a duration T0 is\n\n\n\n\n\n\n E = ∫ ⟨T0⟩ |ejkω0t| 2dt = ∫ 0T0 dt = T0, \n\n\n\n\n\n\nwhich is the normalization factor that will appear in the Fourier series equations, similar to the result for block transforms in Eq. (2.20).\n\n\n\nThe basis b(t) = 1,∀ ⁡t, corresponding to k = 0 in a Fourier series, is responsible for representing the DC level of x(t). All other basis functions have a frequency\n\n\n\n\n\n\n fk = kf0. \n\n\n\n\n\n\nWhen k &gt; 1, frequencies higher than f0 (considering absolute values) are generated and, consequently, a period Tk = T0∕k that is smaller than T0. Therefore, all basis functions but the one for k = 0 are periodic in T0. The frequencies fk that obey a relation fk = kf0 with respect to a fundamental frequency f0 are called harmonics. The frequency f2 = 2f0 is called the second harmonic, 3f0 is the third harmonic and so on.\n\n\n\nIt seems intuitive that Fourier series should not be used to represent non-periodic signals. In fact, the basis functions even depend on the period T0 of the signal to be analyzed. On the other hand, because the signal is periodic, it suffices to find coefficients that represent the signal x(t) during a single period. The trick is then to use these coefficients (obtained with inner products of duration T0) to multiply eternal complex exponentials and properly represent the periodic (and consequently infinite-duration) x(t). Hence, the Fourier series pair is:\n\n\n\n\n\n\n\n\n\n  {   ck  =  1 T0 ∫ ⟨T0⟩x(t)e−j2πkf0tdt  ,k = −∞,…,−1,0,1,…,∞          x(t)  =  ∑ k=−∞∞ckej2πkf0t  ,∀ ⁡t.   \n\n\n(2.21)\n\n\n\n\n\n\nOne can use the reasoning in Theorem 4 (page §) to prove the Fourier series equations.\nWriting the synthesis equation\n\n\n\n\n\n\n x(t) = ∑ k=−∞∞c kej2πkf0t \n\n\n\n\n\n\nis similar to x = ∑ ⁡ i=1Nαibi. Calculating ⟨x,bi⟩ is therefore equivalent to\n\n\n\n\n\n\n ⟨x(t),ej2πmf0t⟩ = ∫ ⟨T0⟩x(t)e−j2πmf0tdt = ∫ ⟨T0⟩  [∑ k=−∞∞c kej2πkf0t] e−j2πmf0tdt. \n\n\n\n\n\n\n(recall from Table 2.2 that ⟨x(t),y(t)⟩ = ∫ x(t)y∗(t)dt when the signals are complex). Due to the properties of the inner product one can write\n\n\n\n\n\n\n\n\n\n ⟨x(t),ej2πmf0t⟩ = ∑ k=−∞∞  [∫ ⟨T0⟩ckej2π(k−m)f0tdt] = T 0cm. \n\n\n(2.22)\n\n\n\n\n\n\nThe last step is due to the orthogonality of the basis functions:\n\n\n\n\n\n\n ⟨ej2πkf0t,ej2πmf0t⟩ = ∫ ⟨T0⟩ej2πkf0te−j2πmf0tdt =  {   T0,  k = m    0,  k≠m.      \n\n\n\n\n\n\nIn summary, the previous steps used the basis functions orthogonality to prove that the analysis equation\n\n\n\n\n\n\n ck = 1 T0⟨x(t),ej2πkf0t⟩ = 1 T0 ∫ ⟨T0⟩x(t)e−j2πkf0tdt \n\n\n\n\n\n\ncorresponds to using the inner product as in Theorem 4 (page §).\n\n\n\nThe negative frequencies fk,k = −∞,…,−2,−1 exist for mathematical convenience. They allow, for example, to represent a cosine as a sum of complex exponentials with “positive” and “negative” frequencies as in Eq. (B.2).\n\n\n\n                                   &lt;div class=\"center\" \n\n\n\n\n\n\n\n\n\n\n\n\n(a) &lt;mi k = 0 (DC)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) &lt;mi k = 1 (fundamental)\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n(c) &lt;mi k = 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d) &lt;mi k = −2"
  },
  {
    "objectID": "ak_dsp_bookse23.html",
    "href": "ak_dsp_bookse23.html",
    "title": "23  Relating discrete and analog frequencies",
    "section": "",
    "text": "If x[n] is obtained by sampling x(t), it is convenient to relate their spectra. This is a followup of Section 1.7. Recall from Eq. (1.22) that ω = ΩFs. Hence, Fs can be used to relate the abscissas of graphs of X(ejΩ) and X(ω). But the periodicity of X(ejΩ) should be taken into account as follows.\n\n\n\nAfter a sampling frequency Fs is specified (e. g., Fs = 8 kHz), the value Fs∕2 represents the frequency f that will be mapped to the angle π rad and its multiples in 2π. This can be seen by\n\n\n\n\n\n\n\n\n\n Ω = ω∕Fs = 2πf∕Fs = 2π4000∕8000 = π. \n\n\n(2.31)\n\n\n\n\n\n\nIn discrete-time (or “digital”) signal processing, the value of Fs∕2 is called Nyquist frequency or folding frequency. If the spectrum X(f) is zero8 for f ∈ [Fs∕2,∞[, then the values of X(ω) in the range 2π × [−Fs∕2,Fs∕2] coincides with the values of X(ejΩ)\nnormalized by Fs in the corresponding range of [−π,π]. In other words:\n\n\n\n\n\n\n X(ejΩ) = F sX(ω), \n\n\n\n\n\n\nwhich will be proved later on. Note that X(ejΩ) = FsX(ω) only holds if the sampling theorem is obeyed, otherwise the components of signal x(t) with frequencies in the range ] −∞,Fs∕2[ and ]Fs∕2,∞[ will be mapped (or folded, which is the reason for calling Fs∕2 the folding frequency) to components of x[n] with angular frequencies in the range ] − π,π[, eventually distorting the discrete-time representation of the original x(t). This phenomenon is called aliasing because a folded component from X(ω) appears in X(ejΩ) as an “alias” or impostor.\n\n\n\n\n\n\n\n\nFigure 2.12: Spectrum X(f) (top) and X(ejΩ) when Fs = 60 Hz. The two indicated points are related by ω = ΩFs and the frequency f = 6.238 Hz is mapped into Ω = 0.6532 rad. Notice three of the infinite number of replicas of X(f) centered at Ω ∈ [−2π,0,2π]. In this case there was no aliasing because the sampling theorem was obeyed. \n\n\n\n\n\nFor the sake of illustration, Figure 2.12 depicts the spectrum X(f) of a continuous-time signal x(t). It also shows the spectrum X(ejΩ) of x[n], obtained by sampling x(t) with Fs = 60 Hz. In this case, ω = 2π × 6.238 rad/s is mapped to 0.6532 rad, with the magnitude being scaled by 60, according to X(ejΩ) = FsX(ω). This discussion aims at illustrating how the Fourier transform X(f) of x(t) and the DTFT of the corresponding x[n] (obtained from x(t) via a C/D conversion) are related. The next paragraphs complement Eq. (2.30) and discuss how to interpret the DFT in Hz.\n\n\n \n\n8 More strictly, only the magnitude matters: the condition is |X(f)|= 0,f ≥Fs∕2 because the phase can be discarded when the magnitude is zero.\n\n                                         &lt;/div&gt;"
  },
  {
    "objectID": "ak_dsp_bookse24.html",
    "href": "ak_dsp_bookse24.html",
    "title": "24  Summary of equations for DFT / FFT Usage",
    "section": "",
    "text": "The DFT resolution in radians is ΔΩ = 2π∕N. Using ω = ΩFs, one can note that the continuous-time angular frequency ω = 2πFs rad/s corresponds to the angle Ω = 2π rad. Hence, the DFT frequency spacing is\n\n\n\n\n\n\n\n\n\n Δf = Fs N . \n\n\n(2.32)\n\n\n\n\n\n\nFor example, assuming Fs = 100 Hz, a DFT of N = 256 points has a resolution of Δf = 0.3906 Hz. In radians, this resolution corresponds to ΔΩ = 2π∕256 ≈ 0.0245 rad.\n\n\n\nNote that Δf can also be written as Δf = Fs∕N = 1∕(NTs), where NTs is the total duration Ttotal of the signal being analyzed, such that\n\n\n\n\n\n\n\n\n\n Δf = 1 Ttotal. \n\n\n(2.33)\n\n\n\n\n\n\nHence, a spectral analysis with resolution of at least Δf Hz requires a data record of length\n\n\n\n\n\n\n\n\n\n Ttotal ≥ 1 Δf. \n\n\n(2.34)\n\n\n\n\n\n\nThe following example illustrates the interpretation in Hz of a DFT result. The DFT performs a sampling operation on the frequency domain Ω according Eq. (2.30). As illustrated in Figure 2.3, a DFT with N = 3 points (or 3-DFT) uses three basis functions ejΩ with the angles Ω = 0,2π∕3,4π∕3 rad, i. e., a counter-clockwise rotation using a step of Δw = 2π∕3. These angles correspond to k = 0,1,2, respectively, as indicated by Eq. (2.11). Note that in Figure 2.3, incrementing k corresponds to a clockwise rotation, given that Eq. (2.12) defines the twiddle factor with a negative exponent.\n\n\n\nThe three angles of a 3-DFT correspond to 0,Fs∕3,−Fs∕3 Hz, respectively, as indicated by ω = ΩFs or directly observing that the DFT uses a grid of Δf = Fs∕N. For N = 4, the four angles correspond to the frequencies 0,Fs∕4,Fs∕2,−Fs∕4 Hz. The angle corresponding to k = 0 is always 0 rad, which is called DC because corresponds to 0 Hz. Note that for\n\nk &gt; N∕2 the DFT values correspond to negative frequencies. The angular frequency corresponding to the k-th angle is\n\n\n\n\n\n\n\n\n\n Ωk = kΔΩ = k2π N , \n\n\n(2.35)\n\n\n\n\n\n\nwhich corresponds to the regular (linear) frequency\n\n\n\n\n\n\n\n\n\n fk = kΔf = kFs N . \n\n\n(2.36)\n\n\n\n\n\n\nThe values representing the largest frequency when N is even is\n\n\n\n\n\n\n\n\n\n k = N∕2, \n\n\n(2.37)\n\n\n\n\n\n\nwhich corresponds to π rad and Fs∕2 Hz. When N is odd, the value at\n\n\n\n\n\n\n\n\n\n k = (N − 1)∕2 \n\n\n(2.38)\n\n\n\n\n\n\nis the one representing the largest frequency Fs(N − 1)∕(2N) Hz. Table 2.6 summarizes some equations typically used with FFT algorithms.\n\n\n\n\n\n\n\nTable 2.6: Summary of equations useful for signal processing with FFT.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExpression \n\n\nReference \n\n\nComment \n\n\n\n\n\n\n\n\n\n\n\n\n\nΔf = Fs∕N\n\n\nEq. (2.32) \n\n\nFrequency spacing (Hz) \n\n\n\n\n\n\n\n\n\n\n\n\n\nΔf = 1∕Ttotal\n\n\nEq. (2.33) \n\n\nDependence on signal duration (Hz) \n\n\n\n\n\n\n\n\n\n\n\n\n\nΔΩ = (2π)∕N\n\n\nEq. (2.53) \n\n\nFrequency spacing (rad) \n\n\n\n\n\n\n\n\n\n\n\n\n\nfk = k Δf\n\n\nEq. (2.36) \n\n\nFrequency of k-th tone (Hz) \n\n\n\n\n\n\n\n\n\n\n\n\n\nΩk = k ΔΩ\n\n\nEq. (2.35) \n\n\nFrequency of k-th tone (rad) \n\n\n\n\n\n\n\n\n\n\n\n\n\nFs∕2\n\n\nEq. (2.37) \n\n\nHighest frequency (Hz) for even N\n\n\n\n\n\n\n\n\n\n\n\n\n\nFs(N −1)∕(2N)\n\n\nEq. (2.38) \n\n\nHighest frequency (Hz) for odd N\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                                                       &lt;/div&gt;\n                                                                          \n                                                                          \n\n\n\nAs discussed, the following three transforms differ only on the normalization factor: DFT, unitary DFT and DTFS. Generalizing Eq. (2.15) and Eq. (2.16), the “core” DFT transform equations can be written as follows:\n\n\n\n\n\n\n\n\n\n X[k] = α∑ n=0N−1x[n]  (W N) nk \n\n\n(2.39)\n\n\n\n\n\n\nand\n\n\n\n\n\n\n\n\n\n x[n] = β∑ k=0N−1X[k]  (W N) −nk. \n\n\n(2.40)\n\n\n\n\n\n\nAs indicated in Eq. (2.20), the only requirement to have a valid pair is that\n\n\n\n\n\n\n\n\n\n αβ = 1 N. \n\n\n(2.41)\n\n\n\n\n\n\nIn summary, the three popular options for choosing α and β:\n\n &lt;ul class='itemize1'&gt;\n &lt;li class='itemize'&gt;\n &lt;!-- l. 1455 --&gt;&lt;p class='noindent'&gt;&lt;span class='ec-lmbx-10x-x-109'&gt;DFT as samples of the DTFT (in frequency domain)&lt;/span&gt;: &lt;/p&gt;&lt;table class='equation'&gt;&lt;tr&gt;&lt;td&gt;\n &lt;!-- l. 1456 --&gt;&lt;p class='noindent'&gt;\n &lt;/p&gt;&lt;!-- l. 1456 --&gt;&lt;math class='equation' display='block' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;\n                          &lt;mstyle class='label' id='x32-101012r42'&gt;&lt;/mstyle&gt;&lt;!-- endlabel --&gt;&lt;mi&gt;α&lt;/mi&gt; &lt;mo class='MathClass-rel' stretchy='false'&gt;=&lt;/mo&gt; &lt;mn&gt;1&lt;/mn&gt;&lt;mstyle class='text'&gt;&lt;mtext class='textrm' mathvariant='normal'&gt;  and  &lt;/mtext&gt;&lt;/mstyle&gt;&lt;mi&gt;β&lt;/mi&gt; &lt;mo class='MathClass-rel' stretchy='false'&gt;=&lt;/mo&gt;  &lt;mfrac&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt; \nN. \n\n\n(2.42)\n\n\n\n\n &lt;!-- l. 1460 --&gt;&lt;p class='noindent'&gt;Matlab/Octave implements this transform in its &lt;span class='ec-lmss-10x-x-109'&gt;fft &lt;/span&gt;and &lt;span class='ec-lmss-10x-x-109'&gt;ifft&lt;/span&gt;\n routines. It allows to estimate the DTFT at specific values of\n &lt;!-- l. 1460 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mi mathvariant='normal'&gt;Ω&lt;/mi&gt;&lt;/math&gt;\n using Eq. (&lt;a href='ak_dsp_bookse22.html#x30-99001r30'&gt;2.30&lt;!-- tex4ht:ref: eq:dtft_via_dft  --&gt;&lt;/a&gt;). Also, it is the fastest in the direct transform because there is no\n scaling factor to be accounted for.\n &lt;/p&gt;&lt;/li&gt;\n &lt;li class='itemize'&gt;\n &lt;!-- l. 1461 --&gt;&lt;p class='noindent'&gt;&lt;span class='ec-lmbx-10x-x-109'&gt;Unitary DFT&lt;/span&gt;: &lt;/p&gt;&lt;table class='equation'&gt;&lt;tr&gt;&lt;td&gt;\n &lt;!-- l. 1462 --&gt;&lt;p class='noindent'&gt;\n &lt;/p&gt;&lt;!-- l. 1462 --&gt;&lt;math class='equation' display='block' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;\n                             &lt;mstyle class='label' id='x32-101013r43'&gt;&lt;/mstyle&gt;&lt;!-- endlabel --&gt;&lt;mi&gt;α&lt;/mi&gt; &lt;mo class='MathClass-rel' stretchy='false'&gt;=&lt;/mo&gt; &lt;mi&gt;β&lt;/mi&gt; &lt;mo class='MathClass-rel' stretchy='false'&gt;=&lt;/mo&gt;   &lt;mfrac&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt; \nN. \n\n\n(2.43)\n\n\n\n\n &lt;!-- l. 1466 --&gt;&lt;p class='noindent'&gt;Because this transform is unitary, Theorem &lt;a href='ak_dsp_bookse21.html#x29-89001r2'&gt;2&lt;!-- tex4ht:ref: th:parseval  --&gt;&lt;/a&gt; applies and the energy &lt;span class='ec-lmss-10x-x-109'&gt;sum(abs(X).^2) &lt;/span&gt;in\n frequency and time &lt;span class='ec-lmss-10x-x-109'&gt;sum(abs(x).^2) &lt;/span&gt;domains coincide. To implement the unitary DFT\n when using Matlab/Octave one can call &lt;span class='ec-lmss-10x-x-109'&gt;X=fft(x)/sqrt(N) &lt;/span&gt;and &lt;span class='ec-lmss-10x-x-109'&gt;x=ifft(X)*sqrt(N) &lt;/span&gt;for the\n direct and inverse transforms, respectively.\n                                                                          \n                                                                          \n &lt;/p&gt;&lt;/li&gt;\n &lt;li class='itemize'&gt;\n &lt;!-- l. 1467 --&gt;&lt;p class='noindent'&gt;&lt;span class='ec-lmbx-10x-x-109'&gt;DFT as DTFS in case the number of DFT points coincides with the signal\n period&lt;/span&gt;: &lt;/p&gt;&lt;table class='equation'&gt;&lt;tr&gt;&lt;td&gt;\n &lt;!-- l. 1468 --&gt;&lt;p class='noindent'&gt;\n &lt;/p&gt;&lt;!-- l. 1468 --&gt;&lt;math class='equation' display='block' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;\n                          &lt;mstyle class='label' id='x32-101014r44'&gt;&lt;/mstyle&gt;&lt;!-- endlabel --&gt;&lt;mi&gt;α&lt;/mi&gt; &lt;mo class='MathClass-rel' stretchy='false'&gt;=&lt;/mo&gt;  &lt;mfrac&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt; \nN  and  β = 1. \n\n\n(2.44)\n\n\n\n\n &lt;!-- l. 1472 --&gt;&lt;p class='noindent'&gt;In this case, the DFT coefficients can be interpreted in Volts when the time-domain\n signal is given in Volts. If the signal is periodic with fundamental period\n &lt;!-- l. 1472 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt; &lt;mo class='MathClass-rel' stretchy='false'&gt;=&lt;/mo&gt; &lt;mi&gt;N&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt; coinciding with\n the number &lt;!-- l. 1472 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mi&gt;N&lt;/mi&gt;&lt;/math&gt;\n of DFT points, choosing Eq. (&lt;a href='#x32-101014r44'&gt;2.44&lt;!-- tex4ht:ref: eq:dft_as_dtfs  --&gt;&lt;/a&gt;) allows to obtain the DTFS coefficients\n via the DFT. To implement this version when using Matlab/Octave, one\n can call &lt;span class='ec-lmss-10x-x-109'&gt;X=fft(x)/N &lt;/span&gt;and &lt;span class='ec-lmss-10x-x-109'&gt;x=ifft(X)*N &lt;/span&gt;for the direct and inverse transforms,\n respectively.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;\n\n\nThe next two sections present the Laplace and Z transforms.9 Both have two parameters that identifies each basis functions, in contrast to Fourier equations that have only the frequency. This extra degree of freedom, represented by sets of basis functions that are more powerful than the ones used in Fourier analysis, has the advantage of allowing the representation of a larger class of signals (and systems). A disadvantage is that the inverse transform is defined as a contour integration in the complex plane, which is an area of complex analysis that is out of the scope of this text. But it is rarely necessary to calculate the inverse transform using complex analysis. The Laplace and Z inverse transforms will be calculated here only by an alternative method called partial fraction expansion, which is discussed in the Appendix, Section B.10.\n\n\n\nThe Laplace and Z transforms are widely used to represent the action of linear and time-invariant systems, as will be detailed in Chapter 3. The Laplace transform has a number of properties that make it useful for analyzing linear systems. The most prominent advantage is that differentiation and integration become multiplication and division, respectively, by s. For example, this converts differential equations into polynomial equations, which are much easier to solve and, once solved, the inverse Laplace transform can provide the\ntime domain formula. Similarly, Z transforms are useful to analyze discrete-time systems.\n\n\n \n\n9 There are lots of materials on the Web. For example, check: [url2pol], [url2lap] and [url2vid].\n\n&lt;/div&gt;"
  },
  {
    "objectID": "ak_dsp_bookse25.html",
    "href": "ak_dsp_bookse25.html",
    "title": "25  Laplace Transform",
    "section": "",
    "text": "Some signals do not have a representation using Fourier analysis. For example, the signal x(t) = e3tu(t), does not have a Fourier transform because the integral\n\n\n\n X(ω)  = ∫ −∞∞x(t)e−jωtdt = ∫ 0∞e(3−jω)tdt =   1 3 − jωe(3−jω)t|  0∞     = 1 3 − jωlim ⁡  t→∞ e3t ejωt = ∞    \n\ndoes not converge. Note that one should evaluate\n\n\n\n\n\n\n lim ⁡  t→∞ e3t ejωt \n\n\n\n\n\n\nconsidering that |ejωt| = 1 (this limit is not ∞∕∞ and L’Hospital’s rule is not appropriate).\n\n\n\nThis limitation in the representational power of Fourier analysis can be circumvented with the trick of pre-multiplying the input signal by an exponential. Assuming the continuous-time domain, this corresponds to multiplying x(t) by an exponential e−σt, σ ∈ ℝ, and then taking the Fourier transform of x(t)e−σt. For example, if x(t) = e3tu(t), then pre-multiplying by e−4t leads to the following transform for z(t) = x(t)e−4t = e−tu(t):\n\n\n\n\n\n\n Z(σ,ω) = ∫ −∞∞z(t)e−jωtdt = ∫ 0∞e(−1−jω)tdt = 1 1 + jω. \n\n\n\n\n\n\nThe signal x(t) could be recovered from the inverse transform of Z(σ,ω) and a post-multiplication by e4t. This trick can be adopted with several different values of σ. For example, z(t) = x(t)e−5t would also work in the previous example, as well as all values of σ &lt; −3.\n\n\n\nBased on this motivation, the Laplace transform X(s), where s = σ + jω is a complex number, can be interpreted as the Fourier transform of the signal multiplied by a generic exponential, as follows:\n\n\n\n\n\n\n X(s) = F{x(t)e−σt} = ∫ −∞∞x(t)e−stdt. \n\n\n\n\n\n\nHence, from another perspective, the Laplace transform uses basis functions e(−σ+jω)t with two parameters σ and ω. Instead of choosing a specific value of σ, when dealing with the Laplace transform, one considers all possible values. The values for which the associated Fourier transform F{x(t)e−σt} converges compose the so-called region of convergence (ROC) of the Laplace transform. For the previous example x(t) = e3tu(t), the ROC is σ &lt; −3.\n\n\n\nIt is intuitive that the Fourier basis functions, which are limited in amplitude, are not the most adequate to represent a signal such as the x(t) = e3tu(t), which has magnitude increasing with t for t &gt; 0. Consider picking a very large value X(f ′) for the coefficient corresponding to the basis function ej2πf ′t, this value X(f ′) would scale the basis along all the time axis, eventually failing to provide the necessary increase in magnitude. The Laplace transform multiplies x(t) by an exponential weighting function eσt, where σ ∈ ℝ can control the amplitude of the exponential, which increases and decreases with t for σ &gt; 0 and σ &lt; 0, respectively. The step is equivalent to adding a new basis function, which is an exponential sinusoid ejωt multiplied by eσt, i. e., e(σ+jω)t.\n\n\n\nThe left graph in Figure 2.13 was obtained with Listing 2.3 and the second graph used sigma = -0.3, with both showing the real part of est, with s = ±0.3 + j10π. The envelope\n\ne±0.3t imposes the peak amplitude of the sinusoid ej10πt of 5 Hz.\n\n\n\nListing 2.3: MatlabOctaveCodeSnippets/snip_transforms_laplace_basis.m\n\n\nsigma = 0.3; w = 2*pi*5; %frequency of 5 Hz \ns=sigma+j*w; %define the complex variable s \nt=linspace(0,3,1000); %interval from [0, 3] sec. \nx=exp(s*t); %the signal \n5envelope = exp(sigma*t); %the signal envelope \nsubplot(121);plot(t,real(x));hold on,plot(t,envelope,':r')\n  \n\n\n\n\n\n\n\nFigure 2.13: Real part of e(σ+j10π)t. The values of σ are 0.3 and -0.3 for the first (left) and second graphs, respectively. The complex exponential ej10πt corresponds to a frequency of 5 Hz and the amplitude envelope is imposed by eσt.\n\n\n\n\n\nThe basis functions est have some peculiarities. If σ &gt; 0 their amplitudes reach ∞ when t →∞. If σ &lt; 0 their amplitudes reach ∞ when t →−∞. Therefore, the Laplace transform is more useful in the analysis of one-sided signals, such as the ones that incorporate u(t) (right-sided) or u(−t) (left-sided). In these cases, the basis function do not reach infinite because the ROC is chosen in a way that the signal itself is zero at the problematic values t = ∞ or  −∞.\n\n\n\nIn fact, the choice of est and the corresponding popularity of the Laplace transform is due to their use as a tool for analyzing systems (see Chapter 3) not signals. Make a comparison with the Fourier transform: if the signal coincides with a given basis function (e. g., x(t) = cos ⁡ (ω0t)), a signal impulse represents the signal in the transform domain (X(ω) = πδ(ω − ω0) for the given example). Such situations do not occur when the Laplace transform is used. The transform domain X(s) does not use impulses and there is no signal x(t) that is represented by a unique basis function.\n\n\n\nThe pair of Laplace equations is:\n\n\n\n\n\n\n  {   X(s)  =  ∫ −∞∞x(t)e−stdt    x(t)  =  1 2πj ∮ γ−j∞γ+j∞X(s)estds,   \n\n\n\n\n\n\nwhere γ is a real number so that the contour path of integration is in the region of convergence of X(s).\n\n\n\nBecause there are two parameters, the values that s can assume\ncompose a plane while in Fourier analysis it was a line (frequency in the abscissa). For example, assuming that x(t) = 1 5  [e−t(3cos ⁡ (2t) + sin ⁡ (2t)) − 3e−2t] u(t), the Laplace transform is\n\n\n\n\n\n\n\n\n\n X(s) = s − 1 (s + 2)(s + 1 − j2)(s + 1 + j2) \n\n\n(2.45)\n\n\n\n\n\n\nwith ROC σ &gt; −1. Figure 2.14 and Figure 2.15 show the magnitude (in dB) and phase of X(s), respectively.\n\n\n\n\n\n\n\n\nFigure 2.14: Magnitude (in dB) of Eq. (2.45).\n\n\n\n\n\n\n\n\n\n\nFigure 2.15: Phase (in rad) of Eq. (2.45).\n\n\n\n\n\nThe values for which X(s) = 0 are called zeros and the ones that lead to X(s) = ∞ are called poles. Figure 2.14 illustrates the zero at s = 1 and the tree poles. The locations of zeros appear as “valleys” while the poles are located at “volcanoes”. It is intuitive that the ROC cannot include poles because they are the positions for which X(s) = ∞. Because the convergence depends on σ, the ROCs are regions formed at the left or right of a given value σ0. When x(t) is right-sided, the ROC is the area at the right of σ0, while for left-sided x(t) the ROC is the area at the left of σ0. For the example in Figure 2.14, the ROC is σ &gt; −1 because x(t) is right-sided and the right-most poles (assuming the orientation of the σ-axis is from  −∞ to  + ∞) are at σ0 = −1.\n\n\n\nFigure 2.15 shows the phase of X(s). In most cases the phase is less instructive than the magnitude. Figure 2.15 emphasizes that the poles are conventionally signalized with “x” marks while zeros are represented by “o” marks.\n\n\n\nThe Laplace transform has redundancy and x(t) can be recovered from X(s)|σ=σ1 if σ1 is in the ROC. An alternative view is that x(t) can be recovered from the knowledge of X(s) and its associated ROC. Note that knowing X(s) does not suffice to to uniquely identify x(t): the ROC must be known too. Distinct signals x(t) can have the same X(s), differing only in the ROCs. The following example illustrates this point.\n\n\n\n Example 2.19. Distinct signals may have the same Laplace transform, and only the ROC can disambiguate. Given the Laplace transform X(s) = 1∕(s + 2), one should find the corresponding signal x(t).\nIn fact, there are two possible signals, depending on the ROC. The pole is at s = −2 and assuming the ROC is Real{s} &gt; −2, then x(t) = e−2tu(t). If the ROC is Real{s} &lt; −2, then x(t) = −e−2tu(−t).   □\n\n\n\nThe Fourier transform basis function is equivalent to the Laplace’s when σ = 0. Knowing that X(s) = F{x(t)e−σt} the Fourier transform of x(t) can be derived from its Laplace transform, i. e.,\n\n\n\n\n\n\n\n\n\n X(ω) = X(s)|σ=0 = X(s)|s=jω \n\n\n(2.46)\n\n\n\n\n\n\nin case σ = 0 is in the ROC of X(s) and, consequently, x(t) has a Fourier transform. Repeating to emphasize: Eq. (2.46) is valid only if the signal x(t) has a Fourier transform. Using the previous example, note that the axis jω is part of the ROC of e−2tu(t), which then has Fourier transform while  − e−2tu(−t) does not have.\n\n\n\nWhen the signal has both Fourier and Laplace transforms, a graph of X(s) incorporates the corresponding graph of X(ω). Figure 2.16 shows the magnitude values for s = jω (the jω axis) as a curve in black, superimposed to Figure 2.14. Alternatively, Figure 2.17 shows only the values of X(s) at the s = jω axis.\n\n\n\n\n\n\n\n\nFigure 2.16: Graph of Figure 2.14 with the identification of the corresponding values of the Fourier transform (magnitude).\n\n\n\n\n\n\n\n\n\n\nFigure 2.17: The values of the magnitude of the Fourier transform corresponding to Figure 2.16.\n\n\n\n\n\nBoth representations in Figure 2.16 and Figure 2.17 are three-dimensional and difficult to work with. Figure 2.18 depicts the frequency response in the conventional way, which is much easier to interpret than looking at Figure 2.17, for example. However, it is interesting to compare the figures and note that the peaks in Figure 2.18 are related to the position of the poles and draw conclusions such as that, the closer the pole is to the σ = 0 axis, the more evident is the corresponding peak at the Fourier transform. In filter design, it is sometimes necessary to locate poles in the vicinity of the ω axis because this conducts to filters with high quality factor.\n\n\n\n\n\n\n\n\nFigure 2.18: Two dimensional representation of Figure 2.17 obtained with the command freqs in Matlab/Octave showing the peak at ω = 2 rad/s due to the respective pole. Because the signal x(t) is real and X(ω) exhibits Hermitian symmetry, only the graphs for ω ≥ 0 are shown.\n\n\n\n\n\nThe need to use the Laplace as an alternative to the Fourier transform would be even greater if impulses were not allowed in Fourier analysis. The use of impulses in both the time and frequency domains allows a much larger class of signals to be represented with Fourier equations. For example, the ramp signal x(t) = tu(t), strictly does not have a Fourier transform because the following integral does not converge:\n\n\n\n\n\n\n X(ω) = ∫ −∞∞x(t)e−jωtdt = ∫ 0∞te−jωtdt =  1 + jωt ω2ejωt | 0∞ = lim ⁡   t→∞1 + jωt ω2ejωt . \n\n\n\n\n\n\nHowever, using impulses, it is possible to use Fourier representations of signals such as x(t) = tu(t), x(t) = u(t) and x(t) = cos ⁡ (ω0t). This fact influences the main applications of the Laplace transform to be the representation of systems, not signals. For example, in terms of signals, while x(t) = cos ⁡ (ω0t)u(t) has the transform X(s) = s∕(s2 + ω02), an infinite-duration sinusoid is not represented in the Laplace transform domain.\n\n\n\nBecause most signals that are analyzed with the Laplace transform are right-sided, sometimes the adopted definition is\n\n\n\n\n\n\n\n\n\n X(s) = ∫ 0∞x(t)e−stdt, \n\n\n(2.47)\n\n\n\n\n\n\nwhich is called the unilateral Laplace transform (in contrast to the bilateral definition of Eq. (2.8)). Both coincide if the signal is right-sided (e. g., by the action of u(t)).\n\n\n\nThe Laplace transform properties and pairs are similar to the Fourier ones as indicated by the following example.\n\n\n\n Example 2.20. A time-domain complex exponential leads to a rational function in s. Consider the following Laplace transform pair\n\n\n\n\n\n\n eatu(t) ⇔ 1 s − a,  converges for:  Real{s} &gt; a \n\n\n\n\n\n\nthat is proved as follows:\n\n\n\n X(s)  = ∫ −∞∞x(t)e−stdt = ∫ 0∞e(a−s)tdt =   1 a − se(a−s)t|  0∞     = 1 a − s  [(lim ⁡  t→∞e(a−s)t) − 1].    \n\nGiven that s = σ + jω, the limit can be obtained as\n\n\n\n\n\n\n lim ⁡  t→∞e(a−s)t = lim ⁡   t→∞e(a−σ)t ejωt = 0 \n\n\n\n\n\n\nwhen (a − σ) &lt; 0 (note |ejωt| = 1 and only the numerator defines the convergence).   □"
  },
  {
    "objectID": "ak_dsp_bookse26.html",
    "href": "ak_dsp_bookse26.html",
    "title": "26  Z Transform",
    "section": "",
    "text": "The Z transform is the counterpart of the Laplace transform for discrete-time signals. The pair of equations is given by\n\n\n\n\n\n\n  {   X(z)  =  ∑ n=−∞∞x[n]z−n    x[n]  =  1 2πj ∮ CX(z)zn−1dz,   \n\n\n\n\n\n\nwhere C is a counterclockwise closed path encircling the origin and entirely in the region of convergence (ROC). The contour or path, C , must encircle all of the poles of X(z).\n\n\n\nThe Laplace and Z transforms are related. When the Laplace transform is performed on a sampled signal xs(t) and a C/D is used, the result is the Z transform10 of a discrete-time sequence x[n] where\n\n\n\n\n\n\n\n\n\n z = esTs  \n\n\n(2.48)\n\n\n\n\n\n\nand Ts is the sampling period.\n\n\n\nEq. (2.48) is used in the matched Z-transform method for converting H(s) in s into H(z) in z, and is further discussed in Section 3.11.3.\n\n\n\n\n\n2.9.1  Some pairs and properties of the Z-transform\n\n\n\nA very useful pair and property are δ[n] ⇔ 1 and x[n − n0] ⇔ X(z)z−n0, respectively. Putting them together leads to δ[n − n0] ⇔ z−n0. Using linearity one can write, e. g.,\n\n\n\n\n\n\n 3δ[n + 4] + 2δ[n] − 2.5δ[n − 3] ⇔ 3z4 + 2 − 2.5z−3. \n\n\n\n\n\n\nIn some cases X(z) = ∑ ⁡ n=−∞∞x[n]z−n can be written as a geometric series and Eq. (B.20) used to obtain X(z). For example, the Z transform of x[n] = anu[n] is obtained as follows:\n\n\n\n\n\n\n X(z) = ∑ n=−∞∞x[n]z−n = ∑ n=−∞∞anu[n]z−n = ∑ n=0∞anz−n = ∑ n=0∞(a∕z)n. \n\n\n\n\n\n\nUsing Eq. (B.20) with a scale factor α = 1, ratio r = az−1 and |a∕z| &lt; 1 leads to\n\n\n\n\n\n\n\n\n\n anu[n] ⇔ 1 1 − az−1 = z z − a,  |z| &gt;  |a|. \n\n\n(2.49)\n\n\n\n\n\n\n Example 2.21. Converting x(t) to discrete-time and finding the corresponding Z-transform. As in Example 1.22, assume a continuous-time signal x(t) = 4e−2tu(t) should be transformed to a discrete-time x[n] with a sampling period Ts, and then have its Z-transform X(z) = Z{x[n]} calculated. From Eq. (1.20):\n\n\n\n\n\n\n\n\n\n X(z) = Z  {4e−2nTs u[n]} = 4Z  { (e−2Ts ) nu[n]} = 4 1 − e−2Tsz−1, \n\n\n(2.50)\n\n\n\n\n\n\nwhere the last step used Eq. (2.49).   □\n\n\n\n\n\n2.9.2  Z-transform region of convergence\n\n\n\nSimilar to the Laplace transform, the values of z for which the transform exists are called the region of convergence (ROC). The ROC of Z transforms are annular regions of the form |z| &gt; m (for right-sided sequences, also called causal), |z| &lt; m (for left-sided sequences) or m &lt; |z| &lt; p (for two-sided sequences), where m,p ∈ ℝ+. When the signal in time-domain has a finite support (duration), the ROC of the associated Z transform is the whole z-plane, eventually with the exceptions of z = 0 and z = ∞.\n\n\n\nTo recover x[n] from its transform X(z), it is essential to know the ROC. For example, the Z transform of x[n] = −anu[−n − 1] is:\n\n\n\n\n\n\n X(z) = ∑ n=−∞∞x[n]z−n = −∑ n=−∞−1anz−n = −∑ n=1∞a−nzn = −∑ n=1∞(z∕a)n. \n\n\n\n\n\n\nIn order to use Eq. (B.20) with factor α = 1 and ratio r = z∕a one can modify the summation interval\n\n\n\n\n\n\n X(z) = −∑ n=1∞(z∕a)n = 1 −∑ n=0∞(z∕a)n = 1 − 1 1 − z∕a = z z − a, \n\n\n\n\n\n\nwith the ROC |z| &lt; |a| (because Eq. (B.20) requires |z∕a| &lt; 1). In summary, both anu[n] and  − anu[−n − 1] have X(z) = z∕(z − a) and only the ROC can disambiguate them when calculating the inverse Z transform.\n\n\n\nThe inverse transform of rational functions can be obtained by following the steps:\n\n\n\nf 1.\n\n\nmake the rational function to have only non-negative11 powers of z,\n\n\nf 2.\n\n\nfind the poles and expand X(z) in partial fractions as discussed in Section B.10,\n\n\nf 3.\n\n\neventually multiply by z to create Y (z) = zX(z) and force the appearance of terms term z∕(z − a),\n\n\nf 4.\n\n\nconvert each parcel of Y (z) to the time domain,\n\n\nf 5.\n\n\nrearrange the terms, especially the ones corresponding to complex conjugate poles and\n\n\nf 6.\n\n\nbecause Y (z) = zX(z), then x[n] = y[n − 1]. Find the final result substituting n by n − 1 in y[n].\n\n\n\n\nFor example, to obtain the inverse transform of\n\n\n\n\n\n\n X(z) = z−2 + 0.9z−3 1 − 1.8z−1 + 1.41z−2 − 0.488z−3, \n\n\n\n\n\n\nwith ROC |z| &gt; 0.8, one can multiply numerator and denominator by z3 and obtain their roots:\n\n\n\n\n\n\n X(z) = z + 0.9 z3 − 1.8z2 + 1.41z − 0.488 = z + 0.9 (z − 0.8)(z − 0.5 ± j0.6). \n\n\n\n\n\n\nThe partial fraction expansion is\n\n\n\n\n\n\n X(z) = 3.78 z − 0.8 +  − 1.89 + j0.11 z − 0.5 + j0.6 +  − 1.89 − j0.11 z − 0.5 − j0.6 . \n\n\n\n\n\n\nMultiplying both sides by z leads to\n\n\n\n\n\n\n Y (z) = zX(z) = 3.78z z − 0.8 + (−1.89 + j0.11)z z − 0.5 + j0.6 + (−1.89 − j0.11)z z − 0.5 − j0.6 , \n\n\n\n\n\n\nwhich can be rewritten by converting the complex numbers from Cartesian to polar form\n\n\n\n\n\n\n Y (z) = 3.78z z − 0.8 + (1.89ej3.08)z z − 0.78ej2.26 + (1.89e−j3.08)z z − 0.78e−j2.26, \n\n\n\n\n\n\nBecause in this case the ROC is for a right-sided sequence, each term z∕(z − a) corresponds to anu[n] and the time domain signal is\n\n\n\n   y[n] = [3.78(0.8)n + 1.89ej3.08(0.78ej2.26)n + 1.89e−j3.08(0.78e−j2.26)n]u[n]    = [3.78(0.8)n + 1.89(0.78)n(ej(3.08+2.26n) + e−j(3.08+2.26n))]u[n]     = [3.78(0.8)n + 2 × 1.89(0.78)n cos ⁡ (2.26n + 3.08)]u[n]     \n\nwhich leads to\n\n\n\n\n\n\n x[n] = [3.78(0.8)n−1 + 2 × 1.89(0.78)n−1 cos ⁡ (2.26(n − 1) + 3.08)]u[n − 1]. \n\n\n\n\n\n\nNote that a pair of complex conjugate poles have complex conjugate residues. Let r = bejα and r∗ = be−jα be the residues for poles p = aej𝜃 and p∗ = ae−j𝜃, respectively, both with multiplicity one. With a ROC corresponding to right-sided signals, the two terms rz∕(z − p) and r∗z∕(z − p∗) in the partial fraction expansion can be rearranged in time domain to compose the general expression\n\n\n\n\n\n\n 2ban cos ⁡ (𝜃n + α)u[n]. \n\n\n\n\n\n\nIf the ROC corresponds to left-sided signals, the same terms correspond to\n\n\n\n\n\n\n −2ban cos ⁡ (𝜃n + α)u[−n − 1]. \n\n\n\n\n\n\nDifferent approaches to obtain x[n] can lead to distinct expressions, but these expressions must correspond to the same values of x[n],∀ ⁡n. For example, some people prefer to obtain the partial fraction expansion of X(z)∕z instead of using the suggested steps 3) and 6). Expanding X(z)∕z allows to multiply the obtained partial fractions by z to get z∕(z − a) factors. An example better illustrates the equivalence of both procedures and the reason for suggesting ours.\n\n\n\nAssume the task is to find the inverse x[n] of X(z) = (8z − 19)∕[(z − 2)(z − 3)] knowing that x[n] is right-sided. Using the alternative procedure of expanding X(z)∕z, one has\n\n\n\n\n\n\n X(z) z = (8z − 19)∕[z(z − 2)(z − 3)] = −19∕6 z + 3∕2 z − 2 + 5∕3 z − 3, \n\n\n\n\n\n\nwhich can be conveniently multiplied by z to obtain parcels in the form z∕(z − a):\n\n\n\n\n\n\n X(z) = −19 6 + 3∕2 z z − 2 + 5∕3 z z − 3, \n\n\n\n\n\n\nthat leads to the inverse\n\n\n\n\n\n\n\n\n\n x[n] = −19 6 δ[n] +  (3 22n + 5 33n) u[n]. \n\n\n(2.51)\n\n\n\n\n\n\nUsing the steps of the suggested procedure, which expands X(z) in partial fractions instead of X(z)∕z, the result is\n\n\n\n\n\n\n x[n] = [5(3n−1) + 3(2n−1)]u[n − 1], \n\n\n\n\n\n\nwhich seems different than Eq. (2.51). However, a closer inspection indicates that, for both expressions, the sample values x[0] = 0,x[1] = 8, etc., are the same, i. e., the procedures led to the same signal, as expected.\n\n\n\nThe Web has several tables of properties and pairs related to the Z transform.12 An interesting result is the initial value theorem, which is valid for signals for which x[n] = 0 for n &lt; 0 (right-sided) and states that\n\n\n\n\n\n\n x[0] = lim ⁡  z→∞X(z). \n\n\n\n\n\n\nUsing this theorem, one can anticipate that x[0] = 0 when X(z) has a denominator with degree larger than the numerator, such as in X(z) = (8z − 19)∕[(z − 2)(z − 3)].\n\n\n\n\n\n\n\n\nFigure 2.19: Magnitude (in dB) of Eq. (2.52).\n\n\n\n\n\n\n\n\n\n\nFigure 2.20: Phase (in rad) of Eq. (2.52).\n\n\n\n\n\nSimilar to Figure 2.14 and Figure 2.15, which are for Laplace, Figure 2.19 and Figure 2.20 depicts the magnitude and phase, respectively, for\n\n\n\n\n\n\n\n\n\n X(z) = z + 0.9 (z − 0.8)(z − 0.5 − j0.6)(z − 0.5 − j0.6), \n\n\n(2.52)\n\n\n\n\n\n\nwhich has one finite zero and three poles as indicated in Figure 2.21.\n\n\n\n\n\n\n\n\nFigure 2.21: Pole / zero diagram for Eq. (2.52).\n\n\n\n\n\n\n\n\n\n\nFigure 2.22: Graph of Figure 2.19 with the identification of the corresponding values of the DTFT (unit circle |z| = 1).\n\n\n\n\n\n\n\n\n\n\nFigure 2.23: The values of the magnitude of the DTFT corresponding to Figure 2.22.\n\n\n\n\n\nThe relation between the Z transform and the DTFT is similar to the one between Laplace and Fourier transform. Figure 2.22 and Figure 2.23 provide an example using Eq. (2.52).\n\n\n\n\n\n\n\n\nFigure 2.24: Magnitude (top) and phase (bottom) of the DTFT corresponding to Eq. (2.52). These plots can be obtained with the Matlab/Octave command freqz and are a more convenient representation than, e. g., Figure 2.23.\n\n\n\n\n\nSometimes it is not convenient to deal with 3-d plots such as Figure 2.23. An alternative is to represent the DTFT using a figure similar to Figure 2.24, which shows the magnitude and phase with the angle as independent variable. As discussed in Section 1.7.4, for convenience, the abscissa is normalized by π, such that “1” corresponds to π rad. Due to the symmetry of X(z) when x[n] is real, it is also common to represent the abscissa in the range [0,π] (instead of [0,2π[ as in Figure 2.24).\n\n\n \n\n10 Some textbooks (e. g.  [?]) that deal with coding theory use the D instead of the Z transform, where D = z−1. For example, H(z) = 1 −0.9z−1 corresponds to H(D) = 1 −0.9D and H(z) = 1 −0.5z corresponds to H(D) = 1 −0.5D−1.\n\n \n\n11 This is not mandatory, but allows to use the same procedure for partial fraction expansion as for the Laplace transform.\n\n \n\n12 Such as the ones at [url2ztr].\n\n                                                                                     &lt;/div&gt;"
  },
  {
    "objectID": "ak_dsp_bookse27.html",
    "href": "ak_dsp_bookse27.html",
    "title": "27  Applications",
    "section": "",
    "text": "This section will briefly discuss some applications of transforms.\n\n\n\n Application 2.1. Example of Gram-Schmidt transform. As an example of the Gram-Schmidt procedure, assume that Listing B.4 is invoked with the commands\n\n\nx=[0,-1,-1,0;  0,2,2,0;  0,1,0,1;  1,1,1,1;  -2,2,2,1] %row vectors \n[Ah,A]=ak_gram_schmidt(x) %perform the orthonormalization procedure \nX=Ah*transpose(x(1,:)) %coefficients corresponding to first vector \nx1=A*X; %reconstruction of first vector x(1,:) (as a column vector)\n\n\nwhere M = 5 vectors and D = 4 is the space dimension. In this case, the number of orthonormal basis functions is N = 4, such that both matrices Ah and A have dimension 4 × 4. The basis functions correspond to the columns of matrix A. The first element of x1, corresponding to the first basis function, is the only non-zero element and has value  2 which is the coefficient that must multiply the first basis function [0,−2∕2,2∕2,0] to reconstruct the first vector [0,−1,−1,0]. The reason is that ak_gram_schmidt chooses the first basis as a normalized (unitary-norm) version of the first vector in x.\n\n\n\nDebugging the execution of the code ak_gram_schmidt, step-by-step, allows to observe more details as listed in Listing 2.4.\n\n\n\nListing 2.4: MatlabOctaveCodeSnippets/snip_transforms_granschmidt_debug.m\n\n\ntol = 1.1102e-015  %calculated (default) tolerance \n%first basis in y is [0  -0.7071  -0.7071  0], numBasis=1 \nk = 2, m = 1 \nprojectionOverBasis = [0 2.0 2.0 0] %2nd input vector \n5errorVector = 1.0e-015 * [0    0.4441    0.4441  0] \nmagErrorVector = 6.2804e-016 %do NOT add error vector to basis set \n%2nd vector is already represented, go to next iteration \nk = 3, m = 1 \nprojectionOverBasis = [0 0.5 0.5 0] %3rd input vector \n10%errorVector below is orthogonal to [0 -0.7071 -0.7071 0] \nerrorVector = [0 0.5 -0.5 1] \nmagErrorVector = 1.2247 %add normalized error vector to basis set \n%second basis is [0 0.4082 -0.4082 0.8165], numBasis=2 \nk = 4, m = 1 \n15projectionOverBasis = [0 1.0 1.0  0] %4th input vector \nerrorVector = [1.0 0.0 0.0 1] %using 1st basis \nk = 4, m = 2 \nprojectionOverBasis = [0.0  0.3333   -0.3333    0.6667] \nerrorVector = [1.0 -0.3333 0.3333 0.3333] %using 2 basis vectors \n20magErrorVector = 1.1547 %add normalized error vector to basis set \n%3rd basis is [0.8660 -0.2887 0.2887 0.2887], numBasis=3 \nk = 5, m = 1 \nprojectionOverBasis = [0 2.0 2.0 0] %5th input vector \nerrorVector = [-2.0 0.0 0.0 1.0] %using only 1st basis \n25k = 5, m = 2 \nprojectionOverBasis = [0 0.3333 -0.3333 0.6667] \nerrorVector = [-2.0 -0.3333 0.3333 0.3333] % using 2 basis vectors \nk = 5, m = 3 \nprojectionOverBasis = [-1.250 0.4167 -0.4167 -0.4167] \n30errorVector = [-0.75 -0.75 0.75 0.75] %using 3 basis vectors \nmagErrorVector = 1.5000 %add normalized error vector to basis set \n%4th basis is [-0.5 -0.5 0.5 0.5], numBasis=4 \n%abort because (numBasis &gt;= N)\n  \n\n\nThe reader can perform a similar analysis for another set of vectors and compare with the results obtained via calculating by hand.    □\n\n\n\n Application 2.2. Time-localization property of Haar coefficients.\n\n\n\n\n\n\n\n\nFigure 2.25: Signal x[n] = δ[n − 11] analyzed by 32-points DCT and Haar transforms. The right column shows the transform coefficients (abscissa is k). The left column shows time-domain plots. Below the signal itself, one can visualize the “best” (the one corresponding to the coefficient with largest absolute value) basis functions for each transform.\n\n\n\n\n\nA very simple experiment will help illustrating the time-localization property of Haar functions. Consider the signal to be analyzed in the transform domain is a single impulse x[n] = δ[n − n0],n0 = 0,…,31, which will be represented by a vector x will zeroed elements but the one corresponding to the n0 position. The script MatlabOctaveBookExamples/ex_transforms_dcthaar_example.m can be used to observe the behavior of the transforms. Figure 2.25 shows the output of the script when the signal is x[n] = δ[n − 11]. In this case, many Haar coefficients are zero and the one with largest magnitude (k = 21) has a corresponding basis function (bottom of the left column) that helps to localize the occurrence of the impulse in x[n]. In contrast, most DCT coefficients have relatively large values and the basis functions do not help in time-localization. The mentioned script allows to investigate this aspect more deeply. Compare the results of DCT and Haar transforms when the signal is a cosine summed to an impulse.   □\n\n\n\n Application 2.3. Inverse Laplace transforms with Matlab’s Symbolic Math Toolbox. Keeping in mind that Matlab adopts the unilateral Laplace transform, its Symbolic Math Toolbox can be used to calculate Laplace transforms and their inverses. For example, the unilateral Laplace transform X(s) = 1∕s of u(t) can be obtained with the commands syms t; laplace(t / t). Similarly, the commands syms w0,t; laplace(sin(w0*t)) indicate that X(s) = ω0∕(s2 + ω02) for x(t) = sin ⁡ (ω0t)u(t). The following commands illustrate the inverse transform of X(s) given by Eq. (2.45):\n\n\n\nListing 2.5: MatlabOnly/snip_transforms_ilaplace.m\n\n\nsyms s %defines s as a symbolic variable \na=1; b=-2; c=-1+j*2; %choose poles and zeros \nX=(s-a)/((s-b)*(s-c)*(s-conj(c))); %define X(s) \nilaplace(X) %Matlab's inverse unilateral Laplace transform\n  \n\n\nThe command pretty (e. g., pretty(ilaplace(X))) can be used to beautify the output, which in this case is - 3/5 exp(-2 t) + 1/5 exp(-t) (3 cos(2 t) + sin(2 t)). Find the inverse Laplace transform for other signals, such as x(t) = cos ⁡ (ω0t)u(t).    □\n\n\n\n Application 2.4. ECG transform coding. Because a\nblock transform is completely specified by an invertible matrix A, going from one domain to another (x to X or vice-versa) is loss-less, which means there is no loss of information. In many applications, such as image coding, where the goal is to minimize the number of bits to represent an image, it is useful to compress the signal using a lossy algorithm. Transform coding can be a lossy algorithm when it discards or quantizes coefficients in the transform domain. This is effective in many applications because, while all samples in x have the same “importance”, in the transform domain X the coefficients can be organized according to some hierarchy or rank. In transform coding the most “important” coefficients are quantized more carefully than the unimportant ones.\n\n\n\nFigure 2.26 depicts a segment (first 1,500 samples) of an ECG signal. The ECG data used here is from the MIT-BIH Arrhythmia Database.13, and Figure 2.26 corresponds to the first channel of file 12531_04.dat. The function ak_rddata.m indicates that the DC offset of the ADC chip was calibrated to be zero. It also indicates that the ECG signals were digitized with a sampling frequency Fs = 250 Hz, 12 bits per sample, and a quantizer with step ΔAD = 1∕400 mV. The (empirical) signal power is 3.07 mW and, following the conventional quantization model, Eq. (1.38) suggests the quantization noise power was 5.2 × 10−10 mW. Hence, the SNR corresponding to the quantization stage alone is approximately 97.7 dB.\n\n\n\n\n\n\n\n\nFigure 2.26: A segment of one channel of the original ECG data.\n\n\n\n\n\nA very simple example of a transform coding system is provided as Matlab/Octave code in the companion software (in directory Applications/ECGTransformCoding). The idea is to simply discard the higher-order coefficients in the transform domain. For example, assume that N is the dimension of x and X, which are related by a N × N transform matrix A. In the encoding stage, a discrete-time input signal x[n] is segmented into blocks of dimension N, composing a set of M input vectors {x1,…,xM}, where N × M is the available number of samples of x[n]. The coding scheme converts each xi into Xi = AHxi and keeps in a new vector X^i only the first K ≤ N elements of Xi (the remaining N − K are discarded). The M vectors X^i can be concatenated to create an encoded output signal X^[n] with M × K samples.\n\n\n\nIn the decoding stage, in order to reconstruct a signal x ′[n] from X^[n] (if there were no losses, x ′[n] = x[n]), an inverse procedure is adopted. The signal X^[n] is blocked into vectors of dimension K and N − K zero elements are inserted to create vectors X ′i of dimension N (this operation is called zero-padding). Each of these N-dimensional vectors is converted to time-domain vectors x ′i = AX ′i, which are then concatenated to form x ′[n].\n\n\n\nFigure 2.27 was obtained using a DCT matrix A of N = 32 points\nand discarding 26 (high-frequency) coefficients. It can be seen that keeping only K = 6 out of 32 coefficients is enough to provide a rough representation of x[n] but the error is significant in high-frequency regions. Note that the DCT operation is performed in blocks of N = 32 samples and Figure 2.27 is the result of processing many of these blocks.\n\n\n\n\n\n\n\n\nFigure 2.27: Original and reconstructed ECG signals with DCT of N = 32 points and discarding 26 (high-frequency) coefficients. Therefore, the error is predominant in high-frequency regions as indicated in the plot.\n\n\n\n\n\nIn practice, a quantization scheme should be adopted to represent X^[n] with a small number of bits per sample. The compression ratio is the number of bits to represent x[n],n = 0,…,M × N divided by the number of bits to represent X^[n],n = 0,…,M × K. For simplicity, the proposed example does not involve quantization and the compression ratio is evaluated by N∕K. This corresponds to assuming that each sample of both x[n] and X^[n] is represented with the same number of bits.\n\n\n\nFigure 2.28 shows the percentage of kept coefficients K∕N in the abscissa and 10log ⁡ 10MSE in the ordinate, where\n\n\n\n\n\n\n MSE = 1 M × N∑ n=0M×N−1(x[n] − x ′[n])2 \n\n\n\n\n\n\nis the mean-squared error, which is equivalent to the power of the error signal. If K = N (100% in the abscissa) the system is lossless and MSE = 0 ( −∞ in log scale), so the graphs do not show these points. The larger N, the better the coding performance but the higher the computational cost.\n\n\n\nAssuming N = 128 and K = 10 as indicated in Figure 2.28, the abscissa is K∕N = 10∕128 ≈ 7.81% and the corresponding error is MSE = 1.4824 × 10−8 W ( − 78.29 dBW) or, equivalently,  − 48.29 dBm. Given that the signal power was estimated as 3.07 mW (4.87 dBm), the SNRdB ≈ 4.87 + 48.3 = 53.16\ndB when considering only the distortion incurred by this specific encoding procedure.\n\n\n\n\n\n\n\n\nFigure 2.28: Performance of five DCT-based ECG coding schemes. The number of points is varied N ∈{4,8,32,64,128} and K = 1,2,…,M − 1. The larger N, the better the coding performance but the higher the computational cost.\n\n\n\n\n\nIt should be noted that the dataset used is formed by abnormal ECG signals and, eventually, better results could be obtained when using another dataset.\n\n\n\nWrite an encoder that uses DCT coding to generate files of a relatively small size and a decoder to “decompress” such files and recover an ECG signal (an approximation to the original one). To do that, it is necessary to quantize the DCT coefficients and pack them in a way that the resulting file has a small size. Consider using scalar or vector quantization. The advantage of vector quantization is that you need to store only the index to the codebook entry (codeword) with a vector that will represent the DCT. In case you use a codebook with 28 = 256 codewords, each entry can be conveniently written as an unsigned char of 8 bits. To design the codebook, besides the Mathwork’s code in the DSP System Toolbox, there are many algorithms such as the K-means with implementations available on the Web.\n\n\n\nAnother interesting exercise is to apply KLT and other transforms to this problem, trying to get better results than with the DCT.    □\n\n\n\n Application 2.5. DCT coding of image.\n\n\n\nThe usefulness of DCT in coding is illustrated by the JPEG image coding standard.14 Before describing an example, two dimensional transforms are briefly discussed.\n\n\n\nLinearly-separable two dimensional (2-D) transforms of a block x (e. g., a matrix with pixel values) is obtained by first using the transform along the rows (or columns) and then transforming this result along the columns (or rows). Alternatively, one can use matrix notation: X = AHxA∗. Because, A is real for a DCT, one has\n\n\n\n\n\n\n X = AT xA. \n\n\n\n\n\n\nThis is equivalent to first calculating T = AHx: the 1-D DCT transform of each column of x\nis placed in its corresponding column in a temporary matrix T. Then, each row of T is transformed by another 1-D DCT, which could be accomplished by AHTT . This result should be transposed to generate X. These operations are equivalent to:\n\n\n\n\n\n\n X = (AH(AHx)T )T  = AHxA∗. \n\n\n\n\n\n\n\n\n\n\n\nFigure 2.29: A zoom of the eye region of the Lenna image.\n\n\n\n\n\nListing 2.6 illustrates the adoption of a 2-D DCT for coding an image block (represented by a matrix).\n\n\n\nListing 2.6: MatlabBookFigures/figs_transforms_dctimagecoding\n\n\n%load black & white 256 x 256 pixels of Lenna: \nfullPath='lenna_bw.gif' %file location \n  [lenaImage,map]=imread(fullPath); %read file \ncolormap(map); %use the proper color map \n5x1=128; x2=135; y1=160; y2=167; %to take a 8x8 block close to her eye \nx=lenaImage(x1:x2,y1:y2); %get the region \nx=double(x); %cast x to be double instead of integer \n%draw a rectangle to indicate the block: \nlenaImage(x1,y1:y2)=1; lenaImage(x2,y1:y2)=1; \n10lenaImage(x1:x2,y1)=1; lenaImage(x1:x2,y2)=1; \nimagesc(lenaImage) %show the image \naxis equal %correct the aspect ratio \n%Some calculations to practice: \nX=octave_dct2(x); %calculate forward DCT \n15x2=octave_idct2(X); %calculate inverse DCT to check \nnumericError = max(x(:)-x2(:)) %any numeric error? \nAh=octave_dctmtx(8); %now calculate in an alternative way \nX2 = Ah*x*transpose(Ah); \n%X2 = transpose(Ah)*x*Ah; %Note: this would be wrong! \n20numericError2 = max(X(:)-X2(:)) %any numeric error?\n  \n\n\nSimilar to the 1-D ECG coding example in Application 2.4, few DCT coefficients can be enough to represent an image. Study the code MatlabOctaveFunctions/ak_dctdemo.m using DCT to represent different images,15 including ones with text. Even better results can be obtained with the wavelet transform, which is used in the JPEG 2000 image coding standard. Write an encoder and decoder that use a transform to compress and decompress an image file.    □\n\n\n\n Application 2.6. Orthogonality of continuous-time sinusoids over a given interval. In applications such as frequency modulation, it is important to know the conditions in which two cosines with frequencies f1 and f2 and phase 𝜃, are orthogonal over a given interval Tsym of interest. Mathematically, this requirement can be expressed as\n\n\n\n\n\n\n ∫ &lt;Tsym&gt; cos ⁡ (2πf1t + 𝜃)cos ⁡ (2πf2t + 𝜃) = 0 \n\n\n\n\n\n\nand, because the phase 𝜃 is the same for both cosines, this requirement is called “coherent orthogonality”. It can be shown16 that this condition is satisfied with integers m and n such that\n\n\n\n\n\n\n f1 = 2n − m 4Tsym ,f2 = 2n + m 4Tsym  and f2 − f1 = m 2Tsym. \n\n\n\n\n\n\nFor example, assume that Tsym = 0.01 seconds, m = 4 and n = 35. In this case, f1 and f2 are 1650 and 1850 Hz, respectively. Note that the minimum value of f2 − f1 is 1∕(2Tsym) = 50 Hz in this case, which corresponds to choosing m = 1.\n\n\n\nAs a side note, for “noncoherent orthogonality”, the minimum frequency separation17 is 1∕Tsym.    □\n\n\n\n Application 2.7. Orthogonality of finite duration discrete-time sinusoids. It is also important to evaluate in what conditions a pair of discrete-time sinusoids are orthogonal considering a finite duration interval.\n\n\n\nTwo discrete-time exponentials ejΩ0n and ejΩ1n, Ω0≠Ω1, are always orthogonal when summed over a range  −∞ to ∞ (see Example 2.12), but this is not the case when the summation interval is finite. One way of inspecting what happens when the duration is finite is to reason as following: 1) keep in mind that the sum of the samples of a single sinusoid over a multiple of its fundamental period is zero. 2) Interpreting Example 2.14 in discrete-time allows to decompose the inner product for testing orthogonality into a sum of two sinusoids. 3) the inner product is mapped to summations of sinusoids with frequencies equal to Ω0 + Ω1 and Ω0 −Ω1 and the goal is to find an integer N that is a multiple of both fundamental periods.\n\n\n\nOne application of this orthogonality principle is the design of a binary transmission, where the two signals are distinguished by their frequencies\n\nf0 and f1. This scheme is called frequency shift keying (FSK). We are interested on determining the minimum symbol period that guarantees orthogonality between the two sinusoids. In the context of telecommunications, this minimum period will correspond to the highest symbol rate. The script MatlabOctaveBookExamples/ex_transforms_check_orthogonality.m can be used for studying this question and is repeated below. The sampling frequency is Fs and one should check different pairs of f0 and f1.\n\n\n\nListing 2.7: MatlabOctaveBookExamples/ex_transforms_check_orthogonality.m\n\n\nif 1 %choose between two options \n    f0 = 980;   %frequency of first sinusoid (in telecom, of bit 0) \n    f1 = 1180;   %frequency of 2nd sinusoid (in telecom, of bit 1) \nelse %another example \n5    f0 = 1650; %1st sinusoid \n    f1 = 1850; %2nd sinusoid \nend \nFs=9600; %assumed sampling frequency \nsumSinusoidFrequency = f0+f1; \n10subractSinusoidFrequency = f1-f0; \n%reduce numbers to rational forms \n[m,n]=rat(sumSinusoidFrequency/Fs) \n[p,q]=rat(subractSinusoidFrequency/Fs) \nNmin = lcm(n,q) %find the least commom multiple \n15disp(['Minimum common period: ' num2str(Nmin)]) \nT_duration = Nmin / Fs; %minimum number of samples in seconds \n%disp(['Maximum symbol rate: ' num2str(1/T_symbolDuration) ' bauds']) \n%% Check if the product of the two sinusoids is really orthogonal \nTsum = 1 / sumSinusoidFrequency; \n20Tsub = 1 / subractSinusoidFrequency; \ndisp('Numbers below should be integers:') \nT_duration/Tsum \nT_duration/Tsub \nTs=1/Fs; %sampling interval \n25t = 0:Ts:T_duration-Ts; %generate (discrete) time \n%use sin or cos with arbitrary phase \nb0 = sin(2*pi*f0*t+4)';  %column vector \nb1 = sin(2*pi*f1*t)';  %column vector \nnormalizedInnerProduct = sum(b0.*b1) / length(b0) %inner product \n30if (normalizedInnerProduct &gt; 1e-13) \n    error('not orthogonal! should not happen!'); \nend\n  \n\n\nModify Listing 2.7 to use discrete-time sinusoids in radians, not Hz. Assume N = 8 and find a set of M sinusoids that are mutually orthogonal over an interval of eight consecutive samples. What was the maximum value of M that you could find? Now, consider the set of basis functions for a N-point DFT and evaluate their orthogonality. Going back to the question about M, can it be larger than N? Why?   □\n\n\n\n Application 2.8. Better visualizing the FFT spectrum: Using fftshift to reorganize the spectrum and masking numerical errors. Two improvements of the procedure adopted to obtain Figure 2.10 are typically used. The first one is with respect to the range k = ⟨N⟩. Figure 2.10 reminds that the definition of an N-point DFT assumes k = 0,…,N − 1. Sometimes it is more convenient to work with ranges that use “negative” frequencies. Typical ranges are k = −N∕2,−N∕2 + 1,…,0,1,N∕2 − 1 when N is even and  − (N − 1)∕2,−(N − 1)∕2 + 1,…,0,1,(N − 1)∕2 when N is odd. In these cases, to represent negative frequencies (or angles in discrete-time processing) the command fftshift can be used. For example, fftshift([0 1 2 3]) returns [2 3 0 1], while fftshift([0 1 2 3 4]) returns [3 4 0 1 2].\n\n\n\n\n\n\n\n\nFigure 2.30: Alternative representation of the DTFS / DFT of x[n] = 10cos ⁡ (π 6 n + π∕3) using fftshift. Compare with Figure 2.10 and note that the current plots clearly indicates that when x[n] is real, the magnitude and phase are even and odd functions, respectively.\n\n\n\n\n\nThe second one aims at eliminating spurious values. Note that, due to numerical errors, the elements of X that should be zero, have small values such as -1.4e-15 - j 3.3e-16, which leads to random angles for k≠1,11 in Figure 2.10. It is a good practice to eliminate these random angles based on a threshold for the magnitude. This post-processing step can be done with a command such as X(abs(X)  &lt; 1e-12)=0, and is adopted in the Matlab/Octave fft routine.\n\n\n\nFigure 2.30 benefits from these two improvements and was obtained using Listing 2.8.\n\n\n\nListing 2.8: MatlabOctaveCodeSnippets/snip_transforms_fftshift.m\n\n\nk=-N/2:N/2-1; %range with negative k (assume N is even) \nX(abs(X)&lt;1e-12)=0;%discard small values (numerical errors) \nX=fftshift(X); %rearrange to represent negative freqs. \nsubplot(211);stem(k,abs(X));subplot(212);stem(k,angle(X));\n  \n\n\nNote that fftshift does not calculate the FFT but simply reorders the elements in a vector.   □\n\n\n\n Application 2.9. DTFS analysis of single cosines. Figure 2.31 depicts five cosines xi[n] = 10cos ⁡ (Ωin) with angular frequencies Ωi = i2π∕32 rad, where i = 0,1,2,16,31, and their respective DTFS spectrum calculated with a DFT of N = 32 points and normalized by N.\n\n\n\n\n\n\n\n\nFigure 2.31: Five cosine signals xi[n] = 10cos ⁡ (Ωin) with frequencies Ωi = 0, 2π∕32, 4π∕32, π, 31π∕16 for i = 0,1,2,16,32, and the real part of their DTFS using N = 32 points. \n\n\n\n\n\nThe first x0[n] = 10 is a DC signal (Ω0 = 0), the second is x1[n] = 10cos ⁡ (2π∕32n), where Ω1 = 2π 32  is the fundamental frequency in this DTFS analysis (given that N = 32) and the other three signals are cosines with the following harmonic frequencies: Ω2 = 2Ω1, Ω16 = 16Ω1 and Ω31 = 31Ω1 rad. Because the cosines are even functions, the imaginary part of their DTFS is zero.\n\n\n\nNote in Figure 2.31 that the frequency increases up to the N∕2-th harmonic Ω16 = 16Ω0 = π. When N is even, the coefficient of k = N∕2 corresponds to Ω = 2π N k|k=N∕2 = π. It is important to note that the highest “frequency” of a discrete-time signal is always Ω = π rad.\n\n\n\nAnother aspect of Figure 2.31 is that for x0[n] and x16[n], corresponding to the angles Ω = 0 and Ω = π, the spectra have only one non-zero coefficient X[k], while the other three signals are represented by two coefficients. Figure 2.3, for N = 4 and N = 6 can be invoked to help interpreting why the coefficients for angles different than Ω = 0 and Ω = π occur in pairs with Hermitian symmetry (i. e., X[k] = X∗[−k] when x[n] is real). The basis functions for Ω = 0 and Ω = π are real signals while the other angles lead to complex-valued signals. In the analysis of a real signal x[n], using the k-th complex basis requires also using the  − k-th basis, such that their imaginary parts can cancel in the synthesis of a real x[n].  \n □\n\n\n\n Application 2.10. DTFS analysis of a sum of sines and cosines. This example aims at illustrating how the cosines and sines are mapped in the DTFS, especially when two of them have the same frequency. Figure 2.32 shows the spectrum of a more complex example obtained with Listing 2.9.\n\n\n\nListing 2.9: MatlabOctaveCodeSnippets/snip_transforms_DTFS_sinusoid.m\n\n\nN=32; %number of DFT-points \nn=0:N-1; %abscissa to generate signal below \nx=2+3*cos(2*pi*6/32*n)+8*sin(2*pi*12/32*n)-... \n    + 4*cos(2*pi*7/32*n)+ 6*sin(2*pi*7/32*n); \n5X=fft(x)/N; %calculate DTFS spectrum via DFT \nX(abs(X)&lt;1e-12)=0; %mask numerical errors\n  \n\n\n\n\n\n\n\nFigure 2.32: Analysis with DFT of 32 points of x[n] composed by three sinusoids and a DC level. From top to bottom: x[n], real, imaginary, magnitude and phase of X[k].\n\n\n\n\n\nNote in Figure 2.32 that the coefficient X[0] = 2 is due to the DC level, X[6] = X[−6] = 1.5 are due to the cosine 3cos ⁡ (6(2π∕32)n), X[12] = −4j and X[−12] = 4j are due to 8sin ⁡ (12(2π∕32)n), X[7] = −2 − 3j and X[−7] = −2 + 3j are due to the two parcels of frequency 7(2π∕32) with the  − 4cos ⁡ (7(2π∕32)n) being represented by the real part ( − 2) and 6sin ⁡ (7(2π∕32)n) represented by the imaginary part (3j).   □\n\n\n\n Application 2.11. Spurious frequency components: When the number N of DFT points is not a multiple of the signal period. Figure 2.33 illustrates the DTFS spectrum of a signal x[n] = 4cos ⁡ ((2π∕6)n) with period of 6 samples obtained via a 16-points DFT. Note that because 16 is not a multiple of 6, the spectrum has many non-zero spurious components. The coefficients X[3] = 0.99 − 1.48j and X[−3] = 0.99 + 1.48j, both with magnitude 1.78, are the closest to representing the angle π∕3 ≈ 1.05 rad. In fact, when using N = 16 points to divide 2π (see Figure 2.3), the angle increment is\n\n\n\n\n\n\n\n\n\n ΔΩ = 2π N , \n\n\n(2.53)\n\n\n\n\n\n\nwhich in this case is ΔΩ = π∕8 ≈ 0.39, and the DFT / DTFS can deal only with the angles [0,0.39,0.78,1.18,1.57,…,5.89] rad. This\nexplains why X[3] and X[−3] are concentrating most of the power P = 42∕2. The other coefficients absorb the remaining power that “leaks” due to the imperfect match of the cosine frequency π∕3 with the discrete grid of 16 points imposed by the DFT / DFTS.\n\n\n\n\n\n\n\n\nFigure 2.33: Spectrum of a signal x[n] = 4cos ⁡ ((2π∕6)n) with period of 6 samples obtained with a 16-points DFT, which created spurious components.\n\n\n\n\n\nThe creation of spurious components is associated to leakage and the picket-fence effect, which are discussed in Section 4.3. This phenomenon is very common when using the FFT to analyze signals that are non-periodic, have infinite duration, etc. In the current case, the spurious components could be avoided by choosing an appropriate number of points for the DFT / DTFS (a multiple of 6). In practice, spurious components due to FFT usage occur most of the times. Even when the original signal is periodic, say x(t) with period T, it is typically not guaranteed that NTs is a multiple of T, where N is the number of DFT points and Ts the sampling period.\n\n\n\n\n\n\n\n\nFigure 2.34: Explicitly repeating the block of N cosine samples from Figure 2.33 to indicate that spurious components are a manifest of the lack of a perfect cosine in time-domain.\n\n\n\n\n\nFigure 2.34 shows part of the signal obtained by repeating the segment of 16 samples depicted in Figure 2.33. This represents the signal “assumed” by the DFT / DTFS. Spurious components appear because, in spite of x[n] having a period of N = 6, an abrupt transition (from  − 4 to 4) occurs in the boundary of the segments of 16 samples. In general, the periodic extension (that is assumed by the FFT when approximating a DTFS) of a signal x[n] not commensurate18 with its period, leads to discontinuities at the boundaries of the replicas of x[n].    □\n\n\n\n Application 2.12. DTFS for periodic discrete-time pulses. Assume a periodic train of pulses x[n] with period N and one period described as x[n] = 1 from n = 0 to N1 − 1 and x[n] = 0 from N1 to N − 1. Its DTFS coefficients are given by\n\n\n\n\n\n\n X[k] = 1 N∑ n=0N−1x[n]e−j2π N nk = 1 N∑ n=0N1−1e−j2π N nk = 1 N∑ n=0N1−1  (e−j2π N k) n, \n\n\n\n\n\n\nwhich is the sum of N1 terms of a geometric series with ratio r = e−j2π N k. Applying Eq. (B.20) leads to\n\n\n\n\n\n\n X[k] = 1 N  (1 − e−j2π N N1k 1 − e−j2π N k ) . \n\n\n\n\n\n\nThis expression can be simplified by applying Eq. (B.14) to both numerator and denominator:\n\n\n\n\n\n\n\n\n\n X[k] = 1 N sin ⁡ (kN1π∕N) sin ⁡ (kπ∕N) e−jkπ N (N1−1), \n\n\n(2.54)\n\n\n\n\n\n\nfor k = 0,…,N − 1 and then periodically repeated in k. For k = 0 the expression is undetermined (because of the 0∕0 fraction) and L’Hospital’s rule leads to X[0] = N1∕N.\n\n\n\nListing 2.10 obtains the DTFS of the periodic pulses in two distinct ways.\n\n\n\nListing 2.10: MatlabOctaveCodeSnippets/snip_transforms_DTFS_pulses.m\n\n\nN=10; N1=5; k=0:N-1; %define durations and k \nXk=(1/N)*(sin(k*N1*pi/N)./sin(k*pi/N)) .* ... \n    exp(-j*k*pi/N*(N1-1)); %obtain DTFS directly \nXk(1)=N1/N; %eliminate the NaN (not a number) in Xk(1) \n5%second alternative, via DFT. Generate x[n]: \nxn=[ones(1,N1) zeros(1,N-N1)] %single period of x[n] \nXk2=fft(xn)/N %DTFS via DFT, Xk2 is equal to Xk\n  \n\n\n\n\n\n\n\nFigure 2.35: Three periods of each signal: pulse train x[n] with N = 10 and N1 = 5 and amplitude assumed to be in Volts (a), the magnitude (b) and phase (c) of its DTFS.\n\n\n\n\n\nFigure 2.35 illustrates the result using N = 20 and N1 = 10. Note that |X[k]| has the behavior of a sinc function, as expected. The DTFS phase in Figure 2.35 c) is the combined effect of the linear phase e−jkπ N (N1−1) and the sign of sin ⁡ (kN1π∕N)∕sin ⁡ (kπ∕N) in Eq. (2.54). This sign is responsible for Figure 2.35 c) not being a linear function of frequency.\n\n\n\n                                   &lt;div class='center'&gt;\n\n\n\n\n                               &lt;div class='subfigure'&gt;&lt;table&gt;&lt;tr&gt;&lt;td style='text-align:left'&gt;&lt;img alt='PIC' src='Figures/dtfs_pulse_duty_4.png' /&gt;\n\n(a) N1 = 4"
  },
  {
    "objectID": "ak_dsp_bookse28.html",
    "href": "ak_dsp_bookse28.html",
    "title": "28  Comments and Further Reading",
    "section": "",
    "text": "Some authors recognize the advantages of presenting concepts of digital signal processing before their analog counterpart. This chapter adopts this approach. For example, it presents block transforms before the continuous time Fourier transform.\n\n\n\nA commonly adopted notation is to call inverse transformation the matrix multiplication x = AX, while the forward (or direct) transformation is denoted as X = A−1x., such that the basis are the columns of A  [?]. Experience proved that this notation confuses beginners and it was not adopted here.\n\n\n\nMost textbooks introduce Z and Laplace transforms using the fact that their basis functions are eigenfunctions of linear and time-invariant systems. In this chapter, both transforms were presented as extensions of their Fourier counterparts.\n\n\n\nAll the four Fourier representations are interrelated. There are many interesting properties among the representations, which are explored in, e. g.,  [?].\n\n\n\nWith respect to block transforms, the attention is restricted to the ones represented by square matrices. Also, the emphasis is on their use and interpretation. Fast algorithms are out of the scope. A more advanced treatment of transforms, including lapped transforms (that correspond to non-square matrices) and fast algorithms can be found in  [?].\n\n\n\nThere are four DCT transforms [?]. The DCT-II was assumed in this text because it is the most popular for coding applications.\n\n\n\nIt is assumed by default the bilateral Laplace and Z transforms. The unilateral versions are useful for solving differential and difference equations, which is a task not emphasized in this text.\n\n\n\nUnitary matrices are sometimes called orthogonal matrices (instead of orthonormal), which is confusing. For vectors, the jargon is more consistent.\n\n\n\nA nice geometrical explanation about linearly independent, orthogonal, and uncorrelated variables is provided in  [?]."
  },
  {
    "objectID": "ak_dsp_bookse29.html",
    "href": "ak_dsp_bookse29.html",
    "title": "29  Review Exercises",
    "section": "",
    "text": "2.1. Given two vectors [4,3] and [−5,2], find their norms, inner product and angle between them. 2.2. Given two vectors x = [4,0] and y = [−5,2], find the projections pyx and pxy and the respective error vectors eyx and exy. 2.3. Is the matrix A =  [  1  −5   3   4  ] unitary? Design a unitary matrix B that is similar to A in the sense that its first basis (column) is a vector in the same direction as [1,3]T . 2.4. Assume a 2-d vector space has non orthogonal basis given by i¯ = [2,1] and j¯ = [0,3]. Find the coefficients α and β that allow to represent the vector x = [4,1] as the linear combination x = αi¯ + βj¯. 2.5. Assuming a 2-d vector space with orthonormal basis vectors i¯ and j¯, prove that inner products can be used to find the coefficients α and β that allow to represent a vector x = [x1,x2] as the linear combination x = αi¯ + βj¯. 2.6. Find the inner product between the signals: a) u(t) and e−0.9t,\n\n\ncos ⁡ (0.5πn) and δ[n],\n\nu[−n − 1] and u[n]."
  },
  {
    "objectID": "ak_dsp_bookse30.html",
    "href": "ak_dsp_bookse30.html",
    "title": "30  Exercises",
    "section": "",
    "text": "2.1. A segment of an ECG signal x[n] with six samples [−1,0.6,0.5,1,2.5,2] must be encoded using a 3-points DCT (i. e., using two frames with three samples each). Find the DCT coefficients of these two vectors. Then, reconstruct an approximation of the two vectors assuming that only the first coefficient can be used while the other two are discarded (assumed to be zero). Confirm Eq. (2.19) calculating the norms of both error vectors et and ef. 2.2. A segment of an ECG signal with four samples [0.5,1,2.5,2] must be encoded using a 2-points DCT (i. e., using two frames with two samples each). The coefficients at the DCT domain must be quantized with 4 bits per frame, with the first coefficient (DC) being quantized with 3 bits and the other one with 1 bit. Both quantizers have a step of Δ = 0.5 and their first output level is 0.25. a) What is the decoded signal (after coding and decoding)? b) What is the mean squared-error? c) What is the SNR in dB? d) What is the bit rate in bps assuming Fs = 150 Hz? e) What is the bit rate and SNR if the second coefficient is discarded (no bits are allocated for it and its value is assumed to be zero during decoding)? 2.3. Consider that a block of pixels C =  [  120  140  139   124   220   120   114  130  122   ] was extracted from an image. Calculate the bi-dimensional DCT and DFT first processing each row and organizing the result of this intermediate step as a matrix T, then apply the transforms to each column of T to generate the final matrix. Follow this procedure to obtain a matrix with the DCT coefficients and then repeat it to obtain the DFT coefficients. 2.4. Design a DCT-based image coding system that is capable of handling 24 bits/pixel color images\nand 8 bits/pixel black and white (B&W, also called gray) images, all with a size of 512 × 512 pixels. The system should operate using: a) 3 bits/pixel, b) 1 bit/pixel or c) 0.25 bit/pixel. For the three cases, provide the average peak SNR (PSNR), i. e., the SNR when assuming the “signal” in the numerator is an image with all pixels at the peak value of 255 and the denominator is the mean squared-error. The test sequence should be the colorful and B&W versions of the images Lenna, Peppers, Baboon, Splash and Tiffany, available from [url2us2]. Do not download B&W versions of the five mentioned images, but make the conversion from color to B&W yourself and describe the conversion method you used. Hence, the test set will have 5 images. You can use as many images you want to compose your training set (distinct from the test set images). Provide the source code and a short report of your results that includes the average PSNR. 2.5. Use the Gram-Schmidt procedure to find an orthonormal basis to represent the vectors in the set C = {[0,0,1],[0,1,0],[0,0,8],[0,5,0],[4,1,2],[0,0,3]}. Show also that any vector in C can be written as a linear combination of the obtained basis vectors. 2.6. KLT (or PCA) is the optimal linear transform for coding. But one needs to remember that PCA assumes the signal is zero mean. This exercise uses this fact to exemplify the importance of following the assumed model (in this case, that the signal is zero mean). Design a coding system for the data x below.\n\n\n\nListing 2.13: MatlabOctaveCodeSnippets/snip_transforms_KLT.m\n\n\nN=100; %number of vectors \nK=4;   %vector dimension \nline=1:N*K; %straight line \nnoisePower=2; %noise power in Watts \n5temp=transpose(reshape(line,K,N)); %block signal \nx=temp + sqrt(noisePower) * randn(size(temp)); %add (AWGN) noise \nz=transpose(x); plot(z(:)) %prepare for plot\n\n\n\nBecause PCA / KLT does not model the mean, in practice, one needs to extract the mean from the data and later recover it for final reconstruction. And, eventually, one also normalizes the variances to be equal to one. This normalization is called in statistics to “z-transform” the data, but this term should not be confused with the Z transform. 2.7. Generate N = 10,000 samples from a bidimensional Gaussian fX 1,X 2(x1,x2) with mean μ^ = (30, − 20)T  and covariance matrix C =  [  10  0.99   0.99   40  ]. Obtain two transforms: Ag using Gram-Schmidt orthonormalization and Ap\nusing PCA. You can remove means if useful. Using each transform, project the data into the new space and compute the variance of the coefficients in each dimension. Which transform you expect to lead to smaller variances of the transformed vectors? 2.8. Following a scheme similar to the one presented in Application 2.4 - ECG Coding, compare the performance of KLT and DCT using a test set of at least 50 ECT records from the MIT-BIH Arrhythmia Database. Note that for performing any preparation / training you should use a training set, disjoint of the test set. Otherwise your results can be biased by the fact that you already knew what should be coded. 2.9. Calculate “manually” the DFT of the sequence x[n] = [2,0,2,0]. Then use the fft function in Matlab/Octave and compare the results. 2.10. What are the forward A and inverse AH matrices for a 3-points orthonormal DFT? 2.11. Consider that s[n] was obtained by sampling with Fs = 100 kHz an analog and periodic signal s(t). The sampling theorem was obeyed. The 8-DFT of s[n] was calculated in Matlab/Octave with fft(s)/N and is shown in Figure 2.41. The value N was chosen to be an exact multiple of the fundamental period of s(t) such that the problem of spurious components illustrated in Figure 2.34 did not occur. a) What is the frequency of each component of s(t)? b) What is the resolution Δf in Hz of this DFT? c) Find a reasonable expression for s(t).\n\n\n \n\nFigure 2.41: DFT with N = 8\npoints of a signal sampled at Fs = 100 kHz.\n\n\n\n\n2.12. Given that\n\n\nX = [8,0,0,0]\n\nwas obtained with a 4-points orthonormal DFT, find its inverse\n\n\nx\n\n. For the same spectrum, assuming that\n\n\nFs = 10\n\nHz, to which frequencies correspond the elements of\n\n\nX\n\n? 2.13. Consider that a periodic\n\n\nx(t)\n\nwas sampled with\n\n\nFs = 20\n\nHz, obeying the sampling theorem. A 128-points FFT was calculated using Matlab/Octave and the output\n\n\nX\n\nwas normalized by\n\n\nN = 128\n\n. The only non-zero coefficients were\n\n\nk = 0,3\n\nand 125, corresponding to the elements of\n\n\nX[1] = 12\n\n,\n\n\nX[4] = 3 + j4\n\nand\n\n\nX[126] = 3 − j4\n\n, respectively. a) What is the frequency in Hz of the component with largest power of\n\n\nx(t)\n\n? b) what is the average (DC) value of\n\n\nx(t)\n\n? c) what is the spacing in Hz between neighboring elements of\n\n\nX\n\n? d) what is the maximum frequency found in\n\n\nx(t)\n\n? 2.14. Using the orthogonality condition of the basis functions derive the pair of Fourier series equations. To do that, recall how the inner product between complex signals is defined using the conjugate, and the normalization factor because the basis are not orthonormal. 2.15. What are the Fourier series coefficients for the signal\n\n\nx(t) = 2 + 3cos ⁡ (2π100t) + 8sin ⁡ (2π300t + π) + cos ⁡ (2π50t − π)\n\n? 2.16. The bilateral Fourier series of a signal\n\n\nx(t)\n\nwith period\n\n\nT0 = 0.01\n\n s has non-zero coefficients\n\n\nc0 = 3\n\n,\n\n\nc1 = 2e−j4\n\n,\n\n\nc4 = 5\n\n,\n\n\nc−1 = 2ej4\n\nand\n\n\nc−4 = 5\n\n. What is the expression for\n\n\nx(t)\n\n? 2.17. What is the Fourier transform of: a)\n\n\nx(t) = rect(t)\n\n, b)\n\n\ny(t) = 4rect(t − 6)\n\nand c)\n\n\nz(t) = 5rect((t − 4)∕3)\n\n? 2.18. What is the Fourier transform of: a)\n\n\nx(t) = sinc(t)\n\n, b)\n\n\ny(t) = 4sinc(t − 6)\n\nand c)\n\n\nz(t) = 5sinc((t − 4)∕3)\n\n? 2.19. Prove the expressions: a) for the DTFT of\n\n\nx[n] = 0.9nu[n] + 0.8nu[n]\n\nand b) for its Z transform, indicating the ROC. 2.20. Considering the range\n\n\nΩ ∈  [−π,π[\n\n, the DTFT\n\n\nH(ejΩ)\n\nof a signal is 1 when\n\n\n −Ωc ≤Ω ≤Ωc\n\nand 0 otherwise (i. e., an ideal low-pass filter). Find its inverse DTFT\n\n\nh[n]\n\nindicating all steps in this proof. 2.21. Prove that\n\n\nX(ejΩ) = ∑ ⁡ k=−∞∞2πδ(Ω −Ω0 + 2πk)\n\nis the DTFT of\n\n\nx[n] = ejΩ0n\n\nand indicate when the impulse sifting property is used. Using the result, carefully draw the graph of the real part of the DTFT of\n\n\ny[n] = 3 + 4ejπn\n\nfor the range\n\n\n − 3π ≤Ω ≤ 3π\n\n. 2.22. Prove the cited properties of the Fourier tools for the case of the Fourier transform. 2.23. Prove the cited Fourier pairs for the case of the DTFT. 2.24. A right-sided signal has Laplace transform\n\n\nX(s) = 5∕(s2 + 2s + 10)\n\n, with ROC\n\n\nσ &gt; −1\n\n. Calculate: a)\n\n\nX(s)|s=2\n\n, b) the corresponding Fourier transform\n\n\nX(ω)\n\nif it exists and c) if\n\n\nX(ω)\n\nexists,\n\n\n|X(ω)|\n\nfor\n\n\nω = 3\n\nrad/s. 2.25. A signal\n\n\nx(t)\n\nhas Laplace transform\n\n\nX(s) = (3s + 2)∕((s − 1)(s − 3))\n\n, with ROC\n\n\nσ &lt; 1\n\n. Find\n\n\nx(t)\n\n. 2.26. A signal\n\n\nx[n]\n\nhas Z transform\n\n\nX(z) = (5z2 + 4z + 3)∕((z − 0.7)(z − 0.8))\n\n, with ROC\n\n\n|z| &gt; 0.8\n\n. Find\n\n\nx[n]\n\n. 2.27. Find the Z transform and associated ROC of the signal\n\n\nx[n] = −3nu[−n − 1]\n\n. Does this signal have a DTFT? 2.28. What is the Z transform\n\n\nX(z)\n\nof the signal\n\n\nx[n] = 4δ[n] + 5δ[n − 11]\n\n? What are the values of\n\n\nX(z)\n\nfor\n\n\nz = 0,−1\n\nand\n\n\nz = 8ejπ\n\n? If the DTFT of\n\n\nx[n]\n\nexists, what are its values for\n\n\nΩ = 0,π\n\nand\n\n\n2π\n\nrad?"
  },
  {
    "objectID": "ak_dsp_bookch3.html",
    "href": "ak_dsp_bookch3.html",
    "title": "Analog and Digital Systems",
    "section": "",
    "text": "3.1  To Learn in This Chapter  3.2  Contrasting Signals and Systems  3.3  Analog and Digital Filters: A Quick Hands-On Tour  3.3.1  Cutoff and natural frequencies  3.3.2  Designing simple filters using specialized software  3.4  Linear Time-Invariant Systems  3.4.1  Impulse response and convolution for LTI systems  3.4.2  Convolution properties  3.4.3  Convolution via correlation and vice-versa  3.4.4  Discrete-time convolution in matrix notation  3.4.5  Approximating continuous-time via discrete-time convolution  3.4.6  Frequency response: Fourier transform of the impulse response  3.4.7  Fourier convolution property  3.4.8  Circular and fast convolutions using FFT  3.5  Sampling and Signal Reconstruction Revisited  3.5.1  A proof sketch of the sampling theorem  3.5.2  Energy and power of a sampled signal  3.5.3  Energy / power conservation after sampling and reconstruction  3.5.4  Sampling theorem uses a strict inequality  3.5.5  Undersampling or passband sampling  3.5.6  Sampling a complex-valued signal  3.5.7  Signal reconstruction and D/S conversion revisited  3.6  Facts About First and Second-Order Systems  3.6.1  First-order systems  3.6.2  Second-order systems  3.7  Bandwidth and Quality Factor  3.7.1  Bandwidth and Quality Factor of Poles  3.7.2  Bandwidth and Quality Factor of Filters  3.8  Importance of Linear Phase (or Constant Group Delay)  3.9  Filtering technologies: Surface acoustic wave (SAW) and others  3.10  Digital Filters  3.10.1  FIR, IIR, AR, MA and ARMA systems  3.10.2  Filter frequency scaling  3.10.3  Filter bandform transformation: Lowpass to highpass, etc.  3.11  IIR Filter Design  3.11.1  Direct IIR filter design  3.11.2  Indirect IIR filter design  3.11.3  Methods to convert continuous into discrete-time system functions  3.11.4  Summary of methods to convert continuous-time system function into discrete-time\n 3.12  Bilinear Transformation  3.12.1  Bilinear mapping between s and z planes and vice-versa  3.12.2  Non-linear frequency warping imposed by bilinear  3.12.3  Bilinear for IIR filter design using pre-warping  3.12.4  Bilinear design of IIR filter from continuous-time specifications  3.12.5  Bilinear for IIR design when continuous-time system function is provided  3.12.6  Properties of the bilinear transform  3.12.7  Pre-warped bilinear applied to a first-order system  3.12.8  Bilinear for digital controller  3.13  FIR Filter Design  3.13.1  A FIR filter does not have finite poles  3.13.2  The coefficients of a FIR coincide with its impulse response  3.13.3  Algorithms for FIR filter design  3.13.4  FIR design via least-squares  3.13.5  FIR design via windowing  3.13.6  Two important characteristics: FIRs are always stable and can have linear phase  3.13.7  Examples of linear and non-linear phase filters  3.13.8  Zeros close to the unit circle may impact the phase linearity  3.13.9  Four types of symmetric FIR filters  3.14  Realization of Digital Filters  3.14.1  Structures for FIR filters  3.14.2  Structures for IIR filters  3.14.3  Running a digital filter using filter or conv  3.14.4  Effects of finite precision  3.15  Multirate Processing  3.15.1  Upsampler and interpolator  3.15.2  Downsampler and decimator  3.16  Applications  3.17  Comments and Further Reading  3.18  Exercises  3.19  Extra Exercises"
  },
  {
    "objectID": "ak_dsp_bookse31.html",
    "href": "ak_dsp_bookse31.html",
    "title": "31  To Learn in This Chapter",
    "section": "",
    "text": "Understand that complex exponentials are eigenfunctions of LTI systems\n\n\nDesign and use analog and digital filters\n &lt;/li&gt;\n &lt;li class='itemize'&gt;Interpret the group delay and use it to evaluate the delay imposed by a system\n &lt;/li&gt;\n &lt;li class='itemize'&gt;Observe how a system (channel) modifies an input (transmitted) signal\n &lt;/li&gt;\n &lt;li class='itemize'&gt;Reinterpret sampling and signal reconstruction&lt;/li&gt;&lt;/ul&gt;"
  },
  {
    "objectID": "ak_dsp_bookse32.html",
    "href": "ak_dsp_bookse32.html",
    "title": "32  Contrasting Signals and Systems",
    "section": "",
    "text": "3.2  Contrasting Signals and Systems\n\n\n\nThe previous discussion focused on signals, while this chapter discusses how to model and work with systems. System is a generic term that can be applied to any entity that converts one (or more) input signal(s) into one (or more) output signal(s). When there are multiple inputs and outputs, the system is called MIMO. This chapter will assume that the systems have a single input and a single output (SISO). It is conventional to call the input x(t) or x[n], for continuous and discrete-time signals, respectively, while the system output is denoted as y(t) or y[n]. In general, the input to output mapping y(t) = H{x(t)} will be described by an operator H and depicted as\n\n\n\n\n\n\n x[n]→ &lt;mstyle  mathvariant=“script”&gt;H  →y[n]orx(t)→ &lt;mstyle  mathvariant=“script”&gt;H  →y(t). \n\n\n\n\n\n\nIn the special case of linear and time-invariant (LTI) systems (these concepts will be further discussed in Section 3.4), a signal called impulse response h(t) or h[n] is capable of fully representing the behavior of the system (i. e., its output) to any input. In other words, the impulse response completely represents the LTI system. For this very special case of LTI systems, the input/output relation is represented by\n\n\n\n\n\n\n x[n]→ h[n] →y[n]orx(t)→ h(t) →y(t) \n\n\n\n\n\n\nin the case of discrete or continuous-time systems, respectively. Note that having a signal (the impulse response) representing a (LTI) system may be confusing.\n\n\n\nThe Laplace transform H(s) of the continuous-time impulse response h(t) is called system function or transfer function. In the discrete-time case, the system function is the Z transform H(z) of h[n]. Similarly, the Fourier transforms H(ω) and H(ejΩ) of the impulse responses are called frequency responses. Instead of the impulse response, the LTI system can be also represented via the system function as, for example in:\n\n\n\n\n\n\n x[n]→ H(z)  →y[n]andx(t)→ H(s)  →y(t) \n\n\n\n\n\n\nor via its frequency response H(ω) or H(ejΩ). Table 3.1 summarizes these relations and nomenclature.\n\n\n\n\n\n\n\nTable 3.1: Relations of the impulse response to the system function and frequency response of LTI systems.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTime \n\n\n\nSystem function\n\n    &lt;/td&gt;&lt;td colspan=\"2\" style=\"text-align:center; white-space:nowrap;\" id=\"TBL-101-1-4\"  \nclass=“td11”&gt;\n\nFrequency response\n\n\n\n\n\n\n\n\n\nNomenclature\n\n\n\n\n\nOutput/input\n\n\n\n\n\nNomenclature\n\n\n\n\n\nOutput/input\n\n\n\n\n\nContinuous \n\n\n\n\n\nH(s) Laplace of h(t)\n\n\n\n\n\n\nH(s) = Y (s) X(s)\n\n\n\n\n\n\nH(ω) Fourier of h(t)\n\n\n\n\n\n\nH(ω) = Y (ω) X(ω)\n\n\n\n\n\nDiscrete \n\n\n\n\n\nH(z)          Z of h[n]\n\n\n\n\n\n\nH(z) = Y (z) X(z)\n\n\n\n\n\n\nH(ejΩ) DTFT of h[n]\n\n\n\n\n\n\nH(ejΩ) = Y (ejΩ) X(ejΩ)\n\n\n\n\n\n\n\n\n\n\n\n\nBoth system function and frequency response relate the input to the output by a scaling factor (gain and phase) κ that varies with the independent variable (s, z, Ω, etc.). For example, if the factor is κ = 3 for a given situation, the output will be three times the input value. In other words, assume that κ = H(ω0) = 3 for a given frequency ω = ω0 rad/s. If the Fourier transform X(ω) of the input has a value X(ω0) = 7 + j4 at this frequency, the output at ω0 is Y (ω0) = H(ω0)X(ω0) = 21 + j12. Note that, in general, both transfer functions and frequency responses are complex-valued.\n\n\n\nThe units for system functions and frequency responses depend on the corresponding units of the input and output signals. For example, H(s) can be given in Ohms if x(t) and y(t) are given in Amperes and Volts, respectively.\n\n\n\nAn important special case of continuous-time LTI systems H(s) are the ones described by linear differential equations with constant coefficients (coefficients that do not change over time). Similarly, the discrete-time LTI system H(z) described by a linear constant-coefficient difference equation (LCCDE) is widely used in DSP. The LCCDE is often simply called difference equation and can be found via the inverse Z transform of H(z).\n\n\n\nMost physical systems have non-linearities and a behavior that changes with time. In other words, most physical systems are not LTI. But even if the system is not strictly LTI, in many important applications it is useful to model the system as such, and benefit from the large number of tools that exist to design and analyze LTI systems."
  },
  {
    "objectID": "ak_dsp_bookse33.html",
    "href": "ak_dsp_bookse33.html",
    "title": "33  Analog and Digital Filters: A Quick Hands-On Tour",
    "section": "",
    "text": "This section presents a brief introduction to a special class of systems called LTI filters, which typically have the characteristic of being frequency-selective. LTI systems will be further discussed in Section 3.4 but the goal here is to provide concrete examples on frequency-selective filters, due to their importance in DSP. Other filters will be discussed, but this section starts with the two most common filters: lowpass and highpass.\n\n\n\nThe lowpass filter is characterized by attenuating (or rejecting) the frequency components that are above a given frequency called bandpass frequency fp, while providing a gain κ &gt; 0 to the frequency components from 0 to fp. The name lowpass is used because the filter allows the low-frequency components of x(t) to “pass” and compose the output. Similarly, the highpass tries to reject the components of x(t) that are located from 0 to its fp in the frequency spectrum, while keeping those higher than fp.\n\n\n\n\n\n\n\n\n\n\n\n(a) Lowpass\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Highpass\n\n\n\n\n\n\n\nFigure 3.1: Ideal magnitude specifications for lowpass and highpass filters.\n\n\n\n\n\nFigure 3.1(a) and Figure 3.1(b) depict the ideal specification of low and highpass filters, respectively. These figures only illustrate the magnitude (gain κ) and, at this moment, the phase Θ is assumed to be zero.\n\n\n\n Example 3.1. Lowpass and highpass filtering examples. For example, assume a signal x(t) = 3cos ⁡ (2π100t) + 8cos ⁡ (2π200t) (t in seconds) is the input to the lowpass filter of Figure 3.1(a) with κ = 1, Θ = 0 and fp = 150 Hz. The output would be y(t) = 3cos ⁡ (2π100t) because the component of frequency 200 Hz would be filtered out. If the same x(t) is the input to the highpass filter of Figure 3.1(b) with κ = 4, Θ = 0 and fp = 190 Hz, the output would be y(t) = 32cos ⁡ (2π200t). In this case, besides eliminating the lowpass component, the filter imposed a gain of 4 to the amplitude of the 200 Hz component.    □\n\n\n\nFigure 3.2 shows the magnitude of the frequency response of a (second order analog) filter. This figure should be contrasted to the ideal case of Figure 3.1. In practice the filter gain cannot instantaneously change from 1 to 0 as idealized in Figure 3.1. Instead, this variation (the attenuation rolloff) depends on the filter order and creates a transition region that is defined as the range of frequencies between fp and the stopband frequency fr.\n\n\n\n\n\n\n\n\nFigure 3.2: Magnitude of the frequency response of a practical analog filter in linear scale (|H(f)|, top) and in dB (20log ⁡ 10|H(f)|), which should be compared to the ideal case of Figure 3.1. Three frequencies are of interest: passband (fp = 100 Hz), cutoff (fc = 158.5 Hz) and stopband (fr = 500 Hz) frequencies. \n\n\n\n\n3.3.1  Cutoff and natural frequencies\n\n\n\nBesides fp and fr, another frequency of interest is the so-called cutoff frequency fc. Assuming the filter gain at the passband is κ, the cutoff is the frequency for which the linear gain is κ∕2 ≈ 0.707κ. The cutoff indicates the frequency in which the filter attenuates a signal component to half of its power at the passband center. For example, assume an input signal x(t) has a component Acos ⁡ (2πfct) with power A2∕2, which is passed through a highpass filter with unitary gain at passband and cutoff frequency fc. This component will show up at the filter output as  A 2 cos ⁡ (2πfct), which has power A2 4 , corresponding to half of the original power. In dB scale, the cutoff frequency corresponds to a gain of 20log ⁡ 10(1∕2) ≈−3.01 dB, as illustrated in Figure 3.2 for a lowpass filter with gain κ = 1 at DC.\n\n\n\nThe cutoff frequency should not be confused with the natural frequency, which is detailed in Figure 3.23. Table 1 is a useful reference for the special frequencies used in signal processing.\n\n\n\n\n\nFilter masks\n\n\n\nThe filter designer often has a specification mask that should be obeyed. The passband and other special frequencies are used to described the mask.\n\n\n\n\n\n\n\n\n\n\n\n(a) Lowpass.\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n(b) Highpass.\n\n\n\n\n\n\n\nFigure 3.3: Example of specification masks for designing low and highpass filters.\n\n\n\n\n\nFigure 3.3(a) and Figure 3.3(b) depict masks for low and highpass filters, respectively. In this case, the values Apass and Astop indicate the maximum and minimum attenuation in dB for the passband and stopband, respectively. These bands are indicated by the frequencies Fp and Fr, for pass and rejection bands.\n\n\n\n\n\n\n\n\nFigure 3.4: Example of specification mask for designing bandpass filters.\n\n\n\n\n\nBesides lowpass and highpass, some other popular filters are the so-called bandpass and bandstop (or band-reject) filters. Figure 3.4 shows a bandpass specification.\n\n\n3.3.2  Designing simple filters using specialized software\n\n\n\nBefore studying digital filters, it is useful to briefly discuss analog filters because most of the times they are required in digital signal processing systems as will be discussed here.\n\n\n\n\n\nExample of analog filter design\n\n\n\n\n\n\nThere are integrated circuits (IC) that implement analog filters. They can also be built using discrete components: resistors, capacitors, inductors and operational amplifiers (opamps). The design of a filter corresponds to obtaining its system function. In the case of analog filters, there are many recipes (well-established algorithms, also called approximations) to obtain a system function H(s) that obeys the specified requirements in a project. The most common approximations are the Butterworth, Chebyshev, Bessel and Cauer (or elliptic).\n\n\n\nHaving H(s), the next step is the realization of the filter using a circuit, i. e., establishing the topology and the components of the circuit that (approximately) implement H(s). These two steps are the subject of many textbooks on filter design. Here, the approach will be to simply provide an example using the FilterPro software from Texas Instruments [url3tif]. This software allows to obtain the schematic and components.1\n\n\n\nAnalog filters that use only resistor, capacitors and inductors are called passive filters. An alternative to avoid using inductors2 are active filters, which are typically based on opamps. When the target is an IC, not a circuit with discrete components, inductors are avoided due to the difficulty of their integration using current microelectronics technology. When an analog filtering operation is required in an IC, an active filter is typically adopted and this is the only class of filters supported by FilterPro.\n\n\n\n Example 3.2. Example of analog filter designed with FilterPro. The following example illustrates the design of an analog filter with a desired cutoff frequency fc = 1 kHz, which is assumed to define the passband, such that the passband frequency is fp = fc. A second-order\nlowpass filter with passband frequency of approximately 977.2 Hz was obtained with FilterPro and its schematic3 is shown in Figure 3.5.\n\n\n\n\n\n\n\n\nFigure 3.5: Lowpass active analog filter designed with Texas Instruments’ FilterPro software.\n\n\n\n\n\nThe system function H(s) can be obtained by analyzing the circuit in Figure 3.5. After that, H(s) can be compared to the ideal one obtained e. g. with Matlab/Octave via the command: [Bs,As]=butter(2,2pi1000,’s’) that gives\n\n\n\n\n\n\n\n\n\n H(s) = B(s) A(s) = 3.9478 × 107 s2 + 8.8858 × 103s + 3.9478 × 107. \n\n\n(3.1)\n\n\n\n\n\n\nThe arguments of the function butter are the filter order and the cutoff frequency in rad/s (not Hz). Note the required ’s’ to indicate to Matlab/Octave that the goal is to design an analog, not a digital filter.   □\n\n\n\n Example 3.3. Further examples of analog filter design in Matlab. Listing 3.1 illustrates how to design some analog filters with a given specification for its magnitude using Matlab (this code is not compatible with Octave). The frequency response magnitude and phase of the four designed filters are depicted in Figure 3.6.\n\n\n\nListing 3.1: MatlabBookFigures/figs_systems_elliptic.m\n\n\nApass=5; %maximum ripple at passband \nAstop=80; %minimum attenuation at stopband \n%% Lowpass Elliptic %%%%%%%%% \nFp=100; Wp=2*pi*Fp; %passband frequency \n5Fr=120; Wr=2*pi*Fr; %stopband frequency \n[N, Wp] = ellipord(Wp, Wr, Apass, Astop, 's') %find order \n[z,p,k]=ellip(N,Apass,Astop,Wp,'s'); %design filter \nB=k*poly(z); A=poly(p); %convert zero-poles to transfer function \n[H,w]=freqs(B,A); %calculate frequency response \n10%% Higpass Elliptic %%%%%%%%% \nFr=100; Wr=2*pi*Fr; %stopband frequency \nFp=120; Wp=2*pi*Fp; %passband frequency \n[N, Wp] = ellipord(Wp, Wr, Apass, Astop, 's') %find order \n[z,p,k]=ellip(N,Apass,Astop,Wp,'high','s'); %design filter \n15B=k*poly(z); A=poly(p); %convert zero-pole to transfer function \n[H,w]=freqs(B,A); %calculate frequency response \n%% Bandpass Elliptic %%%%%%%% \nFr1=10; Wr1=2*pi*Fr1; %first stopband frequency \nFp1=20; Wp1=2*pi*Fp1; %first passband frequency \n20Fp2=120; Wp2=2*pi*Fp2; %second passband frequency \nFr2=140; Wr2=2*pi*Fr2; %second stopband frequency \n[N,Wp]=ellipord([Wp1 Wp2],[Wr1 Wr2],Apass,Astop,'s') %find order \n[z,p,k]=ellip(N,Apass,Astop,Wp,'s');%design filter \nB=k*poly(z); A=poly(p); %convert zero-pole to transfer function \n25[H,w]=freqs(B,A); %calculate frequency response \n%% Bandpass Butterworth %%%%%%%%%%% \n[N, Wn]=buttord([Wp1 Wp2],[Wr1 Wr2],Apass,Astop,'s')%find order \n[z,p,k]=butter(N,Wn,'s'); %design Butterworth filter \nB=k*poly(z); A=poly(p); %convert zero-pole to transfer function \n30[H,w]=freqs(B,A); %calculate frequency response\n  \n\n\n\n\n\n\n\n\n\n\n(a) Lowpass elliptic (N = 8).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Highpass elliptic (N = 8).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Bandpass elliptic (N = 16).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d) Bandpass Butterworth (N = 86).\n\n\n\n\n\n\n\nFigure 3.6: Frequency response of filters designed in Listing 3.1. The magnitude specification masks are indicated. Note that the phase was unwrapped with the command unwrap for better visualization.\n\n\n\n\n\nIt should be noted from Listing 3.1 and Figure 3.6 that a bandpass filter has twice the order that is specified as an input parameter (and obtained by buttord and ellipord in this example). Note also that, in practice, an analog filter typically has an order between 1 to 10. For example, an order N = 86 is not feasible for practical implementation with an analog circuit.    □\n\n\nExample of digital filter design\n\n\n\n\n\n\n\n\nFigure 3.7: The canonical interface of a digital filter H(z) with the analog world via A/D and D/A processes. The analog A(s) and R(s) are the anti-aliasing and external reconstruction filters, respectively.\n\n\n\n\n\nFor comparison, it is interesting to design now an equivalent digital filter H(z) to substitute its analog counterpart corresponding to H(s). But in order to interface H(z) with the external (analog) world, four extra blocks are required, as illustrated in Figure 3.7. The filters A(s) (anti-aliasing) and R(s) (external reconstruction) are lowpass analog filters with passband frequency equal to half of the sampling frequency, which is assumed here to be Fs = 10 kHz. Their gains are not important at this moment and in practice they are determined by the interfacing electronics. Assuming the converters and analog filters are properly defined, it remains to design H(z), which can be done with the command\n\n\n[Bz,Az]=butter(2,(1000/5000))\n\n\nthat leads to\n\n\n\n\n\n\n\n\n\n H(z) = B(z) A(z) = 0.067455 + 0.134911z−1 + 0.067455z−2 1.0 − 1.14298z−1 + 0.41280z−2 . \n\n\n(3.2)\n\n\n\n\n\n\nIt is important to note that when designing an analog filter, the second argument of butter is ω in rad/s. For a digital filter, as explained in Section 1.7.4, Matlab/Octave uses the normalized frequency fN = Ω∕π. In this case, the Nyquist frequency Fs∕2 is 5 kHz and using Eq. (1.25), f = 1000 Hz is normalized by the Nyquist frequency in the command [Bz,Az]=butter(2,(1000/5000)).\n\n\n\nIt will be later discussed in this chapter that the function butter used the bilinear transform to convert H(s) of Eq. (3.1) into H(z) of Eq. (3.2).\n\n\n\nFrom Table 3.1 one knows the definition\n\nH(z) = Y (z)∕X(z) and this should not be confused with H(z) = B(z)∕A(z) in Eq. (3.2). In many applications H(z) is a rational function (a ratio of two polynomials), with B(z) specifying the numerator and A(z) the denominator. In general, B(z)≠Y (z) and A(z)≠X(z).\n\n\n\nDigital filters described by a rational function H(z) can be conveniently implemented via a LCCDE, as discussed next.\n\n\n\n Example 3.4. Obtaining the difference equation and implementing a digital filter. For example, assume that a LTI system with impulse response h[n] = 0.2nu[n] outputs y[n] = δ[n] − 0.3δ[n − 1] when the input is x[n] = δ[n] − 0.5δ[n − 1] + 0.06δ[n − 2]. In this case, X(z) = 1 − 0.5z−1 + 0.06z−2 and Y (z) = 1 − 0.3z−1, which leads to H(z) = 1∕(1 − 0.2z−1) with B(z) = 1 and A(z) = 1 − 0.2z−1. With this concept in mind, Eq. (3.2) can be written as\n\n\n\n   X(z)(0.067455 + 0.134911z−1 + 0.067455z−2)     = Y (z)(1.0 − 1.14298z−1 + 0.41280z−2)    \n\nand taking the Z-inverse one can obtain the difference equation corresponding to Eq. (3.2):\n\n\n\n y[n]  = 0.067455x[n] + 0.134911x[n − 1] + 0.067455x[n − 2]     +1.14298y[n − 1] − 0.41280y[n − 2].    \n\nThe amazing fact is that this can be implemented with a simple code, similar to Listing 3.2.\n\n\n\n \n\n\nListing 3.2: Digital filter implementation in pseudo-code.\n\n\ninitialization() { %all previous values are assumed 0 \n    xnm1=0 %variable to store the value of x[n-1] \n    xnm2=0 %x[n-2] \n    ynm1=0 %y[n-1] \n5    ynm2=0 %y[n-2] \n} \nprocessSample() { \n    xn=readFromADConverter() %read input sample from ADC \n    yn=0.067455*xn + 0.134911*xnm1 + 0.067455*xnm2 + \n10       1.14298 ynm1 - 0.41280 *ynm2 \n    writeToDAConverter(yn) %write output sample into DAC \n    ynm2=ynm1 %update for next iteration. Note the order of ... \n    ynm1=yn   %updates: avoid overwriting a value ... \n    xnm2=xnm1 %that should be used later \n15    xnm1=xn \n}\n\n\nwhere one assumes a timer4 invokes the function processSample at a rate of Fs (10 kHz in this case).\n\n\n\n\n\n\n\n\nFigure 3.8: Comparison of two analog filters with its digital counterpart of Eq. (3.2). The “ideal” analog corresponds to Eq. (3.1) while the “10% tolerance” corresponds to its realization using the schematic of Figure 3.5.\n\n\n\n\n\nFigure 3.8 compares the discussed filters, illustrating that H(z) has a lowpass frequency response that is similar in terms of passband frequency to the original H(s). For frequencies above the passband, H(z) attenuates more than H(s), but this is not deleterious for a lowpass filter. Later, this behavior will be clarified, which is intrinsic of having the bilinear transform mapping the behavior of the analog filter at infinite frequencies into Ω = π rad (or Fs∕2 Hz) for H(z), i. e., mapping H(ω)|ω→∞ into H(ejΩ)|Ω=π.\n\n\n\nIf Fs is changed, the frequency response of the overall system in Figure 3.7 is scaled according to ω = ΩFs (Eq. (1.22)). For example, decreasing Fs from 10 to 5 kHz would divide by two the cutoff frequency of the overall system.    □\n\n\n\nThe goal of this section was to provide an overview of filtering. Appendix B.27 presents a brief review of the most important properties of systems and the next section discusses LTI systems.\n\n\n \n\n1 FilterPro has been deprecated in favor of WEBENCH at [url3twe].\n\n \n\n2 For the readers without background in electrical circuits but interested in the subject, some basic concepts can be found on the Web, e. g. [url3ele].\n\n \n\n3 The FilterPro options were a lowpass Butterworth filter with passband frequency of 1 kHz, with a fixed order of 2 and components with 10% of tolerance. When using the exact values of the calculated resistances and capacitances (two of 11.25 KΩ, one of 5.63 KΩ, 10 nF and 40 nF), the cutoff frequency is exactly 1 kHz, but this is typically not feasible in practice, i. e., commercial components are available only within a finite set of values and have non-zero tolerance.\n\n \n\n4 In practice a hardware interrupt is periodically generated according to a timer. The code that implements the filter is part of the interrupt service routine (ISR).\n\n                                      &lt;/div&gt;"
  },
  {
    "objectID": "ak_dsp_bookse34.html",
    "href": "ak_dsp_bookse34.html",
    "title": "34  Linear Time-Invariant Systems",
    "section": "",
    "text": "3.4  Linear Time-Invariant Systems\n\n\n\nThis section is dedicated to the study of linear and time-invariant (LTI) systems.\n\n\n\n\n\n\n\n\nFigure 3.9: Diagram of systems, emphasizing the linear and time-invariant (LTI) systems and the systems described by linear, constant-coefficient differential (or difference) equations (LCCDE).\n\n\n\n\n\nFigure 3.9 depicts the position of LTI systems and also includes the important subset of systems described by linear, constant-coefficient differential (or difference, in discrete-time) equations (LCCDE). In this text, it is assumed that the LCCDE systems have zero initial conditions or, equivalently, that the system is at rest.\n\n\n3.4.1  Impulse response and convolution for LTI systems\n\n\n\nA LTI system is completely characterized by its impulse response.5 This fact is illustrated in the sequel using a simple example. Assume that a system H is LTI and its impulse response h[n] = 2δ[n] + 5δ[n − 1] is obtained by imposing an input x[n] = δ[n]. For example, assuming x[n] = 4δ[n] − 3δ[n − 1], one has\n\n\n\n y[n]  = H{x[n]} = H{4δ[n] − 3δ[n − 1]}     = 4H{δ[n]}− 3H{δ[n − 1]} (using linearity)   = 4h[n] − 3h[n − 1] (using time-invariance)   = 4(2δ[n] + 5δ[n − 1]) − 3(2δ[n − 1] + 5δ[n − 2])     = 8δ[n] + 14δ[n − 1]) − 15δ[n − 2].    \n\nThis example illustrates that, knowing h[n] and using linearity and time-invariance, one can calculate the output.\n\n\n\nIn general, the input/output relation of a LTI depends on two facts:\n\n\n\n\n\nAs indicated by Eq. (1.3), any signal can be decomposed as the sum of impulses αδ[n − n0] that are shifted in time (by n0) and scaled (by α)\n in amplitude.\n &lt;/p&gt;&lt;/li&gt;\n &lt;li class=\"itemize\"&gt;\n &lt;!--l. 291--&gt;&lt;p class=\"noindent\" &gt;By time-invariance, each impulse &lt;!--l. 291--&gt;&lt;math \nxmlns=“http://www.w3.org/1998/Math/MathML”\ndisplay=“inline” &gt;δ[n − n0] generates a sequence h[n − n0] at the output and by linearity these sequences αh[n − n0] can be scaled and summed to composed the output y[n].\n\n\n\n\n\nThese two key facts lead to the convolution operation:\n\n\n\n y[n]  = H{x[n]} = H{∑ k=−∞∞x[k]δ[n − k]}     = ∑ k=−∞∞x[k]H{δ[n − k]} (using linearity)   = ∑ k=−∞∞x[k]h[n − k]        (using time-invariance) (3.3)  \n\nNote how linearity and time-invariance were both invoked in this proof. The continuous-time convolution is similar:\n\n\n\n\n\n\n\n\n\n y(t) = ∫ −∞∞x(τ)h(t − τ)dτ. \n\n\n(3.4)\n\n\n\n\n\n\nThe convolution is so important that it is represented by the operator ∗. For example, y[n] = x[n]∗h[n] denotes the convolution of x[n] and h[n]. Because ∗\nis the same symbol used for multiplication in programming languages, the context has to distinguish them.\n\n\n\n\n\n\n\n\nFigure 3.10: Example of convolution between x[n] = 2δ[n] − 3δ[n − 1] and h[n] = δ[n] − 2δ[n − 1] + δ[n − 2]. The top-left and top-right plots are x[n] and y[n], respectively, while the other plots are the parcels of x[n] (left) and the corresponding parcels of y[n] (right).\n\n\n\n\n\nFigure 3.10 illustrates the convolution between two sequences. The interpretation is that y[n] is composed by the sum of several scaled and shifted impulse responses. The plots at the right in Figure 3.10 indicate that y[n] is the sum of the two parcels 2h[n] and  − 3h[n − 1]. The Listing 3.3 provides an example of implementing convolution of discrete-time sequences,6 which gives the same result as the (faster) conv function in Matlab/Octave.\n\n\n\nListing 3.3: MatlabOctaveFunctions/ak_convolution.m\n\n\nfunction y=ak_convolution(x,h) \n% function y=ak_convolution(x,h) \n%convolution between sequences x and h \nN1=length(x); %get the number of samples in x \n5N2=length(h); \nN=N1+N2-1; %this is the number of samples in the output y \ny=zeros(1,N); %pre-allocate space for y[n] \nfor i=1:N1 %calculate y[n]= sum_k x[k] h[n-k] \n    y(i:i+N2-1)=y(i:i+N2-1)+x(i)*h; %scaling h by x(i) \n10end\n  \n\n\nNote that if the samples of x[n] and h[n] are organized as elements of vectors of polynomial coefficients, convolving them is equivalent to multiplying the two polynomials. For the example in Figure 3.10 one can obtain the convolution result by multiplying the equivalent polynomials 2x − 3 and x2 − 2x + 1, which leads to 2x3 − 7x2 + 8x − 3, i. e., the coefficients are the samples of y[n].\n\n\n\nIf the impulse response completely characterizes a LTI system, all its properties can be inferred from the corresponding impulse response. This is discussed in Appendix B.27.7.\n\n\n3.4.2  Convolution properties\n\n\n\nThe convolution is:\n\n\n\n\n\nCommutative: a∗b = b∗a\n\n\n\n\n\nAssociative: a∗(b∗c) = (a∗b)∗c\n\n\n\n\n\nDistributive: a∗(b + c) = a∗b + a∗c\n\n\n\n\n\nSome other facts about the convolution:\n\n\n\n\n\nIf N1 and N2 are the duration in samples of two finite-length discrete-time sequences a and b, then a∗b has duration N1 + N2 − 1 samples\n\n\n\n\n\nThe expression Eq. (1.57) can also be written as RX(τ) = x(τ)∗x∗(−τ)\n\n\n\n\n\nIf a[n] = b[n]∗c[n], to obtain a[na] for a specific instant na, one sums all products b[nb]c[nc] for na = nb + nc\n\n\n\n\n\nAn example of this last fact is appropriate: if x1[n] = δ[n] + 2δ[n − 1] + 3δ[n − 2] and x2[n] = 5δ[n] + 6δ[n − 1] + 7δ[n − 2] + 8δ[n − 3], the output y[n] = x1[n]∗x2[n] is given by\n\ny[0] = x1[0] x2[0] = 5\ny[1] = x1[0] x2[1] + x1[1] x2[0] = 16\ny[2] = x1[0] x2[2] + x1[1] x2[1] + x1[2] x2[0] = 34\ny[3] = x1[0] x2[3] + x1[1] x2[2] + x1[2] x2[1] = 40\ny[4] = x1[1] x2[3] + x1[2] x2[2] = 37\ny[5] = x1[2] x2[3] = 24\n\n\n\nwhere one can note that the indexes of the input sequences sum up to the index of the output. Hence, one can alternatively implement convolution as in Listing 3.4.\n\n\n\nListing 3.4: MatlabOctaveFunctions/ak_convolution2.m\n\n\nfunction [y,n]=ak_convolution2(x1,x2,n1,n2) \n% function [y,n]=ak_convolution2(x1,x2,n1,n2) \n%calculate the convolution between the sequences x1 and x2 \n%that start at index n1 and n2, respectively \n5%Example: [y,n]=ak_convolution2(1:3,5:8,-3,2); stem(n,y) \nN1=length(x1); %get the number of samples in x1 \nN2=length(x2); %get the number of samples in x2 \nN=N1+N2-1; %this is the number of samples in the output y \ny=zeros(1,N); %pre-allocate space for y[n] \n10for i=0:N1-1 %calculate y[n] = sum_k x1[k] x2[n-k] \n    for j=0:N2-1 \n        y(i+j+1) = y(i+j+1) + x1(i+1)*x2(j+1); %update \n    end \nend \n15n=n1+n2:n1+n2+N-1; %generate the \"time\" indices\n  \n\n\nNote that Listing 3.4 allows to explicitly deal with sequences that do not start at n = 0. When using Matlab/Octave, it is often necessary to explicitly deal with and generate the time indexes.\n\n\n\nIt can be observed from Listing 3.4 that the first non-zero sample of the convolution between two sequences x1[n] and x2[n] is located at n = n1 + n2, which are assumed to be the indexes of the first non-zero samples of the two sequences. Similarly, the last non-zero sample of the convolution is the sum of the indexes of the last non-zero samples of x1[n] and x2[n]. This can be seen by running Listing 3.5.\n\n\n\nListing 3.5: MatlabOctaveCodeSnippets/snip_systems_ak_convolution.m\n\n\nx1=1:3; %define sequence x1 \nx2=5:8; %define sequence x2 \nnx1=-3:-1; %define abscissa for x1 \nnx2=2:5; %define abscissa for x2 \n5subplot(311); stem(nx1,x1) \nsubplot(312); stem(nx2,x2) \n[y,n]=ak_convolution2(x1,x2,-3,2); %calculate convolution \nsubplot(313); stem(n,y) %show result with proper time axis\n  \n\n\n\n\n3.4.3  Convolution via correlation and vice-versa\n\n\n\nGiven that convolution and cross-correlation are tightly related, Listing 3.6 illustrates how the Matlab/Octave functions xcorr and conv can be used to calculate the other operation.\n\n\n\nListing 3.6: MatlabOctaveCodeSnippets/snip_systems_convolution_correlation.m\n\n\nx=(1:4)+j*(4:-1:1); %define some complex signals as row vectors, such \ny=rand(1,15)+j*rand(1,15); %that fliplr inverts the ordering \n%% Correlation via convolution \nRref=xcorr(x,y); %reference of a cross-correlation \n5xcorrViaConv=conv(x,conj(fliplr(y))); %use the second argument \n%% Convolution via correlation \nCref=conv(x,y); %reference of a convolution \nconvViaXcorr=xcorr(x,conj(fliplr(y))); %using the second argument \n%convViaXcorr=conj(fliplr(xcorr(conj(fliplr(x)),y))); %alternative \n10%% Make sure they have the same length and compare the results \nif length(x) ~= length(y) %this case requires post-processing because \n    %xcorr assumes the sequences have the same length and uses \n    %zero-padding if they do not. We treat the effect of these zeros: \n    convolutionLength=length(x)+length(y)-1; \n15    correlationLength=2*max(length(x),length(y))-1; \n    if length(x) &lt; length(y) %zeros at the end \n        convViaXcorr = convViaXcorr(1:convolutionLength); \n        xcorrViaConv = [xcorrViaConv zeros(1,correlationLength- ... \n            length(xcorrViaConv))]; \n20    elseif length(x) &gt; length(y) %zeros at the beginning \n        convViaXcorr = convViaXcorr(end-convolutionLength+1:end); \n        xcorrViaConv = [zeros(1,correlationLength- ... \n            length(xcorrViaConv)) xcorrViaConv]; \n    end \n25end \nErroXcorr= max(abs(Rref - xcorrViaConv)) %calculate maximum errors \nErroConv = max(abs(Cref - convViaXcorr)) %should be small numbers\n  \n\n\nAs indicated in Listing 3.6, the key step to mimic a convolution via correlation or vice-versa is to flip the signal using x(−t), use its complex conjugate and eventually shift by t0 as indicated in\n\n\n\n\n\n\n\n\n\n x^(t) = x∗(−t + t 0). \n\n\n(3.5)\n\n\n\n\n\n\n\n\n3.4.4  Discrete-time convolution in matrix notation\n\n\n\nConvolution can be denoted in matrix notation. This is especially convenient when dealing with finite-duration discrete-time signals. When these signals are represented by column vectors x1 and x2, a convolution matrix H1 allows to obtain the convolution result y = x1∗x2 as\n\n\n\n\n\n\n\n\n\n y = H1x2, \n\n\n(3.6)\n\n\n\n\n\n\nwhere H1 is composed by the elements of x1. It is also possible to create H2 from the elements of x2 and use y = H2x1.\n\n\n\nThis can be better understood with examples. Considering the goal is to obtain the convolution of two column vectors x1=[1; 2; 3] and x2=[5; 6; 7; 8] (previous example), instead of using y=conv(x1,x2), one can create a matrix\n\nhmatrix = [\n     1     0     0     0\n     2     1     0     0\n     3     2     1     0\n     0     3     2     1\n     0     0     3     2\n     0     0     0     3]\n\n\n\nwith the command hmatrix = convmtx(x1,length(x2)), and then calculate the convolution with y=hmatrix * x2. Note that when using row vectors, the commands could be hm = convmtx(x1,length(x2)); x2*hm (in this case, hm would be the transpose of hmatrix). Application 3.11 discusses the issue of repeatedly processing blocks of samples using the convolution in matrix notation. The matrices created by convmtx are Toeplitz matrices and the command toeplitz.m can also be useful to compose convolution matrices.\n\n\n\n\n\n3.4.5  Approximating continuous-time via discrete-time convolution\n\n\n\nSome situations require approximating the continuous-time convolution denoted by Eq. (3.4) via the discrete-time convolution in Eq. (3.3). In such cases, the sampling interval Ts should be used as a normalization factor, as indicated in Eq. (B.31). This factor is adopted in the next example.\n\n\n\n Example 3.5. Convolution of a pulse with itself leads to a triangular waveform. Assume a pulse p(t) = 4rect(5t − 0.1) (see rect(⋅) in Section 1.3.6) with amplitude A = 4 V and support T0 = 0.2 s. The convolution p(t)∗p(t) corresponds to a triangular waveform: it is 0 for t &lt; 0 and assumes its maximum value at A2T0 = 42 × 0.2 = 3.2 at 0.2 s, which corresponds to the time that the two pulses overlap completely and A2T0 is the area of p2(t). Figure 3.11 depicts both the pulse and continuous-time convolution p(t)∗p(t).\n\n\n\n\n\n\n\n\nFigure 3.11: Convolution of a pulse p(t) = 4rect(5t − 0.1) with itself, obtained with Listing 3.7.\n\n\n\n\n\nBut Figure 3.11 was in fact obtained by representing p(t) via its discrete-time version p[n], and approximating the convolution p(t)∗p(t) with the scaled discrete-time convolution Ts(p[n]∗p[n]), i. e., Eq. (B.31). Listing 3.7 shows the first lines of the script that generated Figure 3.11. After running Listing 3.7, the vectors pulse and triangle are shown in Figure 3.11 with the proper time axes.\n\n\n\nListing 3.7: MatlabOctaveCodeSnippets/snip_systems_continuous_discrete_conv\n\n\nT0=0.2; %pulse \"duty cycle\" (interval with non-zero amplitude): 0.2 s \nTs=2e-3; %sampling interval: 2 ms \nN=T0/Ts; %number of samples to represent the pulse \"duty cycle\" \nA=4; %pulse amplitude: 4 Volts \n5pulse=A*[zeros(1,N) ones(1,N) zeros(1,4*N)]; %pulse \ntriangle=Ts*conv(pulse,pulse); %(approximated) continuous convolution \ndisp(['Convolution peak at ' num2str(T0) ' is ' num2str(A^2*T0)])\n  \n\n\nNote in line 6 the scaling factor Ts, which leads to the correct values, as indicated by the datatip in Figure 3.11.    □\n\n\n3.4.6  Frequency response: Fourier transform of the impulse response\n\n\n\nAs indicated in Table 3.1, the frequency response of an LTI system is the Fourier transform of its impulse response. In continuous-time it will be denoted by H(f) (f in Hz) or H(ω) (ω in rad/s), while in discrete-time the notation is H(ejΩ) (Ω in rad). The frequency response is particularly useful because complex exponentials are eigenfunctions of LTI systems as explained in the sequel.\n\n\n\n\n\nEigenfunctions\n\n\n\nEigenfunctions are closely related to eigenvectors. if A is a matrix (a linear transformation), a non-null vector x is an eigenvector\nof A if there is a scalar λ such that\n\n\n\n\n\n\n Ax = λx. \n\n\n\n\n\n\nThe scalar λ is an eigenvalue of A corresponding to the eigenvector x. In general, the matrix A generates a completely new vector y = Ax, i. e., y and x have different directions and magnitudes. However, if x is an eigenvector of A, then y = λx, which means that the operation changed only the magnitude of x, leaving its direction unchanged (or possibly reversing it in case λ &lt; 0).\n\n\n\nSimilarly, for any LTI system H (continuous-time is assumed here), a complex exponential is an eigenfunction because the output is y(t) = λejω0t when the input is x(t) = ejω0t. The frequency response is useful because the eigenvalue λ = H(ω0) is the value of the frequency response H(ω) at the specific frequency ω = ω0. This implies that an LTI never creates new frequencies, because it can simply change the magnitude and phase of frequencies that were presented at its input. This fact can be represented as\n\n\n\n\n\n\n ejω0t→&lt;mstyle  mathvariant=“script”&gt;H →H(ω 0)ejω0t. \n\n\n\n\n\n\nIt is then possible to analyze an LTI in the frequency domain by following the steps:\n\n\n\nf 1.\n\n\nDecompose the input as a sum (if the signal is periodic) or integral of complex exponentials. For that, one can use the Fourier series or transform X(ω) = F{x(t)}\n\n\nf 2.\n\n\nObtain the frequency response, which provides the eigenvalues for all frequencies: H(ω) = F{h(t)}\n\n\nf 3.\n\n\nConceptually, multiply each complex exponential ejω0t by its eigenvalue H(ω0) and add the partial results to obtain the system output y(t). In practice, this step is performed by Y (ω) = H(ω)X(ω) and then using the inverse transform to obtain y(t) = F−1{Y (ω)}.\n\n\n\n\n\n\n\n\n\nFigure 3.12: Frequency response of H(ω) = 1 jω+2 represented in polar form: magnitude (top) and phase (bottom). The data tips indicate the values for ω = ±4 rad/s.\n\n\n\n\n\n Example 3.6. LTI filtering of continuous-time complex exponential. For example, consider that x(t) = 6ej4t + 6e−j4t = 12cos ⁡ (4t) is the input of an LTI with h(t) = e−2tu(t). The eigenvalues corresponding to each the complex exponential eigenfunctions with frequencies ω = ±4 rad/s can be obtained from the frequency response H(ω) = F{h(t)} = 1 jω+2. The Matlab/Octave commands Omega=4; H=1/(j*Omega+2) calculate H(ω)|ω=4 = 0.1 − 0.2j for the positive frequency. Because h(t) is real-valued, the frequency response presents Hermitian symmetry and H(ω)|ω=−4 = H∗(ω)|ω=4, i. e., H(ω)|ω=−4 = 0.1 + 0.2j. The commands abs(H), angle(H) can transform from Cartesian to polar: H(ω)|ω=4 ≈ 0.2236e−j1.107 and H(ω)|ω=−4 ≈ 0.2236ej1.107. Therefore, the output is y(t) = (0.2236e−j1.107)6ej4t+(0.2236ej1.107)6e−j4t = 1.32ej(4t−1.11)+1.32ej(−4t+1.11) = 2.64cos ⁡ (4t−1.11).\n\n\n\nThe effect of the LTI system was to impose a gain of |H(4)| = 0.2236 and a phase of ∠H(4) = −1.107 rad. Figure 3.12 depicts the frequency response for a range ω = 2π[−3,3] rad/s. Because of the Hermitian symmetry (when h(t) is real), it is common to plot only the positive frequencies. For example, investigate the command freqs(1,[1 2]), which shows the frequency response of H(ω) = 1 jω+2 in a different format.   □\n\n\n\nThe previous discussion was restricted to exponentials of the form ejω0t, but a general complex exponential est (with σ≠0) is also an eigenfunction of any LTI and the eigenvalue is given by the Laplace transform H(s), as represented by\n\n\n\n\n\n\n es0t→&lt;mstyle  mathvariant=“script”&gt;H →H(s 0)es0t. \n\n\n\n\n\n\nThe frequency response H(ejΩ) of discrete-time LTI systems is also very useful. When the time axis t is discretized by sampling, i. e., t = nTs, the complex exponential est becomes esnTs, which is more conveniently represented by zn, where\n\n\n\n\n\n\n z = esTs  \n\n\n\n\n\n\nwith z,s ∈ ℂ.\n\n\n\nThe discrete-time complex exponential zn is an eigenfunction of discrete-time LTI systems. To prove that, consider x[n] = (z0)n,z0 ∈ ℂ, then\n\n\n\n y[n]  = ∑ k=−∞∞(z 0)kh[n − k] = ∑ k=−∞∞h[k](z 0)n−k = (z 0)n ∑ k=−∞∞h[k](z 0)−k     = (z0)nH(z)| z=z0.   \n\n\nThe eigenvalue H(z)|z=z0 can be obtained from the transfer function\n\n\n\n\n\n\n H(z) = ∑ n=−∞∞h[n]z−n, \n\n\n\n\n\n\nwhich is the Z-transform of the system’s impulse response. As a special case, when |z| = 1, the eigenfunction ejΩn has its amplitude and phase eventually modified by a LTI as depicted by:\n\n\n\n\n\n\n ejΩ0n→&lt;mstyle  mathvariant=“script”&gt;H →H(ejΩ0 )ejΩ0n. \n\n\n\n\n\n\n Example 3.7. LTI filtering of discrete-time complex exponentials. This example describes how an LTI system filters a complex exponential. Consider that x[n] = 4ej0.5πn + 4e−j0.5πn = 8cos ⁡ (0.5πn) is the input of an LTI with h[n] = 0.7nu[n]. The eigenvalues can be obtained from H(ejΩ) = F{h[n]} = 1 1−0.7e−jΩ when Ω = 0.5π rad. The Matlab/Octave commands omega=pi/2; H=1/(1-0.7exp(-jomega)) calculate H(ejΩ)|Ω=0.5π = 0.6711 − 0.4698j. Because h[n] is real-valued, the frequency response presents Hermitian symmetry. The commands abs(H), angle(H) can transform from Cartesian to polar: H(ejΩ)|Ω=0.5π ≈ 0.8192e−j0.6107. Therefore, the output is y[n] = (0.8192e−j0.6107)4ej0.5πn+(0.8192ej0.6107)4e−j0.5πn = 3.2769ej(0.5πn−0.6107)+3.2769ej(−0.5πn+0.6107) = 6.5539cos ⁡ (0.5πn−0.6107).\nThe effect of the LTI system was to impose a gain of 0.8192 and a phase of  − 0.6107 rad.\n\n\n\n\n\n\n\n\nFigure 3.13: Frequency response of H(ejΩ) = 1 1−0.7e−jΩ represented in polar form: magnitude (top) and phase (bottom). The data tips indicate the values for Ω = π∕2 rad.\n\n\n\n\n\n\n\n\n\n\nFigure 3.14: Version of Figure 3.13 obtained with the command freqz(1,[1 -0.7]).\n\n\n\n\n\nFigure 3.13 depicts the frequency response for a range Ω = [−15,15] rad. As expected, H(ejΩ) is periodic because H(ejΩ) = H(ej(Ω+2π)). Given this periodicity and the Hermitian symmetry (when h[n] is real), it is common to plot only the positive frequencies in the range Ω = [0,π]. For example, Figure 3.14 shows the plots obtained with the command freqz(1,[1 -0.7]), which shows the frequency response of H(ejΩ) with the magnitude in dB, the phase in degrees (instead of rad) and the abscissa in a “normalized frequency” Ω∕π that maps [0,π] into [0,1].    □\n\n\n3.4.7  Fourier convolution property\n\n\n\nThe convolution property of Fourier transforms states that the convolution between two signals has a Fourier transform given by the multiplication of their Fourier transforms. In other words, the convolution between these signals by can be obtained as the inverse Fourier transform of the multiplication of their Fourier transforms. For example, assuming continuous-time, the convolution y(t) = x1(t)∗x2(t) can be obtained with\n\n\n\n\n\n\n\n\n\n y(t) = F−1{X 1(f)X2(f)}, \n\n\n(3.7)\n\n\n\n\n\n\nwhere X1(f) and X2(f) are the corresponding Fourier transforms. Similarly, in discrete-time, the convolution result y[n] can be\nwritten as\n\n\n\n\n\n\n\n\n\n y[n] = x1[n]∗x2[n] = DTFT−1{X 1(ejΩ)X 2(ejΩ)}, \n\n\n(3.8)\n\n\n\n\n\n\nwhere X1(ejΩ) and X2(ejΩ) are the corresponding DTFTs and, in this case, F−1 denotes the inverse DTFT.\n\n\n\n Example 3.8. Convolution using DTFTs and inverse DTFT. For example, if x1[n] = α1nu[n] and x2[n] = α2nu[n] are two discrete-time complex exponentials with |α1| &lt; 1 and |α2| &lt; 1, their DTFTs are Xi(ejΩ) = 1 − αie−jΩ,i = 1,2, respectively. The convolution result is\n\n\n\n\n\n\n\n\n\n y[n] = x1[n]∗x2[n] = 1 α1 − α2  (α1n+1u[n] − α 2n+1u[n]) \n\n\n(3.9)\n\n\n\n\n\n\nand can be obtained via the inverse DTFT of Y (ejΩ), which is given by\n\n\n\n\n\n\n\n\n\n Y (ejΩ) = X 1(ejΩ)X 2(ejΩ) = 1 (1 − α1e−jΩ)(1 − α2e−jΩ). \n\n\n(3.10)\n\n\n\n\n\n\nEq. (3.9) can be obtained by using partial fraction expansion of Eq. (3.10).    □\n\n\n\nDue to the duality of Fourier transforms, the multiplication in time-domain corresponds to convolution in frequency-domain between the respective spectra. This is called the multiplication property and corresponds to\n\n\n\n\n\n\n\n\n\n x1(t)x2(t) ⇔ X1(f)∗X2(f) = ∫ −∞∞X 1(p)X2(f − p)dp. \n\n\n(3.11)\n\n\n\n\n\n\nin continuous-time.\n\n\n\nIn discrete-time, one needs to take in account that the spectra are periodic and the conventional convolution is not adequate when both signals are periodic. Hence, the multiplication property for discrete-time signals is\n\n\n\n\n\n\n\n\n\n x1[n]x2[n] ⇔ 1 2π∫ &lt;2π&gt;X(ejΩ)X 2(ej(Ω−𝜃))d𝜃. \n\n\n(3.12)\n\n\n\n\n\n\nThe integral in Eq. (3.12) differs from a conventional convolution because it is calculated over only one period (2π rad in this case) and the result is normalized by this period. This modified convolution is denoted as ⊛ and is called periodic, cyclic or circular convolution. As in a Fourier series expansion to obtain coefficients ck, the associated period must be known in order to properly use ⊛. The normalization by the period can be incorporated in its definition, which leads to the following expression for the periodic convolution:\n\n\n\n\n\n\n\n\n\n x1(t)⊛x2(t) = 1 T∫ &lt;T&gt;x1(τ)x2(t − τ)dτ, \n\n\n(3.13)\n\n\n\n\n\n\nwhere both signals are periodic in T.\n\n\n\nThe definition of Eq. (3.13) allows to write Eq. (3.12) as\n\n\n\n\n\n\n\n\n\n x1[n]x2[n] ⇔ X(ejΩ)⊛X 2(ejΩ), \n\n\n(3.14)\n\n\n\n\n\n\nwhere the period 2π is implicit.\n\n\n\nThe periodic convolution corresponds to performing the conventional convolution (called “linear” convolution, in contrast to “circular”) between x1(t) and x ′2(t), where x ′2(t) is a single period of x2(t) that is normalized\nby the period T. The interval to define x ′2(t) can be conveniently chosen as x2′(t) = x2(t)∕T,0 ≤ t &lt; T, or 0 otherwise, or x2′(t) = x2(t)∕T,−T∕2 ≤ t &lt; T∕2, or 0 otherwise.\n\n\n\nAs discussed in the sequel, the periodic convolution is typically associated to FFT-based processing.\n\n\n\n\n\n3.4.8  Circular and fast convolutions using FFT\n\n\n\nRecalling that the FFT corresponds to sampling the DTFT, Eq. (3.8) suggests that FFTs can be used to efficiently compute a convolution. However, even if x[n] were not periodic, when its DTFT X(ejΩ) is sampled (in frequency domain) by the FFT, the FFT values X[k] are representing the spectrum of a periodic version of x[n]. Consequently, when FFTs substitute DTFTs in Eq. (3.8), the result is not the linear but the circular convolution represented as\n\n\n\n\n\n\n\n\n\n y[n] = x1[n]⊛x2[n] = FFT−1{X 1[k]X2[k]}, \n\n\n(3.15)\n\n\n\n\n\n\nwhere both FFTs must have the same length N, which plays the whole of the period of a circular convolution.\n\n\n\nListing 3.8 provides an example of using Eq. (3.15). Note how zero-padding is used to assure the element-wise multiplication fft(x,N).*fft(h,N) uses arrays with the same length. The result of Listing 3.8 confirms that, in general, linear and circular convolution differ.\n\n\n\nListing 3.8: MatlabOctaveCodeSnippets/snip_systems_circularConvolution.m\n\n\nx=[1 2 3 4]; h=[.9 .8]; %signals to be convolved \nshouldMakeEquivalent=0 %in general, linear and circular conv. differ \nif shouldMakeEquivalent==1 \n    N=length(x)+length(h)-1; %to force linear and circular coincide \n5else \n    N=max(length(x),length(h)); %required for FFT zero-padding \nend \nlinearConv=conv(x,h) %linear convolution \ncircularConv=ifft(fft(x,N).*fft(h,N)) %circular convolution, N=4 \n10%circularConv=cconv(x,h,N) %note that Matlab has the cconv function\n  \n\n\nIf the value of shouldMakeEquivalent is made equal to 1, Listing 3.8 returns the same results for both linear and circular convolutions. In fact, when the FFT length is at least the number of non-zero samples of the convolution output, the circular and linear convolution results coincide. This suggests that Eq. (3.15) can be used to calculate a linear convolution, provided that the FFT length is made long enough.\n\n\n\nObtaining a linear convolution via FFTs is trickier when one of the signals to be convolved has infinite duration. In this case, it is obviously not possible to find a large enough FFT length to use Eq. (3.15). However, if the other signal has finite duration, it is feasible and often used in practice to segment the long signal into blocks and calculate the linear convolution by sequentially calculating one FFT per block and properly combining the results. There are basically two alternatives to combine the intermediate results that are called overlap-add and overlap-save methods, and are roughly equivalent. Listing 3.9 illustrates the former.\n\n\n\nListing 3.9: MatlabOctaveCodeSnippets/snip_systems_overlapAdd.m\n\n\nx=1:1000; %infinite duration (or \"long\") input signal \nh=ones(1,3); %non-zero samples of finite-length impulse response \nNh=length(h); %number of impulse response non-zero samples \nNb=5; %block (segment) length \n5Nfft=2^nextpow2(Nh+Nb-1); %choose a power of 2 FFT size \nNx = length(x); %number of input samples \nH = fft(h,Nfft); %pre-compute impulse response DFT, with zero-padding \nbeginIndex = 1; %initialize index for first sample of current block \ny = zeros(1,Nh+Nx-1); %pre-allocate space for convolution output \n10while beginIndex &lt;= Nx %loop over all blocks \n    endSample = min(beginIndex+Nb-1,Nx);%last sample of current block \n    Xblock = fft(x(beginIndex:endSample),Nfft); %DFT of block \n    yblock = ifft(Xblock.*H,Nfft); %get circular convolution result \n    outputIndex  = min(beginIndex+Nfft-1,Nh+Nx-1); %auxiliary variab. \n15    y(beginIndex:outputIndex) = y(beginIndex:outputIndex) + ... \n        yblock(1:outputIndex-beginIndex+1); %add parcial result \n    beginIndex = beginIndex+Nb; %shift begin of block \nend \nstem(y-conv(x,h)) %compare the error with result from conv\n  \n\n\nA typical application of the overlap-add and overlap-save methods is to compute the output of a LTI system represented by a finite-duration impulse response (i. e., a FIR filter, as will be discussed in Section 3.13). Listing 3.9 provides an example where the impulse response has only three non-zero samples, and the long input signal is segmented in blocks of Nb=5 samples.\n\n\n \n\n5 As explained to me by a student: Say you meet a person that tells you he/she is a LTI system. You should then ask his/her impulse response. If you know it, just relax because you can perfectly calculate the person’s reaction (output) to any situation (input).\n\n \n\n6 On the Web one can find applets instructing about continuous and discrete convolution (see, e. g., [url3jhu]).\n\n                                                                                                                 &lt;/div&gt;"
  },
  {
    "objectID": "ak_dsp_bookse35.html",
    "href": "ak_dsp_bookse35.html",
    "title": "35  Sampling and Signal Reconstruction Revisited",
    "section": "",
    "text": "Having established the fundamentals of convolution and Fourier transforms, it is possible now to sketch a proof of the sampling theorem (Theorem 1) without difficulties.\n\n\n\n\n\n3.5.1  A proof sketch of the sampling theorem\n\n\n\nSampling a signal x(t) at each Ts seconds (periodic and uniform sampling) can be modeled as the multiplication of x(t) by a periodic function p(t) with fundamental period Ts. For example, p(t) can be a train of pulses with duty cycle T1. However, it is mathematically convenient7 to adopt an impulse train\n\n\n\n\n\n\n\n\n\n p(t) = ∑ k=−∞∞δ(t − kT s), \n\n\n(3.16)\n\n\n\n\n\n\nwhich allows to model the sampled signal as\n\n\n\n\n\n\n\n\n\n xs(t) = x(t)p(t) = ∑ k=−∞∞x(kT s)δ(t − kTs), \n\n\n(3.17)\n\n\n\n\n\n\nas anticipated in Eq. (1.17).\n\n\n\nFrom Eq. (3.17) and the Fourier convolution property discussed in Section 3.4.7, the Fourier transform of xs(t) is\n\n\n\n\n\n\n\n\n\n Xs(f) = X(f)∗P(f), \n\n\n(3.18)\n\n\n\n\n\n\nwhere X(f) = F{x(t)} and P(f) = F{p(t)} is another impulse train, but in frequency domain. As indicated by Eq. (B.60), the impulses in P(f) are spaced by Fs = 1∕Ts and have area Fs = 1∕Ts.\n\n\n\nThe convolution of X(f) with the impulses in P(f) creates infinite replicas of X(f) at frequencies values that are multiples of Fs as depicted in Figure 3.15. As well-discussed in textbooks, if Fs is not sufficiently large, these replicas will overlap and create aliasing. But in case Fs &gt; 2Fmax, all replicas are “perfect” copies of X(f) scaled by Ts. The original spectrum can then be recovered by keeping one replica and eliminating the others. This filtering procedure is assumed here to be done with an ideal lowpass filter with bandwidth BW = Fmax and gain Ts.\nThis ideal filter will then cancel the undesired replicas of Xs(f) and recover X(f) using precisely the scaling factor Ts.\n\n\n\n\n\n\n\n\nFigure 3.15: Spectrum Xs(f) of a sampled signal xs(t) obtained by the convolution between X(f) and P(f) as indicated in Eq. (3.18).\n\n\n\n\n\nIn summary, a lowpass signal x(t) with maximum frequency Fmax, can be perfectly reconstructed from its sampled version xs(t) if the sampling frequency obeys Fs &gt; 2Fmax (Theorem 1) and the reconstructed signal x^(t) = x(t) is obtained by passing xs(t) through an ideal lowpass filter with frequency response H(f) having gain Ts over the passband.\n\n\n\nThe spectrum X^(f) of x^(t) corresponds to X^(f) = Xs(f)H(f). The multiplication of Xs(f) by H(f) in frequency-domain corresponds to the convolution of xs(t) with the filter’s impulse response h(t) = F{H(f)}. From Eq. (B.54) and the duality property, h(t) = sinc(t∕Ts) in this case. Hence, this convolution is written as\n\n\n\n\n\n\n\n\n\n x^(t) = xs(t)∗h(t) = ∑ n=−∞∞x sa(nT s)sinc  ( t Ts − n), \n\n\n(3.19)\n\n\n\n\n\n\nwhich corresponds to the convolution of the scaled sinc h(t) with impulses8 of area xsa(nTs). The time shift by n in Eq. (3.19) positions the sincs at nTs, and was discussed in Example 1.5.\n\n\n\nEq. (3.19) represents the reconstruction of a band-limited signal by the sinc interpolation of its samples and is called the Whittaker-Shannon interpolation formula.\n\n\n\nHence, when the sampling theorem is obeyed, the whole chain is as follows (the signals are depicted with their associated power in parenthesis as in Block (1.49)):\n\n\n\n\n\n\n\n\n\n x(t)(Pc)→ sampling →xs(t)(Ps)→ S/D →x[n](Pd)→ D/S →xs(t)(Ps)→ h(t) →x^(t)(Pc) = x(t) \n\n\n(3.20)\n\n\n\n\n\n\nwhere h(t) = sinc(t∕Ts) is the discussed ideal filter.\n\n\n\nUsing the notation suggested by Block (3.20), the samples x(nTs) were converted to areas xsa(nTs), which are converted to x[n] = xsa(nTs). Hence, Eq. (3.19) can be conveniently rewritten as\n\n\n\n\n\n\n\n\n\n x^(t) = xs(t)∗h(t) = D/S{x[n]}∗h(t) = ∑ n=−∞∞x[n]sinc  ( t Ts − n). \n\n\n(3.21)\n\n\n\n\n\n3.5.2  Energy and power of a sampled signal\n\n\n\nThe squared of the continuous-time impulse δ2(t) is not defined. This creates a problem when one considers the energy or power of δ(t). When δ(t) is interpreted as a pulse p(t) with unit area and amplitude 1∕Δ, one\ncan argue that when lim ⁡  Δ→0 as in Eq. (B.120), the resulting area of the squared pulse p2(t) is 1∕Δ, which leads to δ2(t) = ∞. However, this would not be mathematically rigorous given that δ(t) is a distribution. Hence, the following route is taken here: instead of defining new transformations9 on the distribution δ(t), the instantaneous power of a sampled signal xs(t) is defined as the instantaneous power of its equivalent discrete-time signal x[n] obtained via a S/D conversion, normalized by the associated Ts, i. e.\n\n\n\n\n\n\n\n\n\n Ps=defPd Ts . \n\n\n(3.22)\n\n\n\n\n\n\nFor example, the pulse train of Eq. (3.16) has average power Ps = 1∕Ts because when converted to discrete-time its power is Pd = 1.\n\n\n\nThe same reasoning can be applied to sampled signals for which the independent variable is not t. The Fourier transform P(f) = {p(t)} of Eq. (3.16) has power 1∕Ts because its discrete-frequency version has power 1∕Ts2 and the normalizing factor is 1∕Ts in this case. Note that, with this definition of instantaneous power of a sampled signal, the power of the impulse trains p(t) and P(f) are the same, as expected from Eq. (B.52).\n\n\n\n\n\n3.5.3  Energy / power conservation after sampling and reconstruction\n\n\n\nWith the help of Eq. (3.21), it is possible to sketch a proof for Eq. (1.51), which is valid when the sampling theorem is obeyed.\n\n\n\nEq. (3.21) states that any band-limited signal x(t) can be represented by its samples x[n]. The interest here is to relate their respective power values Pc and Pd. Assuming x(t) is an energy signal and from Eq. (B.28), its energy Ec can be written as\n\n\n\n\n\n\n\n\n\n Ec = ∫ −∞∞|x(t)|2dt = ∫ −∞∞  |∑ n=−∞∞x[n]sinc(t∕T s − n)|2dt = T s ∑ n=−∞∞|x[n]|2 = T sEd, \n\n\n(3.23)\n\n\n\n\n\n\nwhere Ed is the energy of x[n]. A similar reasoning can be applied to power signals. Rewriting Eq. (1.41) with Δt = NTs leads to\n\n\n\n Pc  = lim ⁡  N→∞  [ 1 (2N + 1)Ts ∫ −NTsNTs  |∑ n=−NNx[n]sinc(t∕T s − n)|2dt]     = lim ⁡  N→∞  [ Ts (2N + 1)Ts ∑ n=−NN|x[n]|2]     = Pd. (3.24)  \n\nFrom Eq. (3.24) and Eq. (3.22), Block (3.20) can be simplified as\n\n\n\n\n\n\n\n\n\n x(t)(P)→ sampling →xs(t)(P∕Ts)→ S/D →x[n](P)→ D/S →xs(t)(P∕Ts)→ h(t) →x^(t)(P) = x(t), \n\n\n(3.25)\n\n\n\n\n\n\nwhere P = Pc = Pd.\n\n\n\n\n\n3.5.4  Sampling theorem uses a strict inequality\n\n\n\nSome authors state this theorem as Fs ≥ 2Fmax, but in this case Fmax would have to be interpreted as the frequency for which X(f) does not have a discrete frequency component δ(f − Fmax). The confusion often arises when textbooks pictorially represent X(f) with a triangle shape as in Figure 3.15 and, in this case, Fmax is the “maximum” but X(Fmax) = 0 such that Fs ≥ 2Fmax “works”. However, as the exercise of Eq. (1.19) suggests, it is not guaranteed to reconstruct a cosine of frequency fc if one takes its samples at rate Fs = 2fc.\n\n\n\nAnother source of confusion with respect to Fs ≥ 2Fmax or\n\nFs &gt; 2Fmax is that when processing a signal sampled at Fs with an FFT, the maximum frequency is the so-called Nyquist frequency Fs∕2 of Table 1.4. Taking that Fmax = Fs∕2, it seems reasonable to adopt Fs ≥ 2Fmax. Note however that the FFT bin corresponding to the Nyquist frequency is representing all signal components within its width Δf and that, unless X(f) has a discrete frequency component δ(f − Fmax) to create ambiguity as exemplified in Eq. (1.19), there is no major practical issue.\n\n\n\n\n\n3.5.5  Undersampling or passband sampling\n\n\n\nMost digital signal processing (DSP) systems are designed to combat aliasing but there are exceptions. In digital communications, it is common to use aliasing to lower the frequency of a signal x(t) in an operation known as undersampling or passband sampling. Among other conditions, x(t) has to be a passband signal with spectrum centered at (a relatively high) frequency fc, but with (a relatively small) bandwidth BW &lt; fc, such that Fs &gt; 2BW. In this case, even if Fs &lt; 2fmax, where the maximum frequency is fmax = fc + 0.5BW, x(t) can still be reconstructed from a replica of its spectrum that was shifted in frequency.\n\n\n\nFor example, consider a passband signal x(t) with spectrum X(f) with BW = 25 Hz and center frequency fc = 70 Hz, as depicted in Figure 3.16. This signal has fmax = fc + 0.5BW = 82.5 Hz and using the sampling theorem as applied to lowpass signals one would be compeled to use Fs &gt; 2 × 82.5 = 165 Hz. However, using Fs = 56 Hz, for example, one can still recover the signal.\n\n\n\n\n\n\n\n\nFigure 3.16: Passband signal with BW = 25 Hz and center frequency fc = 70 Hz.\n\n\n\n\n\nAccording to the classic model for the sampling operation, it corresponds to the convolution of X(f) with an impulse train P(f) such that Xd(f) = X(f)∗P(f) and P(f) has impulses separated by Fs = 56. Hence, the sampled signal has spectrum Xd(f) as depicted in Figure 3.17.\n\n\n\nAssuming x(t) is real and X(f) has Hermitian symmetry, any of the two “replicas” in Figure 3.16 could be used to reconstruct x(t). Similarly, any of the six (among the infinite) replicas that are shown in Figure 3.17 could be used to reconstruct x(t).\n\n\n\n\n\n\n\n\nFigure 3.17: Result of sampling X(f) in Figure 3.16 with Fs = 56 Hz, which places replicas at f = ±14,±42,±,70,±98,…, with only the first six shown in this figure.\n\n\n\n\n\nFor example, placing an ideal lowpass filter with cutoff frequency Fs∕2 would obtain a version of x(t) corresponding to a frequency downconversion of its spectrum X(f) from 70 to 14 Hz.\n\n\n\nThe theory about undersampling indicates the range of Fs that can be used in each situation and can be found in DSP textbooks.10\n\n\n3.5.6  Sampling a complex-valued signal\n\n\n\nThe sampling theorem (Theorem 1) assumed a real-valued signal, which consequently, allowed to assume the spectrum magnitude is even (symmetric). In the more general case of a complex-valued signal, the same principle of having spectrum replicas that cannot overlap is valid, but the “maximum positive frequency” is not enough to determine the minimum Fs.\n\n\n\nFigure 3.18 suggests an example where a complex-valued signal has spectrum X(f) with support from  − 300 to 100 Hz. A careless interpretation of the sampling theorem could lead to the erroneous conclusion that Fs &gt; 2 × 100 Hz suffices to avoid aliasing. But in this case Fs &gt; 400 Hz is required to avoid the overlap of spectrum replicas. Figure 3.18 adopts Fs = 450 Hz.\n\n\n\n\n\n\n\n\nFigure 3.18: Sampling with Fs = 450 Hz a complex-valued signal with non-symmetrical spectrum.\n\n\n\n\n\nStating the sampling theorem for complex-valued signals requires more elaborated definitions of bandwidth (this is discussed in Section 3.7.2.0). But Figure 3.18 (and Figure 3.15) indicate that an efficient strategy is to follow the basic principle of avoiding aliasing after convolving the original spectrum with the train of impulses spaced by Fs.\n\n\n3.5.7  Signal reconstruction and D/S conversion revisited\n\n\n\nSimilar to sampling, the D/S conversion can now be better understood. After that, the important topic of signal reconstruction is discussed.\n\n\n\n\n\nD/S conversion revisited\n\n\n\n\n\n\nWhen a discrete-time signal x[n] with DTFT X(ejΩ) is converted into a sampled signal xs(t) with Fourier transform Xs(ω) via a D/S conversion, as discussed in Section 1.6.6, it has a frequency-domain description given by\n\n\n\n\n\n\n\n\n\n Xs(ω) = X(ejΩ)| Ω=ωTs = X(ejωTs ). \n\n\n(3.26)\n\n\n\n\n\n\nIn other words, the value of Xs(ω0) for a specific frequency ω0 rad/s is obtained from X(ejΩ0) where Ω0 = ω0Ts rad, as dictated by Eq. (1.22).\n\n\n\nThe notation is such that the subscript in Xs(ω) indicates the Fourier transform of a “sampled” signal or, alternatively,\n\nX(ejωTs) can be used. In both cases, the reader should have in mind that a sampled signal has a periodic spectrum.\n\n\n\nEq. (3.26) corresponds to scaling the abscissa of X(ejΩ), originally specified in rad, to create X(ejωTs) with an abscissa ω = ΩFs in rad/s. Figure 3.19 provides an example of the spectra involved in this D/S conversion.\n\n\n\n\n\n\n\n\nFigure 3.19: Result of converting x[n] with spectrum X(ejΩ) into xs(t) with Xs(ω) = X(ejωTs) via a D/S conversion using Fs = 10 Hz.\n\n\n\n\n\nThe datatips in Figure 3.19 highlight that the value |X(ejΩ0)| = 18.86 at Ω0 = 1.571 rad was converted to |X(ω0)| where ω0 = Ω0Fs = 15.71 rad/s.\n\n\n\nThe replicas that occur in the spectrum of a sampled signal are located in Nyquist zones, which are intervals of Fs∕2 when the frequency f is specified in Hertz. For example, the first Nyquist zone is [0,Fs∕2[, the second is [Fs∕2,Fs[ and so on. When the frequencies ω and Ω are specified in rad/s and rad, the bandwidths of the Nyquist zones are πFs and π, respectively.\n\n\n\nIn summary, Xs(ω) inherits the periodicity of X(ejΩ) in spite of the notation indicating it only by the subscript s of “sampled”. Therefore, in some situations, it is convenient to denote the spectrum of xs(t) as X(ejωTs), which indicates that the independent variable ω is in rad/s and makes explicit that this spectrum is periodic. An alternative to describing the sampled-signal spectrum X(ejωTs) in rad/s is to use ω = 2πf and represent X(ej2πfTs) in Hz.\n\n\n\nThe adoption of a filter to eliminate or attenuate the periodic replicas of a sampled signal X(ejωTs) is discussed in the sequel.\n\n\nSignal reconstruction\n\n\n\nDigital signal processing systems that interface with the analog world typically require two analog filters: the anti-aliasing and reconstruction, as indicated in Figure 3.7.\n\n\n\nThe reconstruction process, which converts a sampled signal xs(t) into a continuous-time signal x(t), is mathematically\nmodeled by convolving xs(t) with a signal h(t) to obtain x(t) = xs(t)∗h(t). As discussed along the text, the signal h(t) can be interpreted as the impulse response of a system and the reconstruction process can be pictorially depicted as:\n\n\n\n\n\n\n\n\n\n xs(t) → h(t) → x(t). \n\n\n(3.27)\n\n\n\n\n\n\nWhen one starts with the discrete-time signal x[n], the D/A process has two stages: D/S conversion, that transforms the discrete-time x[n] into a continuous sampled signal xs(t) and then processing (or “filtering”) with h(t) to obtain x(t).\n\n\n\nThere are two important options for h(t):\n\n &lt;ul class='itemize1'&gt;\n &lt;li class='itemize'&gt;&lt;span class='ec-lmri-10x-x-109'&gt;Zero-order hold &lt;/span&gt;reconstruction: where &lt;!-- l. 940 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt; &lt;mo class='MathClass-rel' stretchy='false'&gt;=&lt;/mo&gt; &lt;mi&gt;u&lt;/mi&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt; &lt;mo class='MathClass-bin' stretchy='false'&gt;−&lt;/mo&gt; &lt;mi&gt;u&lt;/mi&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt; &lt;mo class='MathClass-bin' stretchy='false'&gt;−&lt;/mo&gt; &lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\n is a pulse with duration &lt;!-- l. 940 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\n and amplitude 1\n &lt;/li&gt;\n &lt;li class='itemize'&gt;&lt;span class='ec-lmri-10x-x-109'&gt;Unitary energy &lt;/span&gt;reconstruction: where &lt;!-- l. 941 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;\n has unitary energy &lt;!-- l. 941 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;mi&gt;E&lt;/mi&gt; &lt;mo class='MathClass-rel' stretchy='false'&gt;=&lt;/mo&gt; &lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/math&gt;\n as, for example, the normalized pulse &lt;!-- l. 941 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;mi&gt;h&lt;/mi&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt; &lt;mo class='MathClass-rel' stretchy='false'&gt;=&lt;/mo&gt;   &lt;mfrac&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt; \nTs[u(t) − u(t − Ts)]\n\n\n\n\nAs illustrated in Figure 1.23, D/S followed by zero-order hold reconstruction is a simplified model for the actual process executed by a DAC chip. It is adopted here for simplicity. A consequence of ZOH is that Eq. (1.51) holds, and the power in continuous Pc of x(t) and\ndiscrete-time Pd of x[n] are the same.\n\n\n\nIn practice, the reconstruction process heavily depends on the respective analog filter and the DAC sampling frequency Fs. It is often necessary to use Fs higher than the one suggested by the sampling theorem, to simplify the job that must be done by the reconstruction filter H(f) = F{h(t)}.\n\n\n\nFor a signal with approximate bandwidth BW, it is tempting to use Fs = 2BW, which is denoted as Nyquist sampling. In this case the signal is called critically-sampled and the reconstruction should be done by an ideal lowpass filter with passband from 0 to Fs∕2. Even when Fs &gt; 2BW, the transition band of the reconstruction filter should be small enough to significantly attenuate the neighboring spectrum replicas of the sampled signal at the output of the D/S conversion.\n\n\n\nRecall that the spectrum replicas of X(ejωTs) are located at multiples of 2πFs rad/s (see, e. g., Figure 3.19), which correspond to replicas at multiples of Fs when considering X(ej2πfTs) with the frequency f in Hz.\n\n\n\nFigure 3.20 presents an extended version of Figure 1.24 that incorporates the filters A(s) and R(s) (these two analog filters are also indicated in Figure 3.7). While Figure 1.24 assumed ZOH reconstruction, Figure 3.20 illustrates the general case of an arbitrary reconstruction filter h(t).\n\n\n\n\n\n\n\n\nFigure 3.20: Extended version of Figure 1.24 using an arbitrary reconstruction filter h(t) and incorporating the filters A(s) and R(s).\n\n\n\n\n\nNote that there are two reconstruction filters in Figure 1.24, represented by h(t) and R(s). Their effect could be combined in only one filter with impulse response hr(t) = h(t)∗r(t), where r(t) is the inverse Laplace transform of R(s), but it is often pedagogical to distinguish them as follows.\n\n\n\nThe internal filter h(t) represents the filtering process that occurs within a DAC chip. Modern DACs can already incorporate a sophisticated h(t), but is is assumed here that this is not the case, and h(t) may implement ZOH or a filter with small order. The “external” reconstruction filter R(s) complements h(t) and provides improved rejection of the undesired spectrum replicas of ys(t) that may still be present in y(t). In summary, the main role of h(t) is the conversion of the sampled signal ys(t) into an analog signal y(t), while R(s) aims at achieving the specified level of performance with respect to filtering out the replicas in ys(t). The following example illustrates how signal reconstruction can be challenging in practice.\n\n\n\n Example 3.9. Examples of signal reconstruction. Figure 3.21 is the result of an example11 where a random signal y[n] with BW = 25 kHz and Fs = 200 kHz, is converted to an analog signal y(t). The reconstruction is performed by a DAC followed by a 5-th order analog filter H(f), with cutoff frequency fc = BW. This analog filter combines the effects of h(t) and R(s) in Figure 3.20.\n\n\n\nThe top plot in Figure 3.21 shows the magnitude of the DTFT Y (e2πTsf) of ys(t) = D/S{y[n]}, superimposed to the frequency\nresponse |H(f)| of the reconstruction filter. The multiples of Fs are identified in the grid of dashed lines. The bottom plot shows the magnitude of the resulting Fourier transform Y (f) = F{y(t)}.\n\n\n\n\n\n\n\n\nFigure 3.21: Reconstruction of a digital signal with BW = 25 kHz and Fs = 200 kHz using an analog filter with cutoff frequency fc = 25 kHz.\n\n\n\n\n\nFigure 3.22 was obtained under the same conditions used for Figure 3.21 but the signal bandwidth increased from 25 to 80 kHz. In this case, the filter did not significantly attenuate the two spectrum replicas in Y (e2πTsf) that are neighbors of the one centered at f = 0. Thinking of an asymptotic Bode-diagram, one can expect the fifth-order |H(f)| to drop at 6 dB/octave per pole and, from 80 to 160 kHz, reach 5 × (−6) = −30 dB. The replica centered at 200 kHz, for example, has its band starting at 120 kHz, and the reconstruction filter has an attenuation of only (approximately) 20 dB at 120 kHz.\n\n\n\n\n\n\n\n\nFigure 3.22: Same as Figure 3.21, but for a signal with BW = fc = 80 kHz. In this case, the reconstruction generates significant out-of-band power due to the spectrum replicas.\n\n\n\n\n\nAs illustrated by Figure 3.22, in practice, it is typically adopted a value for Fs large enough due to the non-ideal reconstruction filter. In the example corresponding to Figure 3.21, in which the reconstruction seems adequate, the Nyquist frequency Fs∕2 is four times the signal BW.   □\n\n\nCombined digital filtering and D/S steps\n\n\n\nWhen a digital filter Hz(z) (subindices will be used here to avoid confusion and disambiguate H(z) and H(s)) is part of an analog signal processing scheme such as the one depicted in Figure 3.20, it is sometimes useful to combine the roles of Hz(z) and D/S into one “analog” filter with frequency response Hz(ejωTs). This can be interpreted as a simple abscissa scaling in graphs such as Figure 3.19. Another view is that, while Eq. (3.26) refers to a sampled signal, a corresponding version of an equivalent (analog) “system” is\n\n\n\n\n\n\n\n\n\n Hs(ω) = Hz(ejΩ)| Ω=ωTs = Hz(ejωTs ). \n\n\n(3.28)\n\n\n\n\n\n \n\n7 It avoids the “ sin ⁡ (x)∕x” distortion in frequency domain associated to a pulse train.\n\n \n\n8 If needed, recall the notation for sampled signals in Section 1.6.1.\n\n \n\n9 See, e. g., references in Appendix B.26.4.\n\n \n\n10 For example, in [?].\n\n \n\n11 Figure 3.21 and Figure 3.22 were generated with figs_systems_showDACFilterEffect.m, which uses both a low and high sampling frequencies to mimic the reconstruction via an analog filter.\n\n              &lt;/div&gt;"
  },
  {
    "objectID": "ak_dsp_bookse36.html",
    "href": "ak_dsp_bookse36.html",
    "title": "36  Facts About First and Second-Order Systems",
    "section": "",
    "text": "Because they are basic building blocks for higher-order filters, first and second-order systems are discussed in the sequel.\n\n\n\n\n\n3.6.1  First-order systems\n\n\n\nIf the coefficients are real, a first-order system H(s) = 1∕(s + a),a ∈ ℝ, (as the one whose frequency response is depicted in Figure 3.12) is restricted to have a lowpass frequency response. This H(s) has a pole at s = −a and a zero at s = ∞. Using H(s) = a∕(s + a) leads to a unitary gain at DC. Placing a zero at s = 0 leads to H(s) = s∕(s + a), which has a highpass response.\n\n\n\nA system H(s) = 1∕(s + a) with ROC given by σ &gt; −a has an inverse Laplace transform h(t) = e−atu(t), which can be written as h(t) = e−t∕τu(t) where τ is the well-known time constant [url3mit]. After an interval of one time constant, the impulse response has decayed to 36.8% of its initial value.\n\n\n\nIf the ROC of H(s) = a∕(s + a) includes s = jω, the frequency response in this case is H(ω) = a∕(jω + a). As illustrated by Eq. (3.36), the magnitude |H(ω)| = a∕a2   + ω2 at ω = a is then |H(a)| = 1∕2, which is the cutoff frequency ωc = a because it corresponds to a decrease of 3 dB in power.\n\n\n\n\n\n3.6.2  Second-order systems\n\n\n\nThe system function H(s) of a second-order system (SOS), also called a two-pole resonator, has a denominator that can be written as s2 + 2αs + ωn2. When there are two zeros at ∞ and the gain at DC is unitary (H(s)|s=0 = 1), the SOS is:\n\n\n\n\n\n\n\n\n\n H(s) = ωn2 s2 + 2αs + ωn2, \n\n\n(3.29)\n\n\n\n\n\n\nwhere ωn is the natural frequency and α the decay rate parameter, which is useful for defining the damping ratio ζ = α∕ωn. Hence, Eq. (3.29) can be rewritten as:\n\n\n\n\n\n\n\n\n\n H(s) = ωn2 s2 + 2ζωns + ωn2. \n\n\n(3.30)\n\n\n\n\n\n\nThe values of α, ζ and ωn can be related to several characteristics of a SOS. For example,\n\nα represents the rate of exponential decay of oscillations when the system input is a unit step u(t). And depending on ζ, three categories of second-order systems are defined:\n\n &lt;ul class='itemize1'&gt;\n &lt;li class='itemize'&gt;&lt;!-- l. 1065 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;mi&gt;ζ&lt;/mi&gt; &lt;mo class='MathClass-rel' stretchy='false'&gt;=&lt;/mo&gt; &lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/math&gt;:\n &lt;span class='ec-lmri-10x-x-109'&gt;critically damped &lt;/span&gt;SOS, with a double real pole at &lt;!-- l. 1065 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt; &lt;mo class='MathClass-bin' stretchy='false'&gt;−&lt;/mo&gt; &lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;ω&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/math&gt;;\n &lt;/li&gt;\n &lt;li class='itemize'&gt;&lt;!-- l. 1066 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;mi&gt;ζ&lt;/mi&gt; &lt;mo class='MathClass-rel' stretchy='false'&gt;&gt;&lt;/mo&gt; &lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/math&gt;:\n &lt;span class='ec-lmri-10x-x-109'&gt;overdamped &lt;/span&gt;SOS, with real poles at &lt;!-- l. 1066 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt; &lt;mo class='MathClass-bin' stretchy='false'&gt;−&lt;/mo&gt; &lt;mi&gt;ζ&lt;/mi&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;ω&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt; &lt;mo class='MathClass-bin' stretchy='false'&gt;±&lt;/mo&gt; &lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;ω&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/math&gt;;\n &lt;/li&gt;\n &lt;li class='itemize'&gt;&lt;!-- l. 1067 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;mn&gt;0&lt;/mn&gt; &lt;mo class='MathClass-rel' stretchy='false'&gt;≤&lt;/mo&gt; &lt;mi&gt;ζ&lt;/mi&gt; &lt;mo class='MathClass-rel' stretchy='false'&gt;&lt;&lt;/mo&gt; &lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/math&gt;:\n &lt;span class='ec-lmri-10x-x-109'&gt;underdamped &lt;/span&gt;SOS, with complex conjugate poles.&lt;/li&gt;&lt;/ul&gt;\n\n\nNote that these three categories depend only on the denominator of H(s).\n\n\n\nThe numerator of H(s) represents another degree of freedom. Changing this numerator, leads to distinct SOS. An alternative to Eq. (3.29) (or, equivalently, Eq. (3.30)) is\n\n\n\n\n\n\n\n\n\n H(s) = s s2 + 2ζωns + ωn2, \n\n\n(3.31)\n\n\n\n\n\n\nwhich has a zero at the origin (H(s)|s=0 = 0). Yet, other two options of SOS can be defined as\n\n\n\n\n\n\n\n\n\n H(s) = s2 s2 + 2ζωns + ωn2 \n\n\n(3.32)\n\n\n\n\n\n\nand\n\n\n\n\n\n\n\n\n\n H(s) = 2ζωns + ωn2 s2 + 2ζωns + ωn2. \n\n\n(3.33)\n\n\n\n\n\n\nTable 3.2 summarizes these options for the numerator of a SOS.\n\n\n\n\n\n\n\nTable 3.2: Some distinct options for the numerator of a SOS.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic \n\n\nNumerator \n\n\nReference \n\n\n\n\n\n\n\n\n\n\n\n\nTwo-zeros at ∞ and unitary DC gain \n\n\n\nωn2\n\n\nEq. (3.30) \n\n\n\n\n\n\n\n\n\n\n\n\nOne zero at DC \n\n\n\ns\n\n\nEq. (3.31) \n\n\n\n\n\n\n\n\n\n\n\n\nTwo zeros at DC \n\n\n\ns2\n\n\nEq. (3.32) \n\n\n\n\n\n\n\n\n\n\n\n\nOne finite zero at  −ωn∕(2ζ)\n\n\n\n2ζωns + ωn2\n\n\nEq. (3.33) \n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                           &lt;/div&gt;\n                                                                          \n                                                                          \n\n\n\nUsing the quadratic formula to find the roots of a SOS, the poles are s0 = −α ±α2   − ωn   2, which can be rewritten as s0 = −α ± jωn   2   − α2. Figure 3.23 summarizes these relations. Note that ωn = |s0| and, unless α = 0 (poles on the jω axis), the natural frequency ωn is different from the center frequency of the pole ω0 = Imag{p} = ωn   2   − α2 = ωn1 − ζ2.\n\n\n\n\n\n\n\n\nFigure 3.23: Relations between natural frequency ωn, pole center frequency ω0, decay rate α and damping ratio ζ for a pair of complex conjugate poles.\n\n\n\n\n\nNote that for a first-order system H(s) = a∕(s + a) with real-valued coefficient a, the pole is real, its center frequency is ω0 = 0, and the natural and cutoff frequencies ωn = ωc = a coincide. For a second-order system, the expression for the cutoff frequency ωc is more elaborated. Considering Eq. (3.29), the cutoff is\n\n\n\n\n\n\n\n\n\n ωc = ωn1 − 2ζ2   +  4ζ4   − 4ζ2   + 2, \n\n\n(3.34)\n\n\n\n\n\n\nwhich can be found by using |H(s)|s=jωc = 1∕2. In this case, ωc = ωn when ζ = 1∕2.\n\n\n\n Example 3.10. On the damping ratio of a SOS. Figure 3.24 illustrates the influence of the damping ratio using the values ζ = 0.5,0.707,1 and 2.\n\n\n\n\n\n\n\n\nFigure 3.24: Magnitude of the frequency response for the SOS expressed by Eq. (3.30) and Eq. (3.33).\n\n\n\n\n\nFigure 3.24 also compares Eq. (3.30) and Eq. (3.33), which are two of the options contrasted in Table 3.2.   □\n\n\n\nFigure 3.25 illustrates some key time-domain performance parameters for an underdamped system obtained when the input x(t) = u(t) is a step function. The rise time tr is the interval for the step response to rise from 10 to 90% of its final value. The settling time ts is the interval to have the output within a given range, which is 5% in Figure 3.25. The overshoot is the peak amplitude and occurs at the peak time tp.\n\n\n\n\n\n\n\n\nFigure 3.25: Time-domain performance parameters for an underdamped system based on its step response.\n\n\n\n\n\nTable 3.3 summarizes some parameters for the system H(s) described by Eq. (3.30), which can be found12 from the inverse Laplace transform of Y (s) = H(s)∕s given that X(s) = 1∕s is the transform of u(t). The tolerance 𝜖 is used to obtain ts and a typical value is 𝜖 = 0.05.\n\n\n\n\n\n\n\nTable 3.3: Parameters of a second-order system as described by Eq. (3.30).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPerformance parameter \n\n\nExpression \n\n\nFor ζ = 2∕2 ≈0.707\n\n\n\n\n\n\n\n\n\n\n\n\nPeak time \n\n\n\ntp = π∕ω0 = π∕(ωn1 − ζ2)\n\n\n\ntp = 4.4∕ωn\n\n\n\n\n\n\n\n\n\n\n\n\nRise time \n\n\n\ntr ≈(2.23ζ2 −0.078ζ + 1.12)∕ω0\n\n\n\ntr = 3∕ωn\n\n\n\n\n\n\n\n\n\n\n\n\nSettling time \n\n\n\nts = −ln ⁡ (𝜖1 − ζ2)∕(ζωn)\n\n\n\nts = 4.7∕ωn\n\n\n\n\n\n\n\n\n\n\n\n\nOvershoot \n\n\n\nov = e−(ζπ)∕1−ζ2 \n\n\nov=4.32% \n\n\n\n\n\n\n\n\n\n\n\n\n3-dB bandwidth ωc (rad/s) \n\n\n\nωc = ωn1 − 2ζ2   +  2 − 4ζ2   + 4ζ4\n\n\n\nωc = ωn\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                                                                                                    &lt;/div&gt;\n                                                                          \n                                                                          \n\n\n\nTable 3.3 indicates that, for an underdamped system, the rise time when ζ = 2∕2 is tr = 3∕ωn. As expected, all three time parameters are inversely proportional to the natural frequency ωn.\n\n\n\nListing 3.10 indicates the commands to calculate the parameters, emphasizing the factors that depend on ζ (zeta) only.\n\n\n\nListing 3.10: MatlabOctaveCodeSnippets/snip_systems_sos_parameters.m\n\n\nzeta=0.5 %damping ratio, e.g. sqrt(2)/2 = 0.707; \nwn=2 %natural frequency in rad/s \nepsilon=0.05 %tolerance for the settling time (5% in this case) \ntp_factor=pi/sqrt(1-zeta^2) %depends on zeta only \n5tp=tp_factor/wn %peak time \ntr_factor=(2.23*zeta^2 - 0.078*zeta + 1.12)/sqrt(1-zeta^2) %zeta only \ntr=tr_factor/wn %rise time \nts_factor= -log(epsilon * sqrt(1-zeta^2))/(zeta) %zeta only \nts=ts_factor/wn %settling time \n10ov=exp(-(zeta*pi)/sqrt(1-zeta^2)) %overshoot (depends on zeta only) \nbw_factor=sqrt(1-2*zeta^2 + sqrt(2 - 4*zeta^2 + 4*zeta^4)) %zeta only \nwc=wn*bw_factor %cutoff frequency=3-dB bandwidth (in radians/second) \n%% Check whether the cuttoff frequency wc corresponds to a -3 dB gain \ns = 1j*wc %define s = j wc \n15Hs = (wn^2) / (s^2 + 2*zeta*wn*s + (wn^2)) % system function \ngain_at_wc = 20*log10(abs(Hs))  %answer is -3.0103 dB\n  \n\n\nMatlab has the stepinfo function, which can be used to obtain most of the parameters in Table 3.3 as follows:\n\n\nzeta=0.5 %damping ratio, e.g. sqrt(2)/2 = 0.707; \nwn=2 %natural frequency in rad/s \nsys = tf([wn^2],[1 2*zeta*wn wn^2]); %define the transfer function \nS=stepinfo(sys,'RiseTimeLimits',[0.1,0.9], ... \n5    'SettlingTimeThreshold',0.05) %get parameters with stepinfo \nstep(sys) %plot step response to check results if want\n\n\nThe system bandwidth will be discussed in the next section. Table 3.3 informs that, when ζ = 2∕2, the 3-dB bandwidth ωc in radians per second of the SOS given by Eq. (3.30) is simply its natural frequency ωn. In fact, as ζ varies from 0.5 to 0.8, which are the values typically used, its ωc varies from 1.27ωn to 0.87ωn. This justifies using ωn as a rough approximation of bandwidth for Eq. (3.30).\n\n\n \n\n12 These results can be found in textbooks of control systems or, e. g., [url3con].\n\n                     &lt;/div&gt;"
  },
  {
    "objectID": "ak_dsp_bookse37.html",
    "href": "ak_dsp_bookse37.html",
    "title": "37  Bandwidth and Quality Factor",
    "section": "",
    "text": "3.7.1  Bandwidth and Quality Factor of Poles\n\n\n\nThe overall behavior of a filter is dictated by the combined effect of its poles and zeros. The position of a pole in the s or Z plane determines how it influences the overall system function (H(s) or H(z), respectively).\n\n\n\nIt can be shown that asymptotically each pole contributes with a roll-off of 20 dB per decade (equivalent to 6 dB per octave) and a zero with an increase of 20 dB per decade. This asymptotic behavior is used to obtain graphs known as Bode diagrams. This clearly indicates that the sharpness of a frequency response (shorter transition regions) can be improved by increasing the order of the corresponding system function H(s) (or H(z)), i. e., adding poles and/or zeros. However, increasing the order of an analog filter requires more components (capacitors, opamps, etc.) and for a digital filter it requires more multiplications and additions. Hence, it is important to make the best use of poles (and zeros) and, motivated by that, to have figures of merit for them.\n\n\n\nBesides its frequency, the bandwidth and quality factor of a single pole or zero are important to assess its influence. Poles will be emphasized in this section, but similar definitions are applied to zeros. This section discusses characteristics of a single pole while Section 3.7.2 extends the definitions to filters.\n\n\n\n\n\nBandwidth of a pole\n\n\n\nThe bandwidth of a pole is illustrated in Figure 3.26 and defined here as BW = f2 − f1, where the cutoff frequencies f1 and f2 are those in which the gain\n\n|H(f0)| at the center frequency f0 (that may be different than the natural frequency fn) has decreased to |H(f0)|∕2. The factor 1∕2 corresponds to 20log ⁡ 10(1∕2) − 3.0103 dB, or approximately  − 3 dB. Therefore, this definition is called the 3-dB bandwidth or half-power BW.\n\n\n\n\n\n\n\n\nFigure 3.26: Bandwidth BW = f2 − f1 defined by the cutoff frequencies where the gain falls  − 3 dB below the reference value at f0.\n\n\n\n\nPole bandwidth in continuous-time\n\n\n\nAssuming a first-order system H(s) = −α∕(s − p), where p = α + jω0 is the pole, the bandwidth of p in Hz is given by\n\n\n\n\n\n\n\n\n\n BW = |α| π . \n\n\n(3.35)\n\n\n\n\n\n\nThis result can be obtained by noting that for any frequency ω\n\n\n\n\n\n\n |H(ω)| = |α| (ω − ω0   )2   + α2. \n\n\n\n\n\n\nHence, the squared gain at the specific pole frequency ω = ω0 is\n\n\n\n\n\n\n |H(ω0)|2 = α2 (ω0 − ω0)2 + α2 = 1 \n\n\n\n\n\n\nand the squared gains at cutoff frequencies ω = ω0 ± α fall to 1∕2 as required by the definition of cutoff frequency:\n\n\n\n\n\n\n\n\n\n |H(ω0 ± α)|2 = α2 (ω0 ± α − ω0)2 + α2 = 1 2. \n\n\n(3.36)\n\n\n\n\n\n\nHence, a range from ω0 − α to ω0 + α is determined by the two cutoff frequencies of a pole at ω0 such that its bandwidth BW is 2|α| rad/s. Dividing by 2π leads to BW = 2|α|∕(2π) in Hz, as indicated in Eq. (3.35). A similar relation holds in discrete-time, as follows.\n\n\n\n\n\nPole bandwidth in discrete-time\n\n\n\nUsing the previous results and assuming the transformation z = esTs of Eq. (2.48), the pole p = α + jω0 in the plane s is mapped\ninto the pole z = rejΩ0 in the plane Z via\n\n\n\n\n\n\n rejΩ0  = epTs  = e(α+jω0)Ts  = eαTs ejω0Ts . \n\n\n\n\n\n\nHence, r = eαTs and Ω0 = ω0Ts. Assuming α &lt; 0 for a causal and stable H(s), Eq. (3.35) indicates that α = −BWπ, such that r = eαTs = e−BWπTs and\n\n\n\n\n\n\n\n\n\n BW = −ln ⁡ (r) πTs \n\n\n(3.37)\n\n\n\n\n\n\nin Hz for a pole rejΩ0 in the Z plane with r &lt; 1.\n\n\n\nOne can rewrite Eq. (3.37) as ln ⁡ (r) = −BWπTs and use the Taylor series expansion of ln ⁡ (⋅) around a value x = a\n\n\n\n\n\n\n ln ⁡ (x) = ln ⁡ (a) + 1 a(x − a) − 1 2a2(x − a)2 + 1 3a3(x − a)3 −… ⁡ , \n\n\n\n\n\n\nwith a = 1, to keep only the first two terms and achieve\n\n\n\n\n\n\n ln ⁡ (r) ≈ ln ⁡ (1) + 1 1(r − 1) = r − 1. \n\n\n\n\n\n\nHence, the approximation r − 1 ≈−BWπTs allows to write\n\n\n\n\n\n\n\n\n\n BW ≈ (1 − r)Fs π , \n\n\n(3.38)\n\n\n\n\n\n\nwhich can be used when r is close to 1.\n\n\n\n\n\nQuality factor of poles\n\n\n\nBesides the number of poles and zeros, another factor that influences the sharpness of a frequency response is the quality factor (or Q-factor) of each pole, which is defined as\n\n\n\n\n\n\n\n\n\n Q=def fn BW, \n\n\n(3.39)\n\n\n\n\n\n\nwhere fn = ωn∕(2π) is the natural frequency in Hz and BW = f2 − f1 is the 3-dB bandwidth (also in Hz), as indicated in Figure 3.26.\n\n\n\nFor a second-order resonator, such as Eq. (3.29), the quality factor can be shown [url3dsp] to be\n\n\n\n\n\n\n\n\n\n Q = ωn 2α. \n\n\n(3.40)\n\n\n\n\n\n\nAppplication 3.6 presents a discussion on the influence of the Q-factor in frequency responses.\n\n\n\n\n\n3.7.2  Bandwidth and Quality Factor of Filters\n\n\n\nThe previous definitions of bandwidth and Q-factor of poles can be extended to systems, especially frequency-selective filters such as bandpass and lowpass.\n\n\n\n\n\nBandwidth definitions for signals and systems\n\n\n\n\n\n\nFor a generic system there are definitions for the bandwidth (BW) other than the 3-dB bandwidth of Figure 3.26. In all cases, if the frequency response has Hermitian symmetry, only the positive frequencies are taken in account.\n\n\n\nA generalization of BW is to adopt the X-dB bandwidth where X is the attenuation of interest, such as 1 dB.\n\n\n\nAlternatively, the absolute bandwidth can be applied when the signal is bandlimited. It is simply defined as the frequency range in which the spectrum is non-zero. For example, H(f) = rect(f) has BW = 0.5. Note that in this case the 3-dB BW definition would not be appropriate.\n\n\n\nAnother alternative definition of BW is the so-called null-to-null or zero-crossing BW. In the case of a lowpass spectrum, the BW is determined by the first null of |H(ω)| (or |H(Ω)| in discrete-time). For example, a frequency response given by the H(f) = sinc(fT) of Eq. (B.54) has BW = 1∕T according to this definition. For a passband spectrum, the two neighbor nulls of the center frequency determine BW. For example, the BW of H(f) = sinc(fT − 5T) is BW = 2∕T.\n\n\n\nWhen a signal is complex-valued, its spectrum X(f) does not have to exhibit Hermitian symmetry. In this case, the sampling theorem and other results depend on the double-sided or bilateral bandwidth, which takes in account the support of X(f) from negative to positive frequencies.\n\n\n\nFor example, first consider the “conventional” case of X(f) corresponding to a real-valued ideal lowpass filter with unitary gain from  − 200 to 200 Hz: its BW is 200 Hz, and its double-sided bandwidth is 400 Hz. Contrast now with a complex-valued signal\nwith X(f) being a square pulse from  − 300 to 100 Hz as depicted in Figure 3.18. In the latter case, the double-sided bandwidth continues to be 400 Hz but the conventional BW fails to indicate, for example, the minimum Fs according to the sampling theorem. In this case, the sampling theorem could be stated with respect to the double-sided bandwidth.\n\n\n\nAnother approach when dealing with complex-valued signals is to assume that X(f) is zero for negative frequencies (such signals are called “analytic”). In this case, the bandwidth BW takes in account only positive frequencies, as usual, but it may be misleading the fact that the sampling theorem in this case of an analytic signal can be stated as\n\n\n\n\n\n\n\n\n\n Fs &gt; BW. \n\n\n(3.41)\n\n\n\n\n\n\nFor example, if X(f) is non-zero from DC to 300 Hz, sampling the corresponding complex-valued (analytic) time-domain signal x(t) at Fs = 350 Hz suffices to avoid aliasing. In this case, the first spectrum replica at positive frequencies has support from 0 to 300 Hz, the second one from 350 to 650 Hz, and so on, indicating that there is no aliasing.\n\n\n\nA bandwidth definition very useful when dealing with filtered white noise is the equivalent noise bandwidth (ENBW). The ENBW of a filter H(f) is the bandwidth of a fictitious ideal (e. g., lowpass or bandpass) filter He(f) with rectangular spectrum that obeys\n\n\n\n\n\n\n\n\n\n ∫ −∞∞|H(f)|2df = ∫ −∞∞|H e(f)|2df \n\n\n(3.42)\n\n\n\n\n\n\nand has the maximum value Hmax of |He(f)| is equal to the maximum of |H(f)|. The name ENBW is because both filters lead to the same output power when the input is white noise (discussed in details later, in Section 4.5.2 ) such that the equivalent filter can substitute the original (and more complicated one) while leading, for instance, to the same SNR in a simulation or theoretical development.\n\n\n\nAn example is provided in Figure 3.27, which was obtained with Matlab’s function13 enbw as informed in Listing 3.11. In this case, the ENBW was 102.4∕50 ≈ 2 times larger than the cutoff frequency.\n\n\n\nListing 3.11: Code/MatlabBookFigures/figs_digicomm_enbw\n\n\nFs = 1000; %sampling frequency in Hz \nfc = 50; %filter cutoff frequency in Hz \n[B,A]=butter(4,fc/(Fs/2)); %4-th order Butterworth filter \nN=100; %number of samples in impulse response hn \n5hn = impz(B,A,100); %impulse response \nHf = fftshift(fft(hn)); %sampled DTFT \nequivalentBW = enbw(hn,Fs); %estimate equivalent noise bandwidth \n%code to plot the figure continues from here...\n  \n\n\n\n\n\n\n\nFigure 3.27: DTFT magnitude in dB of a 4-th order Butterworth filter with cutoff frequency of 50 Hz and the equivalent ideal filter with absolute bandwidth of 102.4 Hz.\n\n\n\n\n\nObserving Figure 3.27, due to the rectangular shape of |He(f)|,\n\n\n\n\n\n\n ∫ −∞∞|H e(f)|2df = 2 ENBW × Hmax2. \n\n\n\n\n\n\nHence, using Eq. (3.42) this can be written as:\n\n\n\n\n\n\n\n\n\n ENBW = ∫ −∞∞|H(f)|2df 2Hmax2 , \n\n\n(3.43)\n\n\n\n\n\n\nwhich for real signals simplify to\n\n\n\n\n\n\n\n\n\n ENBW = ∫ 0∞|H(f)|2df Hmax2 . \n\n\n(3.44)\n\n\n\n\n\nQuality factor for filters\n\n\n\n\n\n\nThe Q-factor of a pole, as defined in Eq. (3.39), can be easily adapted to a system with a frequency response that allows obtaining its 3-dB bandwidth BW (see Figure 3.26) such as a bandpass filter.14 In this case, the pole natural frequency fn in Eq. (3.39) is substituted by the filter’s center frequency fc such that\n\n\n\n\n\n\n\n\n\n Q=def fc BW. \n\n\n(3.45)\n\n\n\n\n\n\nThe inverse of Q, i. e., BW∕fc is called fractional bandwidth and often specified in percentage.\n\n\n \n\n13 The function enbw was designed for lowpass (windows) spectra with the maximum value at DC. It does not work for passband spectra.\n\n \n\n14 Actually, the Q factor is used in several areas of engineering and physics.\n\n                          &lt;/div&gt;"
  },
  {
    "objectID": "ak_dsp_bookse38.html",
    "href": "ak_dsp_bookse38.html",
    "title": "38  Importance of Linear Phase (or Constant Group Delay)",
    "section": "",
    "text": "In some applications such as analog signal transmission and audio amplification, the system should ideally have an output y(t) identical to its input x(t). Given that obtaining y(t) = x(t) is often unfeasible due to propagation delays inherent of communication channels and electronic systems, a more realistic target is to obtain y(t) = x(t − t0), a delayed version of the input. From the time-shift Fourier property (see Appendix B.17), the delay by t0 corresponds to a linear phase e−j2πft0 in frequency domain. Hence, having a linear phase is an important property of a system to achieve distortionless transmission, i. e., letting a signal pass without distortion.\n\n\n\nA linear phase filter with frequency response H(ω) has a phase Θ = ∠H(ω) that corresponds to a line segment in the passband (the phase behavior in the stopband is considered irrelevant). In practice, the line has a negative slope because a positive slope would correspond to a non-causal behavior (the output would be an anticipated version x(t + t0) of the input signal). Hence, it is convenient to define the group delay as:\n\n\n\n\n\n\n\n\n\n τg(ω) = −dΘ dω. \n\n\n(3.46)\n\n\n\n\n\n\nSimilarly, the discrete-time version is\n\n\n\n\n\n\n\n\n\n τg(ejΩ) = −dΘ dΩ \n\n\n(3.47)\n\n\n\n\n\n\nwith Θ = ∠H(ejΩ).\n\n\n\n Example 3.11. Impact of the system phase on the output signal. To help understanding the practical importance of a system with linear phase, it is adopted a periodic pulse x[n] with N1 = 15 (number of non-zero samples in a period) and period N = 50 (see Example 2.12). The experiment is to obtain Y [k] by multiplying the DTFS X[k] of x[n] by e−j2πN0k∕N, where N0 = 4. Listing 3.12 carries out the operation, including the conversion of Y [k] to y[n].\n\n\n\nListing 3.12: MatlabOctaveCodeSnippets/snip_systems_linear_phase_system.m\n\n\n%Specify: N-period, N1/N-duty cicle, N0-delay, k-frequency \nN=50; N1=15; N0=4; k=0:N-1; \nxn=[ones(1,N1) zeros(1,N-N1)]; %x[n] \nXk=fft(xn)/N; %calculate the DTFS of x[n] \n5phase = -2*pi/N*N0*k; %define linear phase \nYk=Xk.*exp(j*phase); %impose the linear phase \nyn=ifft(Yk)*N; %recover signal in time domain\n  \n\n\nFigure 3.28 illustrates the result: y[n] = x[n − N0] is a perfect delayed version of x[n]. The middle plot shows the phase that was added to the original phase of X[k]. The magnitude of X[k] was left unchanged but that would not be enough to keep the shape of x[n] in case the phase was not linear with frequency.\n\n\n\n\n\n\n\n\nFigure 3.28: Effect of adding a linear phase e−j2πN0k∕N (N0 = 4 and N = 50) to the DTFS of x[n] resulting in a delayed version y[n] = x[n − 4] .\n\n\n\n\n\nTo be contrasted with the linear phase case, Figure 3.29 provides an example where the phase is 0, 2 or  − 2 rad, as depicted in the top-most plot. In order to assure that y[n] is real, the phase is an odd function to preserve the Hermitian symmetry.\n\n\n\n\n\n\n\n\nFigure 3.29: Effect of adding the specified nonlinear phase (top) to the pulse in Figure 3.28, which leads to a distorted signal (bottom).\n\n\n\n\n\nNote that in the case of Figure 3.29, the pulses in x[n] are severely distorted due to the effect of a non-linear phase.    □\n\n\n\nConsidering continuous-time (same is valid for discrete-time), a linear phase e−j2πft0 avoids distorting an input signal x(t) because the system delays all components of x(t) by the same time interval t0. To obtain that, a linear phase system adds to the input signal component with frequency fi, a phase Θi = −2πfit0 that is proportional to fi (the larger fi, the larger the phase Θi added by the system). The following example concerns this aspect.\n\n\n\n Example 3.12. A linear phase allows components with distinct frequencies to be delayed by the same angle. For example, assume components corresponding to ω = 200 and 400 rad/s with t0 = 4 s. The phases Θ = ωt0 are 800 and 1600 rad, respectively. On the other hand, if these two components pass a system that adds a fixed phase Θ = 1200 rad to both, the delays would be 6 and 3 s, respectively, which would potentially distort the delayed version with respect to the original signal x(t).    □\n\n\n\nFor non-linear phase filters, the group delay must be interpreted as an average delay that is frequency-dependent (compare Figure 3.49 and Figure 3.50).15\n\n\n\n Example 3.13. Gaussian filter and minimum group delay. In some applications, such as digital communications, minimizing the group delay is important to achieve low latency. In this case, the Gaussian filter is competitive because it has the minimum possible group delay. As a consequence, this filter has no overshoot to an input u(t) while minimizing the rise and fall time intervals. Its non-causal (and ideal, which requires truncation in practice) impulse response is\n\n\n\n\n\n\n\n\n\n h(t) = π α e− (πt α )2 , \n\n\n(3.48)\n\n\n\n\n\n\nwhere α controls the 3-dB bandwidth BW. More specifically, α depends on the product of BW and the symbol period Tsym as follows:\n\n\n\n\n\n\n\n\n\n α = 1 BWTsym ln ⁡   2 2 . \n\n\n(3.49)\n\n\n\n\n\n\nThe frequency response of this filter is H(f) = e−α2f2  and decreases with f but is not zero for finite f.   □\n\n\n \n\n15 After discussing digital filters, their phase and group delay will be exemplified in Figure 3.49 and Figure 3.50.\n\n                                                                                                              &lt;/div&gt;"
  },
  {
    "objectID": "ak_dsp_bookse39.html",
    "href": "ak_dsp_bookse39.html",
    "title": "39  Filtering technologies: Surface acoustic wave (SAW) and others",
    "section": "",
    "text": "There are several technologies to build an electronic filter. The options include ceramic, microelectromechanical system (MEMS), yttrium iron garnet (YIG), SAW and many others. Even if the scope is reduced to radio-frequency (RF) communications, the diversity of filters is huge. Consequently, parameters such as the Q-factor, center frequency fc and insertion loss IL(f) (which is the inverse of the insertion gain IG(f) = 1∕IL(f), as discussed in Section B.25), vary widely. This section provides some practical examples.\n\n\n\n Example 3.14. Performance of commercial SAW filter. Figure 3.30 illustrates the typical performance of a commercial SAW filter16 with some of its specifications listed in Table 3.4. The insertion gain IG(f) is shown in Figure 3.30 at 1 and 10 dB per division (div) at the left plot and superimposed (at 1 dB per division) to the group delay at the right plot.\n\n\n\n\n\n\n\n\nFigure 3.30: Performance of a commercial SAW filter. The insertion gain IG(f) at two resolutions at the left plot and superimposed to the group delay at the right.\n\n\n\n\n\nAs also indicated in Table 3.4, Figure 3.30 indicates that df1, df3 and df40 are the bandwidths considering the attenuation at 1, 3 and 40 dB, respectively, with values 13.08, 13.88 and 16.6 MHz. The insertion loss IL(f)|f=fc at fc = 70.12 MHz is 24.7 dB. The group delay plot only shows the variation at the order of ns, but Table 3.4 indicates the delay as 1.6 μs. As indicated in Section B.25, IG(f) is sometimes called IL and Figure 3.30 uses a negative IL to express that the filter attenuates the power by 24.7 dB as properly expressed in Table 3.4.\n\n\n\n\n\n\n\nTable 3.4: Specifications of a commercial SAW filter.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter \n\n\nMinimum \n\n\nTypical \n\n\nMaximum \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCenter frequency at 25o C (fc or f0, MHz) \n\n\n69.8 \n\n\n70 \n\n\n70.2 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInsertion loss at fc (IL, dB) \n\n\n- \n\n\n24.7 \n\n\n28.0 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1-dB bandwidth (df1, MHz) \n\n\n12.75 \n\n\n13.0 \n\n\n- \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3-dB bandwidth (df3, MHz) \n\n\n13.0 \n\n\n13.8 \n\n\n- \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n40-dB bandwidth (df40, MHz) \n\n\n- \n\n\n16.6 \n\n\n17.1 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPassband variation (ripple, dB) \n\n\n- \n\n\n0.5 \n\n\n0.6 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGroup delay variation (ns) \n\n\n- \n\n\n25 \n\n\n50 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbsolute delay (μs) \n\n\n- \n\n\n1.6 \n\n\n- \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                &lt;/div&gt;\n                                                                          \n                                                                          \n\n\n\nThe Q-factor of the filter depicted in Figure 3.30 is Q = 70.12∕13.88 ≈ 5 and provides more than 60 dB of attenuation at the stopband. Building such (relatively) highly-selective filters is easier when the center frequency fc is fixed than when it has to vary, for tuning purposes. For example, the filter of Figure 3.30 is supposed to operate with fc = 70 MHz, which is an intermediate frequency (IF) used in wireless signal reception. When a receiver has to operate over a frequency range, a common strategy is to pre-select the frequency band of interest using a so-called RF-filter with center frequency fcRF (e. g., 2.1 GHz) and convert it via frequency shifting to the IF (e. g., 70 MHz), when it is then better filtered and further processed.   □\n\n\n\nA common IF for broadcast AM radio is fc = 455 kHz, which motivates the next filter example.\n\n\n\n Example 3.15. Commercial ceramic filter. Figure 3.31 illustrates the performance of a commercial ceramic filter17 that can be used with an IF of 455 kHz and has some specifications listed in Table 3.5. Note that this filter’s stopband is not flat, especially within the range from 600 to 700 kHz. This makes the specifications harder to interpret than for an ideal bandpass filter.\n\n\n\n\n\n\n\n\nFigure 3.31: Performance of a commercial ceramic filter. The right plot is a zoom of the attenuation at the left plot, centered at fc = 455 kHz and includes the group delay with values ranging between approximately 50 and 120 μs in passband.\n\n\n\n\n\n\n\n\n\nTable 3.5: Specifications of a commercial ceramic filter where fn = 455 kHz is the nominal frequency and fc is the center of the 6-dB BW.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParameter \n\n\nValue \n\n\n\n\n\n\n\n\n\n\nCenter frequency fc (kHz) \n\n\n\n455 ±1.5\n\n\n\n\n\n\n\n\n\n\nMinimum 6-dB bandwidth (kHz) \n\n\n\nfn ±7.5\n\n\n\n\n\n\n\n\n\n\nMaximum stop bandwidth, within 40 dB (kHz) \n\n\n\nfn ±15.0\n\n\n\n\n\n\n\n\n\n\nMinimum stopband attenuation within fn ±100.0 kHz (dB) \n\n\n27 \n\n\n\n\n\n\n\n\n\n\nMaximum insertion loss (at minimum loss point, dB) \n\n\n6 \n\n\n\n\n\n\n\n\n\n\nMaximum ripple within fn ±5.0 kHz (dB) \n\n\n1.5 \n\n\n\n\n\n\n\n\n\n\n                                                                              &lt;/div&gt;\n                                                                          \n                                                                          \n\n\n\nCaution has to be exercised when interpreting the “Maximum stop bandwidth” in Table 3.5, which indicates the filter attenuates 40 dB at the end of a band centered on fn and with a bandwidth of approximately 30 kHz. But this attenuation then decreases. According to Table 3.5, an attenuation of at least 27 dB is guaranteed only within 455 ± 100 kHz.   □\n\n\n \n\n16 The Vanlong BP60160, [url3van].\n\n \n\n17 Filter SFPKA455KE4A-R1 in page 5 of [url3mur].\n\n                                                        &lt;/div&gt;"
  },
  {
    "objectID": "ak_dsp_bookse40.html",
    "href": "ak_dsp_bookse40.html",
    "title": "40  Digital Filters",
    "section": "",
    "text": "A digital filter is a discrete-time system that deals with quantized input xq[n] and output yq[n] signals (quantized amplitudes). A possible practical scenario for a digital filter is the one shown in Figure 3.7. There are many different kinds of digital filters but this section assumes filters that are based on linear constant-coefficient difference equations (LCCDE). As indicated in Figure 3.9, when zero initial conditions are assumed, the LCCDE systems are a special case of LTI systems. But when one takes in account the non-linear quantization effects that occur in practical CPUs (or digital signal processors) due to their limited precision, the overall system is not strictly LTI. In spite of that, it is convenient to isolate the effects of quantization and finite-precision arithmetics, and study digital filters as LTI LCCDE systems. Later on, the impact of quantizing the filter coefficients and the roundoff errors due to finite-precision can be incorporated. Therefore, hereafter, the term digital filter refers to discrete-time LTI LCCDE systems. Besides, the systems are assumed to be causal.\n\n\n\nA system that has the output and input signals related by a LCCDE such as\n\n\n\n\n\n\n\n\n\n ∑ k=0Na ky[n − k] = ∑ k=0Mb kx[n − k] \n\n\n(3.50)\n\n\n\n\n\n\ncan be represented by (taking the Z-transform on both sides of the LCCDE)\n\n\n\n\n\n\n Y (z)∑ k=0Na kz−k = X(z)∑ k=0Mb kz−k. \n\n\n\n\n\n\nIn this case, the transfer function is\n\n\n\n\n\n\n\n\n\n H(z) = Y (z) X(z) = ∑ k=0Mbkz−k  ∑ k=0Nakz−k, \n\n\n(3.51)\n\n\n\n\n\n\nwhich, after eventual algebraic simplifications, can be written as the ratio\n\n\n\n\n\n\n\n\n\n H(z) = B(z) A(z) \n\n\n(3.52)\n\n\n\n\n\n\nof two polynomials in z. Calculating the zeros zk (roots of the numerator B(z)) and the poles pk (roots of the denominator A(z)), one can write\n\n\n\n\n\n\n H(z) = g∏ k=1M(z − zk) ∏ k=1N(z − pk) , \n\n\n\n\n\n\nwhere g is a gain. The coefficients of Eq. (3.50) can be real or complex. For simplicity, they will be assumed real (ak,bk ∈ ℝ) unless otherwise stated. In this case, g ∈ ℝ and the roots zk,pk are real or, for each root rej𝜃, its complex conjugate re−j𝜃 is also a root such that the product (z − rej𝜃)(z − re−j𝜃) = z2 + 2rcos ⁡ (𝜃) + r2 leads to a second-order polynomial with real coefficients.\n\n\n\nIt is useful to know that the value of H(z) at z = 1 and z = −1 correspond to the values of the frequency response H(ejΩ) at Ω = 0 (DC) and Ω = π rad, respectively, which can be written as\n\n\n\n\n\n\n\n\n\n H(z)|z=1 = H(ejΩ)| Ω=0    and   H(z)|z=−1 = H(ejΩ)| Ω=π. \n\n\n(3.53)\n\n\n\n\n\n\nFor example H(z) = 3 + z−1 has a frequency response with H(ejΩ)|Ω=0 = 3 + 1 = 4 and H(ejΩ)|Ω=π = 3 − 1 = 2.\n\n\n\n Example 3.16. Specifying a filter in Hz, rad or normalized frequency. When designing digital filters, it is important to remember the fundamental relation\n\nω = ΩFs, first presented in Eq. (1.22). Figure 3.32 illustrates how this relation can be used to convert a specification for a lowpass analog filter with passband and stopband frequencies of 500 and 850 Hz, respectively, into a specification of a digital filter with Fs = 2000Hz, which leads to passband and stopband frequencies of 2π500∕2000 ≈ 1.57 and 2π850∕2000 ≈ 2.67 rad, respectively.\n\n\n\n\n\n\n\n\n\n\n\n(a) Analog (f in Hz)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Digital (Ω in rad)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) Digital (fN in Ω∕π)\n\n\n\n\n\n\n\nFigure 3.32: Comparison of analog filter specification (a) and two corresponding digital versions assuming Fs = 2000 Hz: (b) was obtained with ω = ΩFs and (c) uses the convention adopted in Matlab/Octave.\n\n\n\n\n\nFigure 3.32 also shows the convention used by Matlab/Octave, which gets rid of the irrational number π by using a normalized axis fN = Ω∕π, as listed in Table 1.4. In this case, ω = ΩFs can be written as 2πf = fNπFs, which simplifies to obtaining fN = f∕(Fs∕2) with the normalization of f by the Nyquist frequency Fs∕2. For example, f = 500 Hz in Figure 3.32(a) turns into fN = 500∕1000 = 0.5 in Figure 3.32(c). This normalization by the Nyquist frequency is sensible because digital filters are always constrained to work with frequencies of at most Fs∕2 Hz, which corresponds to π rad.   □\n\n\n3.10.1  FIR, IIR, AR, MA and ARMA systems\n\n\n\nAn important characteristic of a digital filter is the duration of its impulse response, which allows to classify it as a finite impulse response (FIR) or infinite impulse response (IIR) filter. For example, a filter with hf[n] = 3δ[n] + 2δ[n − 1] is FIR and a filter with hi[n] = 0.8nu[n] is IIR. Note that if one plots hi[n] in a computer, due to the limited precision, the values of hi[n] would underflow to zero for large enough n, but theoretically this impulse response has infinite duration.\n\n\n\nAnother characteristic is whether the filter is recursive or not. A recursive filter has the output y[n] depending on past values of the output itself (y[n − 1],y[n − 2],…) while the output of a non-recursive filter depends only on the input x[n] and its past values (assuming causality). The system function H(z) of a non-recursive filter has N = 0 in Eq. (3.51) and no finite poles.\n\n\n\nIn practice, FIR can be associated to non-recursive filters and IIR to recursive filters. But recursiveness and impulse response duration are two distinct concepts! It is possible to create a recursive filter with an impulse response of finite duration using cancellation of poles and zeros, for example.\n\n\n\nIIR and FIR filters will be discussed in the sequel. While the subject is presented in the context of digital filtering, many conclusions are valid for systems in general. In other words, IIR and FIR systems can be found in applications in which the goal is not filtering, but system identification, equalization, control, tracking, statistics, etc. In some of these areas, H(z) is not classified as FIR or IIR, but as MA, AR or ARMA “model”.\n\n\n\nA moving-average (MA) model H(z) is a non-recursive FIR filter and an autoregressive (AR) filter is a special case of a recursive IIR filter for which M = 0 in Eq. (3.51). The AR IIR filter does not have finite zeros other than at the origin z = 0 and is given by\n\n\n\n\n\n\n\n\n\n H(z) = g zN  ∏ k=1N(z − pk) = g 1 + a1z−1 + a2z−2 + … + aNz−N. \n\n\n(3.54)\n\n\n\n\n\n\nNote in Eq. (3.54) that the gain value g for the numerator is chosen such that the denominator coefficient a0 of Eq. (3.51) is equal to 1. The numerator zN leads to N zeros at z = 0 and makes H(z) causal.\n\n\n\nThe autoregressive-moving-average (ARMA) model is a generic H(z) model composed by an AR (denominator) and MA (numerator) sections. There are many generalizations of these basic models, which are adopted in areas such as statistics. Parametric modeling is the task of, according to a predefined criterion such as least-squares, fit the data to the assumed model. In spite of being defined in distinct scenarios, filter design and parametric modeling share several fundamental results.\n\n\n\nBefore discussing the design of IIR and FIR filters, it is useful to note the importance of frequency scaling.\n\n\n\n\n\n3.10.2  Filter frequency scaling\n\n\n\nComputational routines often produce a prototype with normalized frequencies (e. g., a cuttof frequency of 1 rad/s) that are then converted (eventually by another routine) to the required frequencies via frequency scaling.\n\n\n\nWith frequency scaling one can modify a filter to obtain a different cutoff frequency. For example, if H(s) has a cutoff of ωc rad/s, the transformation s → s∕k moves this cutoff frequency to kωc rad/s. The operation is similar to using x(2t) to double the “speed” of a signal represented by x(t), which leads to x(2t) having half of the support of x(t). Similarly, x(t∕2) has twice the support of x(t). More specifically, if the prototype H(s) has a cutoff frequency of 1 rad/s, then\n\n\n\n\n\n\n\n\n\n G(s) = H(s)|s=s∕ωc \n\n\n(3.55)\n\n\n\n\n\n\nscales the complex plane, and consequently s = jω, such that ωc is the cutoff frequency of the new filter G(s).\n\n\n\nThere are computational routines that provide a prototype filter with (normalized) cutoff frequency of 1 rad/s, which can then be converted to a new filter with an arbitrary cutoff frequency ωc with the transformation s → s∕ωc. For example, this change in cutoff frequency can be done in Matlab by applying the function lp2lp to a lowpass analog prototype (current version of Octave misses this functionality) as indicated in Listing 3.13.\n\n\n\nListing 3.13: MatlabOnly/snip_systems_filter_conversion.m\n\n\nN = 5; %filter order \n[B,A] = butter(N,1,'s'); %prototype with cutoff=1 rad/s \n[Bnew,Anew]=lp2lp(B,A,100); %convert to cutoff=100 rad/s \nfreqs(Bnew,Anew) %plot mag (linear scale) and phase\n  \n\n\nNote that lp2lp requires the prototype to have a cutoff frequency of ωc = 1 rad/s (besides being an analog lowpass filter).\n\n\n\n Example 3.17. Frequency conversion when the prototype does not have a cutoff frequency of 1 rad/s. Assume that a frequency conversion must be applied to H(s) = 100∕(s + 100) such that its cutoff frequency of ωc = 100 rad/s is shifted to 30 rad/s. Because this is a simple first-order filter, the final result G(s) = 30∕(s + 30) can be immediately obtained. But the whole procedure is described because it is useful for higher-order filters.\n\n\n\nA convenient step is to move the cutoff frequency of H(s) to 1 rad/s using a substitution of s by 100s, which leads to\n\n\n\n\n\n\n F(s) = H(100s) = 100 100s + 100 = 1 s + 1. \n\n\n\n\n\n\nNow, the final filter can be obtained by expanding ωc to 30 by substituting s in F(s) by s∕30:\n\n\n\n\n\n\n G(s) = F(s∕30) = 1  s 30 + 1 = 30 s + 30. \n\n\n\n\n\n\nAlternatively, one could directly use G(s) = H(100s∕30). These frequency conversions are widely used in the design of IIR filters as will be discussed.   □\n\n\n\n\n\n3.10.3  Filter bandform transformation: Lowpass to highpass, etc.\n\n\n\nThere are many transformations to convert a prototype (or reference) filter into another one. Section 3.10.2 briefly discussed frequency scaling. It is also possible to convert a lowpass prototype filter into a highpass, bandpass and bandstop, for example, which is called bandform transformation.\n\n\n\nWhen dealing with analog filters, it is often necessary to apply impedance scaling to have the filter input and/or output with a desired impedance. The intent in this case is not to change the transfer function, but to perform impedance matching. All these three kinds of filter transformations are well-discussed in the literature18. In the sequel, bandform transformation is briefly discussed.\n\n\n\nBandform transformations can be used to create a bandpass from a lowpass prototype, for example. These transformations differ considerably for analog and digital frequencies, especially due to the fact that a digital filter has a periodic spectrum.19 Having a lowpass analog prototype, a Matlab user20 can use the functions lp2bp, lp2bs and lp2hp to convert it to bandpass, bandstop and highpass filters, respectively.\n\n\n\nSometimes frequency scaling is incorporated to bandform transformation such that a routine converts a lowpass prototype with normalized 1 rad/s cutoff frequency into another filter with desired bandwidth and frequencies. For example, the Matlab’s function lp2bp converts a normalized lowpass into a bandpass with the desired center frequency and bandwidth as illustrated in Listing 3.14.\n\n\n\nListing 3.14: MatlabOnly/snip_systems_lowpass_to_bandpass.m\n\n\nN = 4; %prototype order (bandpass order is 2N = 8) \n[B,A] = butter(N,1,'s'); %prototype with cutoff=1 rad/s \n[Bnew,Anew]=lp2bp(B,A,50,30); %convert to cutoff=50 rad/s \nfreqs(Bnew,Anew) %plot mag (linear scale) and phase\n  \n\n\nThe functions lp2bp, lp2bs, lp2hp and lp2lp are restricted to analog filters. Transformations of digital filters can be achieved via a mapping provided by allpass filters, which are filters that have unitary magnitude for all frequencies and just change the phase of the input signal. Besides frequency transformation, allpass filters are adopted for phase equalization and other applications. Some Matlab functions that provide support to digital filter transformations are iirlp2bs, iirlp2bp, iirlp2xn, iirftransf, allpasslp2xn and zpklp2xn.\n\n\n\nFor example, Listing 3.15 compares the conversion of a Butterworth and a elliptic lowpass prototypes, both with cutoff frequency Ωc = 0.5 rad, into stopband versions with cutoff frequencies Ω1 = 0.5 and Ω2 = 0.8 rad. The two prototypes are halfband filters because their cutoff frequencies is 0.5 rad.\n\n\n\nListing 3.15: MatlabOnly/snip_systems_bandstop_conversion.m\n\n\nWc=0.5; %cutoff (normalized) frequency of prototype filter \nWstop = [0.5 0.8]; %stopband cutoff frequencies \nN=3; %order of prototype filter \n[B,A] = butter(N,Wc); %Butterworth prototype \n5[Bnew,Anew]=iirlp2bs(B,A,Wc,Wstop); %Bandstop Butterworth \n[B2, A2] = ellip(N, 3, 30, Wc); %Elliptic prototype \n[Bnew2,Anew2]=iirlp2bs(B2,A2,Wc,Wstop); %Bandstop elliptic \nfvtool(Bnew2, Anew2, Bnew, Anew); %compare freq. responses\n  \n\n\nFor the comparison, it was used the Matlab’s Filter Visualization Tool (fvtool). Note that the fourth argument of ellip is the passband frequency (not necessarily the cutoff frequency). In this case, a maximum ripple of 3 dB was specified using the second argument of ellip to have the passband frequency coinciding with the cutoff frequency Wc (Ωc = 0.5 rad). Besides, a minimum attenuation of 30 dB was required for the stopband of the elliptic lowpass prototype.\n\n\n\nAs another example, the following Matlab code converts a Butterworth lowpass prototype with cutoff frequency Ωc = 0.5 rad into a bandpass with cutoff frequencies Ω1 = 0.25 and Ω2 = 0.75 rad.\n\n\nN = 5; %filter order \n[B,A] = butter(N,0.5); %prototype with cutoff=0.5 rad \n[Bnew,Anew]=iirlp2xn(B,A,[-0.5 0.5],[0.25 0.75],'pass');\n\n\nThe third and fourth arguments of function iirlp2xn ([-0.5 0.5] and [0.25 0.75]) indicate frequency points in the original filter and to what frequencies they should be mapped. In this case, Ω = −0.5 rad should be\nmapped to Ω = 0.25 rad and Ω = 0.5 to Ω = 0.75 rad.\n\n\n\nInstead of creating a prototype and then converting to the desired filter, most routines allow to directly design the filter. The following commands illustrate the design of highpass, bandpass and bandstop elliptic filters.\n\n\n\nListing 3.16: MatlabOctaveCodeSnippets/snip_systems_elliptic_filter_design.m\n\n\nN=5; %filter order for the lowpass prototype (e.g., bandpass is 2*N) \nRp=0.1; %maximum ripple in passband (dB) \nRs=30; %minimum attenuation in stopband (dB) \n[B,A]=ellip(N,Rp,Rs,0.4,'high'); %highpass, cutoff=0.4 rad \n5[B,A]=ellip(N,Rp,Rs,[0.5 0.8]); %bandpass, BW=[0.5, 0.8], order=2*N \n[B,A]=ellip(N,Rp,Rs,[0.5 0.8],'stop'); %BW=[0.5, 0.8], order=2*N\n  \n\n\nSimilarly, FIR design routines provide support to directly designing filters other than lowpass ones. For example, Listing 3.17 designs a stopband filter. Note the normalization by the Nyquist frequency Fs/2.\n\n\n\nListing 3.17: MatlabOctaveCodeSnippets/snip_systems_FIR_filter_design.m\n\n\nFs = 44100; %sampling freq. (all freqs. in Hz) \nN  = 100;   %filter order (N+1 coefficientes) \nFc1  = 8400;  % First cuttof frequency \nFc2  = 13200; % Second cuttof frequency \n5B  = fir1(N, [Fc1 Fc2]/(Fs/2), 'stop', hamming(N+1));\n  \n\n\nBecause there are plenty of routines for bandform transformation, the discussion about IIR and FIR filter design concentrates on lowpass filters.\n\n\n \n\n18 See references in [url3tra].\n\n \n\n19 Compare Matlab’s documentation for “Digital frequency transformations” and “Analog filter transformation”, currently at [url3mad] and [url3maa], respectively.\n\n \n\n20 As indicated, the current version of Octave misses this functionality.\n\n                                 &lt;/div&gt;"
  },
  {
    "objectID": "ak_dsp_bookse41.html",
    "href": "ak_dsp_bookse41.html",
    "title": "41  IIR Filter Design",
    "section": "",
    "text": "There are two main categories of IIR filter design: direct and indirect methods. In the latter category, one first designs a continuous-time system function H(s), and in a second step converts H(s) into a discrete-time filter H(z). In contrast, the direct methods obtain H(z) directly from the specifications.\n\n\n\n\n\n3.11.1  Direct IIR filter design\n\n\n\nThe direct methods for IIR design are tightly related to parametric power spectrum estimation. For instance, the Yule-Walker IIR filter design method is related to the techniques discussed in Section 4.8.\n\n\n\nThe Yule-Walker method is implemented in the Matlab function yulewalk. Its input is a simplified description of the desired discrete-time frequency response magnitude |H(ejΩ)|, which can be arbitrary. In other words, |H(ejΩ)| is not constrained to be the standard lowpass, highpass, bandpass or bandstop responses, which is an advantage of this method. Yule-Walker basically consists in discretizing the specification of |H(ejΩ)| to obtain |H[k]|, finding the inverse DFT of |H[k]| and solving modified Yule-Walker equations to obtain the filter coefficients  [?]. The phase information cannot be specified. The code below illustrates how Matlab can design a digital filter with two passbands and order 16:\n\n\nN = 16; %filter order \nf = [0 0.3 0.3 0.6 0.6 0.8 0.8 1]; %frequencies \nm = [1 1   0   0   1   1   0   0]; %magnitudes (linear scale) \n[B,A] = yulewalk(N,f,m); %get H(z)=B(z)/A(z) \n5[h,w] = freqz(B,A,128); %frequency response with 128 points \nplot(w/pi,20*log10(abs(h))) %plot magnitude in dB\n\n\n\n\n3.11.2  Indirect IIR filter design\n\n\n\nWhen the desired filter should have standard lowpass, highpass, bandpass or bandstop responses, indirect design methods are typically adopted. The reason is that there are many well-established recipes for designing analog filters H(s) and also methods for mapping H(s) into a discrete-time H(z) filter.\n\n\n\nThe indirect IIR filter design has the following steps:\n\n\n\nf 1.\n\n\nconvert the filter specification from discrete to continuous-time,\n\n\nf 2.\n\n\ndesign the continuous-time H(s),\n\n\nf 3.\n\n\nconvert H(s) into H(z).\n\n\n\n\nSome practical analog filter designs (the step that obtains H(s)) are discussed in Example 3.3. The emphasis in the next paragraphs is the last step: conversion of H(s) into H(z).\n\n\n\nNote that, after H(s) is converted into H(z), the frequency response H(ω) of the former is mapped to H(ejΩ). When the goal of H(z) is to perform filtering, it is often of interest to compare H(ejΩ) and H(ω). In this case, as discussed in Section 3.5.7.0, the D/S conversion of the discrete-time impulse response corresponding to H(ejΩ) can be used to obtain H(ejωTs) in rad/s, which can then be more directly compared with the original H(ω) that is also in rad/s. In such situation, it may be worth to use subscripts Hs(ω) and Hz(ejωTs) to identify the original and D/S converted frequency responses, respectively. Given that Hz(ejωTs) is periodic, a reasonable goal may be to seek a good approximation over the first Nyquist zone, i. e.\n\n\n\n\n\n\n\n\n\n Hz(ejωTs ) ≈ Hs(ω),   over   0 ≤ ω &lt; πFs. \n\n\n(3.56)\n\n\n\n\n\n\nFive methods to convert a continuous-time H(s) into a discrete-time H(z) transfer function are briefly discussed in the sequel with an emphasis on the bilinear method, which is the most used in designing standard filters (lowpass, highpass, etc.).\n\n\n\n\n\n3.11.3  Methods to convert continuous into discrete-time system functions\n\n\n\n\n\nMatched Z-transform or pole-zero matching\n\n\n\nThe matched Z-transform method, also called pole-zero matching method, consists in simply using Eq. (2.48) to map all finite21 poles and zeros of H(s) into new poles and zeros, in the z-plane, which then compose H(z). In other words, assuming si is the i-th zero or pole of H(s), it is mapped into a corresponding zero or pole of H(z) given by zi = esiTs, where Ts is the sampling interval.\n\n\n\nAfter the mapping is performed, a scaling factor is used such that the magnitude of H(z) is the same of H(s) for a specific frequency of interest. For example, for lowpass systems, the DC gain is adjusted to have H(z)|z=1 = H(s)|s=0.\n\n\n\n Example 3.18. Matched Z-transform of a first-order system. The matched Z-transform of\n\nH(s) = a∕(s + a) is discussed in this example. The pole at s = −a is mapped to a pole at z = e−aTs such that H(z) = 1∕(z − e−aTs). Note that H(s) at DC is 1, while H(z) at DC is H(z)|z=1 = 1∕(1 − e−aTs)≠1. Now, one can choose a scaling factor g to create a new H(z) = g∕(z − e−aTs) that has H(z)|z=1 = H(s)|s=0 = 1.\n\n\n\nIn this case, the scaling factor is g = 1 − e−aTs and the new H(z) is\n\n\n\n\n\n\n\n\n\n H(z) = 1 − e−aTs z − e−aTs = (1 − e−aTs)z−1 1 − e−aTsz−1 . \n\n\n(3.57)\n\n\n\n\n\n\nThe extra delay z−1 imposed by the numerator of H(z) in Eq. (3.57) is often eliminated and the final result is\n\n\n\n\n\n\n\n\n\n H(z) = 1 − e−aTs 1 − e−aTsz−1, \n\n\n(3.58)\n\n\n\n\n\n\nwhich minimizes the delay in the corresponding difference equation.    □\n\n\n\n\n\nPole-zero placement\n\n\n\nPole-zero placement is a filter design technique that is closely related to the matched Z-transform. Instead of converting H(s) into H(z), the pole-zero placement consists in directly providing the zeros and poles of H(z). The following example illustrates its use for designing a notch filter.\n\n\n\n Example 3.19. Design a 2nd-order notch filter to minimize the impact of interference at a given frequency. The value f0 of the frequency used by electrical power systems depends on the country and is typically f0 = 50 or 60 Hz. The power line signal can cause strong interference at this specific frequency and its harmonics. For instance, a circuit that digitizes an ECG signal may have to filter out the interference at f0 to obtain better results. Here we are interested in using pole-zero placement to design a notch filter to reduce the power at f0. Four project guidelines will be used:\n\n &lt;ul class='itemize1'&gt;\n &lt;li class='itemize'&gt;The filter will have two complex-conjugate zeros &lt;!-- l. 1889 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\n and &lt;!-- l. 1889 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;z&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\n and two complex-conjugate poles &lt;!-- l. 1889 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\n and &lt;!-- l. 1889 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;.\n &lt;/li&gt;\n &lt;li class='itemize'&gt;The digital frequencies &lt;!-- l. 1890 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mi mathvariant='normal'&gt;Ω&lt;/mi&gt;&lt;/math&gt;\n (the angles) of poles and zeros are the same. Given Eq. (&lt;a href='ak_dsp_bookse7.html#x14-39001r22'&gt;1.22&lt;!-- tex4ht:ref: eq:freqdiscrete2continuous  --&gt;&lt;/a&gt;), which can be\n written as &lt;!-- l. 1891 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi mathvariant='italic'&gt;πf&lt;/mi&gt; &lt;mo class='MathClass-rel' stretchy='false'&gt;=&lt;/mo&gt; &lt;mi mathvariant='normal'&gt;Ω&lt;/mi&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/math&gt;,\n the frequency &lt;!-- l. 1892 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\n specified in Hz can be mapped to the digital domain as &lt;!-- l. 1892 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi mathvariant='normal'&gt;Ω&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt; &lt;mo class='MathClass-rel' stretchy='false'&gt;=&lt;/mo&gt; &lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo class='MathClass-bin' stretchy='false'&gt;∕&lt;/mo&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/math&gt;,\n in radians.\n &lt;/li&gt;\n &lt;li class='itemize'&gt;The zeros have magnitudes equal to one because they must be located on the\n unit circle.\n &lt;/li&gt;\n &lt;li class='itemize'&gt;\n &lt;!-- l. 1894 --&gt;&lt;p class='noindent'&gt;The poles magnitude &lt;!-- l. 1894 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;/math&gt;\n                                                                          \n                                                                          \n depend on &lt;!-- l. 1894 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mstyle class='text'&gt;&lt;mtext class='textrm' mathvariant='normal'&gt;BW&lt;/mtext&gt;&lt;/mstyle&gt;&lt;/math&gt;,\n which is the 3 dB filter bandwidth. More specifically,\n &lt;!-- l. 1894 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;/math&gt;\n can be obtained from Eq. (&lt;a href='ak_dsp_bookse37.html#x46-147001r37'&gt;3.37&lt;!-- tex4ht:ref: eq:pole_bw_discretetime  --&gt;&lt;/a&gt;) as &lt;/p&gt;&lt;table class='equation-star'&gt;&lt;tr&gt;&lt;td&gt;\n &lt;!-- l. 1895 --&gt;&lt;math class='equation' display='block' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;\n                                          &lt;mi&gt;r&lt;/mi&gt; &lt;mo class='MathClass-rel' stretchy='false'&gt;=&lt;/mo&gt; &lt;msup&gt;&lt;mrow&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mo class='MathClass-bin' stretchy='false'&gt;−&lt;/mo&gt;&lt;mstyle class='text'&gt;&lt;mtext class='textrm' mathvariant='normal'&gt;BW&lt;/mtext&gt;&lt;/mstyle&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;mo class='MathClass-bin' stretchy='false'&gt;∕&lt;/mo&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;\n           &lt;/mrow&gt;&lt;/msup&gt;&lt;mo class='MathClass-punc' stretchy='false'&gt;,&lt;/mo&gt;\n\n\n\n\n\n &lt;!-- l. 1898 --&gt;&lt;p class='noindent'&gt;or from Eq. (&lt;a href='ak_dsp_bookse37.html#x46-147002r38'&gt;3.38&lt;!-- tex4ht:ref: eq:approximation_pole_bw_discretetime  --&gt;&lt;/a&gt;), which leads to &lt;/p&gt;&lt;table class='equation-star'&gt;&lt;tr&gt;&lt;td&gt;\n &lt;!-- l. 1899 --&gt;&lt;math class='equation' display='block' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;\n                                           &lt;mi&gt;r&lt;/mi&gt; &lt;mo class='MathClass-rel' stretchy='false'&gt;≈&lt;/mo&gt; &lt;mn&gt;1&lt;/mn&gt; &lt;mo class='MathClass-bin' stretchy='false'&gt;−&lt;/mo&gt; &lt;mfrac&gt;&lt;mrow&gt;&lt;mstyle class='text'&gt;&lt;mtext class='textrm' mathvariant='normal'&gt;BW&lt;/mtext&gt;&lt;/mstyle&gt;&lt;/mrow&gt; \nFs π. \n\n\n\n\n &lt;!-- l. 1902 --&gt;&lt;p class='noindent'&gt;The latter is a good approximation when &lt;!-- l. 1902 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;/math&gt;\n is in the range &lt;!-- l. 1902 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;[&lt;/mo&gt;&lt;mn&gt;0.9&lt;/mn&gt;&lt;mo class='MathClass-punc' stretchy='false'&gt;,&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;[&lt;/mo&gt;&lt;/mrow&gt;&lt;/math&gt;.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;\n\n\nThese guidelines lead to the following pairs of zeros: z1 = ejΩ0 and z2 = e−jΩ0, while the poles are p1 = rejΩ0 and p2 = re−jΩ0.\n\n\n\nListing 3.18 provides an example of notch filter design assuming f0 = 60 Hz.\n\n\n\nListing 3.18: MatlabOctaveCodeSnippets/snip_systems_notch.m\n\n\nf_notch=60.0; % notch frequency in Hz \nBW_3dB=4;  % 3 dB bandwidth in Hz \nFs=400; % sampling frequency in Hz \nOmega_0=2*pi*(f_notch/Fs); % digital angular frequency \n5r=exp(-(BW_3dB/Fs)*pi); % poles radius \nnotch_zeros = [1*exp(1i*Omega_0) 1*exp(-1i*Omega_0)]; \nnotch_poles = [r*exp(1i*Omega_0) r*exp(-1i*Omega_0)]; \nBz_notch=poly(notch_zeros); % numerator B(z) \nAz_notch=poly(notch_poles); % denominator A(z) \n10freqz(Bz_notch, Az_notch) % show frequency response\n  \n\n\nThe frequency response shows that this filter can attenuate the signal by more than 20 dB around the frequency f0. However, when one decreases BW to avoid filtering out components other than f0, the poles get closer to the unit circle, which may lead to unstable filters when implemented in an embedded system. For instance, if BW = 4 Hz, then r = 0.9691. If BW is decreased to BW = 0.1 Hz, then r = 0.9992 and the filter attenuation is rather small because the poles are almost canceling the zeros.    □\n\n\n\nSimilar to the matched Z-transform, the next filter design techniques assume H(s) is available.\n\n\n\n\n\nImpulse invariance\n\n\n\nThe impulse invariant transformation method is based on the criterion of sampling the continuous-time impulse response such that h[n] = Tsh(nTs), where h(t) and h[n] are the impulse responses of H(s) and H(z), respectively, with\n\n\n\n\n\n\n\n\n\n H(z) = TsZ{h(t)|t=nTs} \n\n\n(3.59)\n\n\n\n\n\n\nand Z{⋅} denoting the Z-transform.\n\n\n\nThe impulse invariant method is restricted to strictly proper H(s) (degree of numerator smaller than the denominator’s). Besides, special care must be exercised when H(s) has multiple-order poles.22\n\n\n\n Example 3.20. Example of H(s) to H(z) conversion using impulse invariance. Assume that Ts = 0.5 and H(s) = 4∕[(s + 2)(s + 3)]. Performing a partial fraction expansion leads to\n\n\n\n\n\n\n H(s) = 4 s + 2 +  − 4 s + 3, \n\n\n\n\n\n\nwhich, assuming a causal system (to define the ROC), corresponds to\n\n\n\n\n\n\n h(t) = L−1{H(s)} = 4[e−2t − e−3t]u(t). \n\n\n\n\n\n\nThe impulse invariance is obtained after a S/D process. Using the simplified notation of Example 1.22, one can write\n\n\n\n\n\n\n h[n] = Tsh(t)|t=nTs = 0.5 × 4  [ (e−2Ts ) n −  (e−3Ts ) n] u[n] \n\n\n\n\n\n\nand then, using a procedure similar to the one in Example 2.21, leads to\n\n\n\n H(z) = Z{h[n]}  = 2 1 − e−2Tsz−1 − 2 1 − e−3Tsz−1 = 2 1 − 0.3679z−1 − 2 1 − 0.2231z−1     = 0.2895z−1 1 − 0.5910z−1 + 0.0821z−2. (3.60)   \n\nNote that the poles s = −2 and s = −3 in the s-plane were converted to z = e−2Ts and z = e−3Ts in z-plane, respectively, which corresponds to using Eq. (2.48) of the matched Z-transform method. And the numerators of the parcels of H(z) are the pole residues scaled by Ts. Listing 3.19 illustrates the use of impinvar in this example.\n\n\n\nListing 3.19: MatlabOctaveCodeSnippets/snip_systems_impulseinvariance\n\n\nTs=0.5; %sampling period \nBs=4; As=poly([-2 -3]); % Define H(s)=Bs/As \n[Bz,Az] = impinvar(Bs,As,1/Ts);%impulse invariance conversion to H(z) \nt=0:Ts:10; ht=4*(exp(-2*t)-exp(-3*t)); %create h(t) to compare \n5hn=impz(Bz,Az,length(ht)); %discrete-time h[n] from H(z)=Bz/Az \nh_Error=Ts*ht(:) - hn(:); %error from column vectors \ndisp(['mean square error (MSE) = ' num2str(mean(h_Error.^2))]); \nplot(t,Ts*ht); hold on; plot(t,hn,'rx'); %compare curves\n  \n\n\nThe result of Listing 3.19 is a MSE = 7.7342e-34, which indicates that h[n] ≈ Tsh(nTs), as desired.   □\n\n\n\nWhen H(s) has only single-order poles, the impulse invariant transformation method can be summarized as:\n\n\n\nf 1.\n\n\na partial fraction expansion of H(s) to obtain the residue ri of each pole pi and\n\n\nf 2.\n\n\ncreate the parcels of H(z) with poles z = epiTs and respective numerators Tsri.\n\n\n\n\nIn this specific case, the impulse invariant and matched Z-transform methods lead to the same poles of H(z).\n\n\n\n\n\nStep invariance\n\n\n\nThe step invariant transformation method is similar to the impulse invariant but uses the step responses in continuous q(t) and discrete-time q[n] instead of the impulses responses. Similar to the impulse response h(t), which is the system output when the input is δ(t), the step response q(t) is the system output when the input is u(t). The goal of this transformation is to achieve\n\n\n\n\n\n\n\n\n\n q[n] = q(t)|t=nTs, \n\n\n(3.61)\n\n\n\n\n\n\nwhich can be written as\n\n\n\n\n\n\n\n\n\n Z−1  {H(z) 1 1 − z−1 } = L−1   {H(s)1 s }|t=nTs, \n\n\n(3.62)\n\n\n\n\n\n\nwhere the S/D process is denoted in the simplified notation of Example 1.22.\n\n\n\nEq. (3.62) used the fact that 1∕s and 1∕(1 − z−1) are the Laplace and Z transforms of u(t) and u[n], respectively. Taking the Z-transform of both sides of Eq. (3.62) leads to\n\n\n\n\n\n\n\n\n\n H(z) = Z  {L−1   {H(s)1 s }|t=nTs} (1 − z−1). \n\n\n(3.63)\n\n\n\n\n\n\nThe script snip_systems_stepinvariance.m illustrates the step invariance method for a single pole H(s) = a∕(s + a).\n\n\n\n\n\nBackward difference and forward difference\n\n\n\nThe backward difference and the forward difference methods consist in substituting s by (1 − z−1)∕Ts and (1 − z−1)∕(Tsz−1), respectively. They are motivated by the solution of differential equations using finite differences. For digital filter design these methods are outperformed by others, and have more pedagogical than practical importance.\n\n\n\n\n\nBilinear or Tustin’s\n\n\n\nIn the bilinear transform (also called Tustin’s) method, s is substituted by (2∕Ts)(z − 1)∕(z + 1).\n\n\n\nNow that all major IIR design techniques were discussed, the next subsection provides an example of each of them.\n\n\n\n\n\n3.11.4  Summary of methods to convert continuous-time system function into discrete-time\n\n\n\nIn previous sections, several methods to convert H(s) into H(z) were discussed. Table 3.6 summarizes them. The matched Z-transform in Table 3.6 used Eq. (3.58).\n\n\n\n\n\n\n\n\n\n\n\nTable 3.6: Methods to convert H(s) into H(z).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMethod\n\n       &lt;/td&gt;&lt;td class='td11' id='TBL-106-1-2' style='text-align:center; white-space:nowrap;'&gt;                                                                             &lt;div class='multirow'&gt;&lt;!--  rows=2  --&gt;\nMapping\n\n\n\n\n\n\nH(z) when\n\n\n\n\n\n\n\n\n\n\nH(s) = a s+a\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMatched Z-transform\n\n\n\nMap poles or zeros at \n\n\n\n\n\n(1−e−aTs)z−1 1−e−aTsz−1 \n\n    &lt;/td&gt;\n\n\n\n\n\n\ns = a to z = eaTs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImpulse-invariant \n\n\n\nH(z) = TsZ  {L−1   {H(s)}|t=nTs}\n\n\n\n Tsa 1−e−aTsz−1 \n\n\n\n\n\n\n\n\n\n\n\n\nStep-invariant \n\n\n\nH(z) = Z  {L−1   {H(s)1 s }| t=nTs} (1 − z−1)\n\n\n\n(1−e−aTs)z−1 1−e−aTsz−1 \n\n\n\n\n\n\n\n\n\n\n\n\nBackward difference \n\n\n\ns = 1−z−1 Ts \n\n\n\n a 1−z−1 Ts +a\n\n\n\n\n\n\n\n\n\n\n\n\nBilinear \n\n\n\ns = 2 Ts  (1−z−1 1+z−1 )\n\n\n\n a  2 Ts  (1−z−1 1+z−1 )+a\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue to its importance, the bilinear transform is further discussed in the next section.\n\n\n \n\n21 As discussed in [?], a variation of the method maps poles and zeros at ∞ to z = −1.\n\n \n\n22 See [?,?], which discuss pitfalls of Matlab’s impinvar function. You can also try using As=poly([-2 -2 -3]) in Listing 3.19 to observe that impinvar does not work well with a multiple-order pole (s = −2 in this case).\n\n                                                                                                                &lt;/div&gt;"
  },
  {
    "objectID": "ak_dsp_bookse42.html",
    "href": "ak_dsp_bookse42.html",
    "title": "42  Bilinear Transformation",
    "section": "",
    "text": "The most popular approach to designing conventional lowpass, highpass, bandpass and bandstop IIR filters is the bilinear transformation because it is capable of preserving in H(z), most of the good features of the original H(s). In this method, the discrete-time filter is obtained by:\n\n\n\n\n\n\n\n\n\n H(z) = H(s)|s= 2 Ts z−1 z+1  = H(s)|s=2F sz−1 z+1 . \n\n\n(3.64)\n\n\n\n\n\n\nAs depicted in Figure 3.33, the sampling frequency Fs can assume any value when the bilinear is used to design a digital filter. In contrast, if a digital controller G(z) must be obtained from a continuous-time version G(s) to properly control a given plant H(s), the value of Fs must be chosen according to the characteristics of the involved signals and impact the hardware (ADC and DAC) used in the actual digital control system deployment.\n\n\n\n\n\n\n\n\nFigure 3.33: The choice of the sampling frequency Fs in the bilinear transformation depends on the application, and can be arbitrary in the case of digital filter design.\n\n\n\n\n\nIn the following paragraphs, it will be assumed that Fs is not arbitrarily chosen, until we discuss Section 3.12.3. The following examples illustrate the bilinear transform.\n\n\n\n Example 3.21. Examples of bilinear transformations. Assuming Fs = 1000 Hz, the bilinear transform of H(s) = 1 s+2 (used in Figure 3.12) is\n\n\n\n\n\n\n H(z) = 1 2Fs(z−1) z+1 + 2 = 1 2002  ( z + 1 z − 1998 2002 ) = 4.995 × 10−4  ( z + 1 z − 0.998 ). \n\n\n\n\n\n\nThis operation can be very tedious when the order of H(s) is high. But Matlab/Octave has the handy bilinear function. Because it differs from Octave’s, the Matlab’s bilinear syntax will be adopted here unless otherwise stated.23 The previous result could be obtained with [HzNum,HzDen]=bilinear(1,[1 2],1000). Alternatively, it is possible to work with poles and zeros: [HzZeros,HzPoles,gain]=bilinear([],-2,1,1000).\n\n\n\nAs a second example, consider the conversion of H(s) = (s − 3)∕(s2 + 4s + 5) (the poles are  − 2 ± j and the zero is 3) assuming Fs = 10 Hz:\n\n\n\n\n\n\n H(z) =  2 Ts z−1 z+1 − 3   ( 2 Ts z−1 z+1 ) 2 + 4  ( 2 Ts z−1 z+1 ) + 5 = 0.035 − 0.012z−1 − 0.047z−2 1 − 1.629z−1 + 0.670z−2 \n\n\n\n\n\n\nwhich can be obtained with the command [HzNum,HzDen]=bilinear([1 -3],[1 4 5],10).An alternative call would be [HzZeros,HzPoles,gain]=bilinear(3,transpose([-2+j -2-j]),1,10) that leads to\n\n\n\n\n\n\n\n\n\n H(z) = 0.035(z + 1)(z − 1.353) (z − 0.81 + j0.082)(z − 0.81 − j0.082). \n\n\n(3.65)\n\n\n\n\n\n\nBut note that when using the function bilinear with poles and zeros, one needs to use column instead of the row vectors used with transfer functions.    □\n\n\n\n Example 3.22. Bilinear transform of second order H(s) systems using Matlab’s symbolic math. The second order H(s) given by Eq. (3.30) can be converted to a discrete-time version H(z) using Listing 3.20.\n\n\n\nListing 3.20: MatlabOctaveCodeSnippets/snip_systems_sos_bilinear.m\n\n\n%define s, z, natural frequency, damping ratio and sampling interval \nsyms s z wn zeta Ts %all as symbolic variables \nHs=wn^2/(s^2+2*zeta*wn*s+wn^2) %H(s). Use pretty(Hs) to see it \n%Hs=(2*zeta*wn*s+wn^2)/(s^2+2*zeta*wn*s+wn^2) %Another H(s) \n5Hz=subs(Hs,s,(2/Ts)*((z-1)/(z+1))) %bilinear: s &lt;- 2/Ts*(z-1)/(z+1) \nHz=simplify(Hz) %simplify the expression \npretty(Hz) %show it using an alternative view\n  \n\n\nThe bilinear transform of Eq. (3.30) is obtained as\n\n\n\n\n\n\n\n\n\n H(z) = ωn2Ts2(z + 1)2 (4 + 4ζωnTs + (ωnTs)2)z2 + (2(ωnTs)2 − 8)z + 4 − 4ζωnTs + (ωnTs)2, \n\n\n(3.66)\n\n\n\n\n\n\nwhich can be simplified using Eq. (1.22), i. e., substituting ωnTs = Ωn to obtain\n\n\n\n\n\n\n\n\n\n H(z) = Ωn2(z + 1)2 (4 + 4ζΩn + Ωn2)z2 + (2Ωn2 − 8)z + 4 − 4ζΩn + Ωn2. \n\n\n(3.67)\n\n\n\n\n\n\nSimilarly, Listing 3.20 can be used to find the bilinear transform of Eq. (3.33) as\n\n\n\n\n\n\n\n\n\n H(z) = (4ζΩn + Ωn2)z2 + 2Ωn2z + Ωn(Ωn − 4ζ) (4 + 4ζΩn + Ωn2)z2 + (2Ωn2 − 8)z + 4 − 4ζΩn + Ωn2. \n\n\n(3.68)\n\n\n\n\n\n\nListing 3.21 can be used to verify Eq. (3.66) and Eq. (3.68)\n\n\n\nListing 3.21: MatlabOctaveCodeSnippets/snip_systems_check_sosbilinear.m\n\n\nwn=2; %natural frequency in rad/s \nzeta=0.5; %damping ratio \nFs=10*wn/(2*pi); %Fs chosen as 10 times the natural freq. in Hz \nWn=wn*(1/Fs); %convert angular freq. from continuous to discrete-time \n5if 1 %Hs=wn^2/(s^2+2*zeta*wn*s+wn^2) \n    [Bz,Az]=bilinear(wn^2,[1 2*zeta*wn +wn^2],Fs) %to compare below \n    Bz2=Wn^2*[1 2 1]; %expression from this textbook \n    Az2=[4+4*zeta*Wn+Wn^2 2*Wn^2-8 4-4*zeta*Wn+Wn^2]; %from textbook \nelse %Hs=(2*zeta*wn*s+wn^2)/(s^2+2*zeta*wn*s+wn^2) %Another H(s) \n10    [Bz,Az]=bilinear([2*zeta*wn wn^2],[1 2*zeta*wn +wn^2],Fs)%compare \n    Bz2=[4*zeta*Wn+Wn^2 2*Wn^2 Wn*(Wn-4*zeta)]; %from textbook \n    Az2=[4+4*zeta*Wn+Wn^2 2*Wn^2-8 4-4*zeta*Wn+Wn^2]; %from textbook \nend \nBz2=Bz2/Az2(1), Az2=Az2/Az2(1) %normalize as done by bilinear.m\n  \n\n\nTogether, Listing 3.20 and Listing 3.21 indicate the power of combining symbolic math and numerical functions in Matlab.    □\n\n\nDerivation of the bilinear transform\n\n\n\nThere are several ways to explain and/or motivate the bilinear method. One of them is by modifying the matched Z-transform of Eq. (2.48), which relates z and s by z = esTs. Multiplying this relation by e−sTs∕2∕e−sTs∕2 and using the Taylor expansion ex = 1 + x + x2∕2 + … truncated to the first order ex ≈ 1 + x, leads to24\n\n\n\n\n\n\n z=defesTs  = esTs∕2 e−sTs∕2 ≈ 2 + sTs 2 − sTs. \n\n\n\n\n\n\nSolving for s, one obtains the bilinear transform:\n\n\n\n\n\n\n\n\n\n s ≈ 2 Ts  [z − 1 z + 1 ] \n\n\n(3.69)\n\n\n\n\n\n\nand, therefore, the conversion is performed with Eq. (3.64).\n\n\n\nThe approximation ex ≈ 1 + x requires x to be close to 0, which in the bilinear transform corresponds to having sTs ≈ 0, i. e., having Ts small enough (a high sampling frequency) for the values of s of interest.\n\n\n\n\n\n3.12.1  Bilinear mapping between s and z planes and vice-versa\n\n\n\nEq. (3.64) imposes a mapping between the s and z planes that depends only on Fs. The code ak_map_s_into_z.m and ak_map_z_into_s.m allows to explore the mapping from s into z and vice-versa, respectively. The first mapping (from Eq. (3.64)) is\n\n\n\n\n\n\n\n\n\n s = 2 Ts z − 1 z + 1 \n\n\n(3.70)\n\n\n\n\n\n\nand the mapping from s to z can be derived by isolating z:\n\n\n\n\n\n\n\n\n\n z = 2 + Tss 2 − Tss. \n\n\n(3.71)\n\n\n\n\n\n\n Example 3.23. Interpreting the mapping imposed by the bilinear transform. According to Eq. (3.64), the bilinear transform obtains the value of H(z0) for a given z0, from the value of H(s)|s= 2 Ts z0−1 z0+1 . For instance, assuming z0 = 3 + j4 and Ts = 0.5 seconds, then H(z0) = H(s)|s=0.75+j0.25. To avoid confusion and disambiguate H(z) and H(s), it is useful to use a subindex and denote them Hz(z) and Hs(s), respectively. The result of the previous example can now be conveniently written as Hz(3 + j4) = Hs(0.75 + j0.25).    □\n\n\n\n                                   &lt;div class='center'&gt;\n\n\n\n\n                         &lt;div class='subfigure'&gt;&lt;table&gt;&lt;tr&gt;&lt;td style='text-align:left'&gt;&lt;img alt='PIC' src='Figures/bilinear_map1.png' /&gt;\n\n(a) z in unit circle, Fs = 8 Hz"
  },
  {
    "objectID": "ak_dsp_bookse43.html",
    "href": "ak_dsp_bookse43.html",
    "title": "43  FIR Filter Design",
    "section": "",
    "text": "3.13.1  A FIR filter does not have finite poles\n\n\n\nEssentially, most FIR filters are non-recursive and have a transfer function that can be written as H(z) = B(z), i. e., the denominator is A(z) = 1 in Eq. (3.52). Given this denominator, all poles of FIR filters are located in z = 0 or |z| = ∞ (while IIR filters do not have this restriction). For example, H(z) = 1 + z−1 + z−2 has a pole of order (or multiplicity) 2 at z = 0. You can check that with zplane([1 1 1],1) on Matlab/Octave30 or note it by rewriting as H(z) = (z2 + z + 1)∕z2. Another example is the noncausal system H(z) = z2 + z + 1, which has a pole of multiplicity 2 at infinity.31\n\n\n\n\n\n3.13.2  The coefficients of a FIR coincide with its impulse response\n\n\n\nAn interesting property of FIR filters is that the coefficients of H(z) = B(z) (also called “taps”) are exactly the non-zero values of the filter’s impulse response h[n]. For example, using the inverse Z-transform on H(z) = 3 + 4z−1 + 5z−2, one obtains h[n] = Z−1{H(z)} = 3δ[n] + 4δ[n − 1] + 5δ[n − 2].\n\n\n\n\n\n3.13.3  Algorithms for FIR filter design\n\n\n\n\n\n\n\n\nFigure 3.46: Mask for a differentiator filter specified with the syntax for arbitrary filter magnitudes.\n\n\n\n\n\nThere are several algorithms for both FIR and IIR. Instead of using pre-defined functions such as Butterworth’s, a FIR design is typically done with alternative methods, which seek to optimize a given criterion. The most common algorithm is the least-squares, which is implemented in Matlab/Octave via firls. Another algorithm is Parks-McClellan, implemented in Matlab’s firpm, but not available in Octave 3.2.3.\n\n\n\nThese and other optimization-based algorithms are executed according to a syntax that allows specifying an arbitrary function, not restricted to be lowpass, highpass, etc. Figure 3.46 illustrates the specification of a desired magnitude response of a differentiator, which has a gain in passband that is proportional to frequency. An arbitrary function will be specified in NB bands of interest. For each band b, four parameters are necessary: the start fbs and end fbe frequencies, the desired function values (the magnitude in this context) Abs and Abe at the start and end frequencies. An optional parameter is the weight of each band (e. g., W1 and W2 in Figure 3.46). The larger the weight, the higher the importance given to that band in the design process.\n\n\n\nIn Matlab/Octave, the function fir2 implements another algorithm for designing a FIR with arbitrary frequency response magnitude, using a different syntax. Instead of working with bands, fir2 allows the specification of the desired magnitude at frequencies of interest.\n\n\n\nAn example of using least-squares for FIR design with firls is discussed in the sequel.\n\n\n3.13.4  FIR design via least-squares\n\n\n\nThe least-squares method tries to minimize the total squared error between the desired function and the obtained approximation over the specified band. It is possible to give a different weight to each band. The larger the weight, the smaller the error in that band. A vector W with weights is an optional input parameter of firls.\n\n\n\nFor example, an ideal lowpass with cutoff Ω = π∕2 rad can be specified with NB = 2 bands. The (normalized) frequencies could be f1s = 0,f1e = 0.5,f2s = 0.5,f2e = 1 and the amplitudes A1s = 1,A1e = 1,A2s = 0,A2e = 0. These values are organized in vectors F=[0 0.5 0.5 1] and A=[1 1 0 0] and a M = 100-order FIR can be obtained with B=firls(100,F,A). Note that A and F have the same length, which must be\n\n2NB (even number), and W must have length equal to NB if specified. The default is to weight all bands equally, which corresponds to W with all elements equal to 1.\n\n\n\n\n\n\n\n\n\n\n\n(a) 1st passband prioritized\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) 2nd passband prioritized\n\n\n\n\n\n\n\nFigure 3.47: Frequency response of filters obtained with firls. In (a), the first passband had a larger weight than the second, while in (b) it was the opposite.\n\n\n\n\n\nFigure 3.47 compares the frequency responses of two filters obtained with firls. Figure 3.47(a) was obtained with Listing 3.26.\n\n\n\nListing 3.26: MatlabOctaveCodeSnippets/snip_systems_firls.m\n\n\nf=[0 0.25 0.3 0.55 0.6 0.85 0.9 1]; %frequencies \nA=[1 1 0 0 1 1 0 0]; %amplitudes \nM=20; %filter order \nW=[1000 1 1 1]; %weights, first passband is prioritized \n5B=firls(M,f,A,W); \nfreqz(B);\n  \n\n\nThe horizontal lines were superimposed to indicate the bands in which the gain should be 0 dB. Figure 3.47(b) was obtained with the same commands, but changing W to W=[1 1 1000 1]. Note that the impact of W is visible with the band that is not prioritized having larger deviations.\n\n\n3.13.5  FIR design via windowing\n\n\n\nThe windowing method for FIR design is implemented in fir1. In terms of computational cost, the method is very simple: the FIR impulse response h[n] is obtained by multiplying the impulse response of an ideal filter (which has an infinite duration) by a finite-length sequence w[n] called window with M + 1 non-zero samples. Recall that the coefficients B(z) of a FIR coincide with its h[n]. For the case of an ideal lowpass filter, the impulse response is given by\n\n\n\n\n\n\n\n\n\n hLP[n] = sin ⁡ (Ωcn) πn , \n\n\n(3.82)\n\n\n\n\n\n\nwhich is the inverse DTFT of a “rectangular pulse” in frequency-domain.\n\n\n\nThe impulse response for the ideal highpass filter can be obtained by noting that HHP(ejΩ) = 1 − HHP(ejΩ) and converting this to time-domain leads to\n\n\n\n\n\n\n\n\n\n hHP[n] = δ[n] − hLP[n]. \n\n\n(3.83)\n\n\n\n\n\n\nThe steps to obtain a FIR filter hw[n] of order M (assumed to be an even number here) via the windowing method are:\n\n\n\nf 1.\n\n\nObtain the impulse response for the corresponding ideal filter: h[n] has an infinite duration but the next steps will require only M + 1 of its coefficients.\n\n\nf 2.\n\n\nChoose a window: Pick a window w[n] with M + 1 non-zero coefficients in the range [−M∕2,M∕2].\n\n\nf 3.\n\n\nGet a filter with finite order M (i. e., M + 1 coefficients): Multiply the chosen window w[n] by the ideal filter impulse response h[n] to get a temporary\n\n\n\n\n\n\n\n\n hwt[n] = h[n]w[n]. \n\n\n(3.84)\n\n\n\n\n &lt;/dd&gt;&lt;dt class='enumerate-enumitem'&gt;&lt;span class='ec-lmbx-10x-x-109'&gt;f&lt;/span&gt;\n\n\n\nMake the filter causal: Delay hwt[n] such that the first hw[n] coefficient is at n = 0:\n\n\n\n\n\n\n\n\n hw[n] = hwt[n − M∕2]. \n\n\n(3.85)\n\n\n\n\n &lt;/dd&gt;&lt;/dl&gt;\n\n\nThe DTFT of h[n], w[n], hwt[n] and hw[n] are H(ejΩ), W(ejΩ), Hwt(ejΩ) and Hw(ejΩ), respectively. Figure 3.48 illustrates the effect of multiplying a signal (h[n] in this case) by a finite-duration window. This is a very useful approach for modeling the process of truncating an infinite-duration signal for further processing. This model allows to calculate, for instance, the spectrum of the finite-duration signal based on the convolution between the original and the window spectra. This is useful not only in FIR design, but many other situations such as segmenting a signal into frames.\n\n\n\n\n\n\n\n\n\nFigure 3.48: FIR design described in both time and frequency domains.\n\n\n\n\n\nIn windows-based FIR design, the filter frequency response Hw(ejΩ) is the result of the (circular) convolution between H(ejΩ) and W(ejΩ), as indicated in Figure 3.48. The ripples (ringing) observed in Hw(ejΩ) are located at the transitions of H(ejΩ) and indicate that the main lobe of W(ejΩ) dictates the transition band and the amplitude of W(ejΩ) sidelobes determine the attenuation at the stopband.\n\n\n\nThe properties of the filter are completely dependent on the adopted window type. The most common windows are named after their inventors: Hamming, Blackman, Kaiser and Hann (also called the hanning window). Among these four, the Kaiser is the only one with an extra degree of freedom provided by a parameter β. The other three are completely specified by the order M.\n\n\n\nThe rectangular window is useful to better understand the windowing process. The rectangular is 1 for M + 1 samples and 0 otherwise. For example, a 4-th order lowpass filter with Ωc = π∕3 can be designed with a rectangular windows according to Listing 3.27.\n\n\n\nListing 3.27: MatlabOctaveCodeSnippets/snip_systems_rectangular_window.m\n\n\nN=2 %the FIR order is M=2N in this case \nn=-N:N %time indices \nwc = pi/3 %specified cutoff frequency \nhn = sin(wc * n) ./ (pi * n) %ideal filter response \n5hn(N+1)=wc/pi %correct undetermined value hn=0/0 at n=0 \nhn = hn/sum(hn) %normalize filter to have gain 1 at DC \nB=fir1(2*N,wc/pi,rectwin(2*N+1)) %alternatively, use fir1 \nmax(abs(hn-B)) %compare hn with B obtained with fir1\n  \n\n\nThere is support in Matlab/Octave for obtaining windows other than the rectangular. For example, the command w=window((hamming?),10) obtains a Hamming window with 10 non-zero samples, which can be used to obtain a FIR of order M = 9 with cutoff frequency Ω = π∕2 rad via the command B=fir1(9,0.5,w).\n\n\n\nThe adopted window determines the frequency response of the associated FIR designed via windowing. Section 4.3 discusses the characteristics of windows in more details.\n\n\n\nOne tricky feature of fir1 is that it generates filters with cutoff frequency Ωc that corresponds to a gain of  − 6 dB\n(0.5 in linear scale). In contrast, the cutoff frequency of a filter obtained with butter and other IIR filter design functions corresponds to a gain of  − 3 dB (1∕2 in linear scale). Another aspect is that, if the order M is too small when using fir1, the gain at Ωc may not reach  − 6 dB. This also happens with other filter design procedures.\n\n\n3.13.6  Two important characteristics: FIRs are always stable and can have linear phase\n\n\n\nBecause practical FIR filters have all M + 1 coefficients with finite values, Eq. (B.122) indicates that all FIR filters are BIBO stable. Besides, FIR filters can easily have linear phase, which is a desirable property in many applications (such as telecommunications, as will be discussed in Application 3.13). It can be proved that the linear phase is achieved if and only if there is symmetry in h[n] about some value ns. For example, a FIR H(z) = 1.5 − 2z−1 + 5z−2 − 2z−3 + 1.5z−4 of order M = 4 has 5 non-zero samples in h[n] and exhibit symmetry about ns = 2, which corresponds to h[2] = 5, such that h[n] = h[4 − n]. The phase is linear over frequency because, for the given example:\n\n\n\n H(ejΩ)  = F{h[n]} (3.86)   = F{1.5δ[n] − 2δ[n − 1] + 5δ[n − 2] − 2δ[n − 3]     + 1.5δ[n − 4]} (3.87)   = 1.5 − 2e−jΩ + 5e−j2Ω − 2e−j3Ω + 1.5e−j4Ω (3.88)   = e−j2Ω  (1.5ej2Ω − 2ejΩ + 5 − 2e−jΩ + 1.5e−j2Ω) (3.89)   = e−j2Ω  [1.5(ej2Ω + e−j2Ω) − 2(ejΩ + e−jΩ) + 5] (3.90)   = e−j2Ω  [3cos ⁡ (2Ω) − 4cos ⁡ (Ω) + 5] (3.91)   = e−j2ΩA(Ω), (3.92)  \n\nwhere A(Ω) = 3cos ⁡ (2Ω) − 2cos ⁡ (Ω) + 5 is a real function that provides the magnitude |H(ejΩ)|, such that the phase e−j2Ω of H(ejΩ) is linear  − 2Ω, given by the factor .\n\n\n\nAs Eq. (3.92) indicates, a symmetric FIR can always be decomposed as\n\n\n\n\n\n\n H(ejΩ) = e−jΩτg A(Ω), \n\n\n\n\n\n\nwhere A(Ω) is a real function and τg is the group delay, to be further discussed along with Eq. (3.47).\n\n\n\n\n\n3.13.7  Examples of linear and non-linear phase filters\n\n\n\nWhen the system has linear phase it is relatively easy to calculate its group delay because it coincides with the slope of a straight line. Listing 3.28 uses the FIR from Eq. (3.92) to show an example of calculating the slope from two points k1 and k2, comparing it with the result\nof the grpdelay function:\n\n\n\nListing 3.28: Group delay estimation for linar phase system\n\n\nh=[1.5 -2 5 -2 1.5]; %coefficients of symmetric FIR \nN=8; %number of FFT points \nFs=44100; %sampling frequency (Hz) \nH=fft(h,N); %calculate FFT of N points \n5f=Fs/N*(0:N/2); %create abscissa in Hz, where Fs/N is the bin width \np = unwrap(angle(H(1:N/2+1))); %calculate the phase in rad \nplot(f,p), xlabel('f (Hz)'), ylabel('Phase (rad)'), pause \nk1=2; k2=4; %choose two points of the line \n%Calculate derivative as the slope. Convert from Hz to rad/s first: \n10groupDelay = -atan2(p(k2)-p(k1),2*pi*(f(k2)-f(k1))) %in seconds \ngroupDelayInSamples=round(2*groupDelay*Fs)/2; %quantize with step=0.5 \ngrpdelay(h,1,f(1:N/2+1),Fs); %find delay for positive frequencies\n  \n\n\nLine 11 used round to avoid numerical errors. The trick of multiplying by 2 and after rounding divide by 2 allows to take into account that a symmetric FIR, unless the estimation was troubled due to the existence of zeros on the unit circle, has group delay that can be represented with a quantizer of step Δ = 0.5 sample.\n\n\n\nFigure 3.49 and Figure 3.50 were obtained by executing Listing 3.29 for the filters [0.3 -0.4 0.5 0.8 0.5 -0.4 0.3] and [0.3 -0.4 0.5 0.8 -0.2 0.1 0.5], respectively.\n\n\n\nListing 3.29: Code for plotting the group delay and phase.\n\n\nfunction ak_plotGroupDelay(B,A) \n% function ak_plotGroupDelay(B,A) \n%Plots both the group delay and phase using plotyy \ngd = grpdelay(B,A,128); %estimate the group delay \n5gd(1) = []; %avoid NaN at first sample \n[H,w] = freqz(B,A,128); %obtain DTFT at 128 points \nH(1) = []; w(1) = []; %keep the same size as gd vector \nphase = unwrap(angle(H)); %unwrap the phase for better visualization \nax=plotyy(w,gd,w,phase); %use two distinct y axes \n10ylabel(ax(1),'Group Delay (samples)'); ylabel(ax(2),'Phase (rad)'); \nxlabel('Frequency \\Omega (rad)'); grid;\n  \n\n\n\n\n\n\n\nFigure 3.49: Group delay and phase for a channel represented by a symmetric FIR with linear phase and constant group delay of 3 samples.\n\n\n\n\n\n\n\n\n\n\nFigure 3.50: Group delay and phase for a channel represented by a non-symmetric FIR h=[0.3 -0.4 0.5 0.8 -0.2 0.1 0.5] with non-linear phase.\n\n\n\n\n\nThe filter of Figure 3.49 has a linear phase while the one in Figure 3.50 does not. It can be seen from Figure 3.50 that even causal systems can have a negative group delay for some frequencies.\n\n\n3.13.8  Zeros close to the unit circle may impact the phase linearity\n\n\n\nAn often overlooked fact is that if A(Ω) approaches zero, the phase graph is not guaranteed to be visualized as a linear function of Ω. For example, the first filter (from Eq. (3.92)) in the script below shows a linear phase via the freqz function, but the second does not due to a zero at approximately e±j1.994:\n\n\nh1=[1.5 -2 5 -2 1.5]; freqz(h1); pause %has linear phase \nh2=[-0.7 6 4 6 -0.7]; freqz(h2); %does not have linear phase \nabs(roots(h2)), angle(roots(h2));%h2 has zeros on unit circle!\n\n\nThe function grpdelay properly treats the zeros on the unit circle and indicates τg = 2 samples for both h1 and h2 filters. Because conventional filters such as lowpass and bandpass do not have zeros at the passband, the symmetry of their coefficients guarantees linear phase within the band of interest and the discussed behavior often manifest itself only in the stopband.\n\n\n\nAnother way of interpreting the problem caused by zeros on the unit circle is to note that they lead to a zero magnitude. If the magnitude of a complex number is zero or close enough to zero, its phase will not be important.\n\n\n\n\n\n3.13.9  Four types of symmetric FIR filters\n\n\n\nDepending on the symmetry, there are four types of linear-phase FIR filters. The filter of Eq. (3.92) is classified as a linear-phase FIR of type I. In general, a type I has an order M that is even (the length of h[n] is odd) and is symmetric h[n] = h[M − n]. Table 3.8 summarizes all types.\n\n\n\n\n\n\n\nTable 3.8: Types of linear-phase FIR filters. It is assumed that the filters are causal with the first non-zero sample at n = 0. Type III filters have h[M∕2] = 0.\n\n\n\n\n\n\n\n\n\n\n\n\n\nType \n\n\n\n\nSymmetry\n\n\n\n\n\nOrder (M)\n\n\n\n\n\n\nH(ejΩ)|Ω=0 (DC)\n\n\n\n\n\n\nH(ejΩ)|Ω=π (Nyquist)\n\n\n\n\n\nI \n\n\n\n\nSymmetric: h[n] = h[M −n]\n\n\n\n\n\neven\n\n\n\n\n\nany value\n\n\n\n\n\nany value\n\n\n\n\n\nII \n\n\n\n\nSymmetric: h[n] = h[M −n]\n\n\n\n\n\nodd\n\n\n\n\n\nany value\n\n\n\n\n\n0\n\n\n\n\n\nIII \n\n\n\n\nAnti-symmetric: h[n] = −h[M −n]\n\n\n\n\n\neven\n\n\n\n\n\n0\n\n\n\n\n\n0\n\n\n\n\n\nIV \n\n\n\n\nAnti-symmetric: h[n] = −h[M −n]\n\n\n\n\n\nodd\n\n\n\n\n\n0\n\n\n\n\n\nany value\n\n\n\n\n\n\n\n\nRecall that, because H(ejΩ)|Ω=0 = H(z)|z=1, the behavior of a FIR filter at DC can be obtained with\n\n\n\n\n\n\n H(z)|z=1 = ∑ k=0Mb k = ∑ n=0Mh[n]. \n\n\n\n\n\n\nHence, the anti-symmetric filters in Table 3.8 are restricted to have a zero at DC. Similarly, at the Nyquist frequency Ω = π rad, H(ejΩ)|Ω=π = H(z)|z=−1, and for a type III FIR with order M even:\n\n\n\n H(z)|z=−1  = ∑ k=0Mb k = ∑ k=0M∕2[b 2k(−1)2k + b 2k+1(−1)2k+1]     = ∑ n=0M∕2(h[2n] − h[2n + 1]) = h[M∕2] = 0,    \n\nwhere the first parcels in the summations correspond to even (2k or 2n) and the second parcels to odd coefficients. This kind of relation is better visualized via examples as in Listing 3.30.\n\n\n\nListing 3.30: MatlabOctaveCodeSnippets/snip_systems_FIR_types.m\n\n\nhI=[1 2 3 2 1] %Type I FIR \nhII=[1 2 2 1]  %Type II FIR \nhIII=[1 2 0 -2 -1] %Type III FIR \nhIV=[1 2 -2 -1] %Type IV FIR \n5disp(['I: DC=' num2str(polyval(hI,1)) ... \n    ' Nyquist=' num2str(polyval(hI,-1))]) \ndisp(['II: DC=' num2str(polyval(hII,1)) ... \n    ' Nyquist=' num2str(polyval(hII,-1))]) \ndisp(['III: DC=' num2str(polyval(hIII,1)) ... \n10    ' Nyquist=' num2str(polyval(hIII,-1))]) \ndisp(['IV: DC=' num2str(polyval(hIV,1)) ... \n    ' Nyquist=' num2str(polyval(hIV,-1))])\n  \n\n\nRunning Listing 3.30 outputs\n\nI: DC=9 Nyquist=1\nII: DC=6 Nyquist=0\nIII: DC=0 Nyquist=0\nIV: DC=0 Nyquist=2\n\n\n\nas expected from Table 3.8. Because the gain at DC is zero, linear-phase FIR filters of type III and IV are useful to operate as differentiator filters, for example.\n\n\n\nFigure 3.51 shows the impulse and frequency responses for the four types of symmetric FIR filters exemplified in Listing 3.30. Note that the type II FIR in this case (hII=[1 2 2 1]) has three zeros on the unit circle (use abs(roots(hII))) and its phase has a discontinuity in spite of a constant group delay (use grpdelay(hII)), as warned in Section 3.13.8.\n\n\n\n\n\n\n\n\nFigure 3.51: Impulse and frequency responses for the four types of symmetric FIR filters exemplified in Listing 3.30.\n\n\n\n\n \n\n30 Note that Matlab shows the pole multiplicity in zplane while Octave version 3.2.3 does not.\n\n \n\n31 See [url3poi] for a brief discussion about poles at infinity.\n\n                                               &lt;/div&gt;"
  },
  {
    "objectID": "ak_dsp_bookse44.html",
    "href": "ak_dsp_bookse44.html",
    "title": "44  Realization of Digital Filters",
    "section": "",
    "text": "3.14  Realization of Digital Filters\n\n\n\nHaving H(z), the next step is to choose a realization. For example, the difference equation y[n] = 5x[n] − 5x[n − 1] could be implemented as y[n] = 5(x[n] − x[n − 1]) to save computation. In this case, instead of two multiplications followed by a subtraction, the alternative requires only one subtraction and one multiplication. However, a difference equation is not adequate to distinguish realizations. For example, algebraically, they are of course the same: y[n] = 5x[n] − 5x[n − 1] = 5(x[n] − x[n − 1]). In order to specify realizations of digital filters, a flow chart or diagram is used. The “delay block” is represented by z−1, such that its output is the previous value of the input. For example:\n\n\n\n\n\n\n x[n]→ z&lt;mrow  &gt;−1&lt;/msup  &gt; →x[n − 1] \n\n\n\n\n\n\nand\n\n\n\n\n\n\n y[n]→ z&lt;mrow  &gt;−1&lt;/msup  &gt; →z&lt;mrow  &gt;−1&lt;/msup  &gt; → z&lt;mrow  &gt;−1&lt;/msup  &gt; → y[n − 3]. \n\n\n\n\n\n\nThe output of each delay block needs to be stored in RAM memory, such that the number of these blocks should be minimized if one aims at a reduced memory requirement. The filter memory is the set of values taken from the output of the delay blocks, which are stored for using in the next iteration.\n\n\n\nOther blocks are the multiplier and adder. These three help to indicate the order of operations when implementing a difference equation. Figure 3.52 depicts the two realizations mentioned previously. A careful observation concludes that such diagrams still have some ambiguity with respect to the order of operations but they are more descriptive than a difference equation.\n\n\n\n\n\n\n\n\n\n\n\n(a) \n\n\n\n\n\n\n       \n\n\n\n\n\n\n\n(b) \n\n\n\n\n\n\n\nFigure 3.52: Two distinct realizations of y[n] = 5x[n] − 5x[n − 1].\n\n\n\n\n\nAt this point, it may not be clear that the value of y[n] can be distinct in the two cases of Figure 3.52. This may happen when one compares these two filter realizations using a finite number of bits to represent numbers. For example, assume that the hardware used to implement the equations provides only 8 bits to represent integer numbers in the range [−128,127] in two’s complement. Say that x[0] = 85 and x[1] = 92 and the goal is to obtain y[1]. Using the realization of Figure 3.52(b), the difference x[1] − x[0] = 92 − 85 = 7 is multiplied by 5 and y[n] = 35. In contrast, using Figure 3.52(a) leads to an overflow when calculating 5x[0] = 425, which would require at least 10 bits because converting from decimal to binary leads to 425D = 01 1010 1001B, with the most significant bit representing the sign. Therefore, unless the hardware was able to implement saturation and represent this product as 127, the result with finite precision would be 5x[0] = −87D = 1010 1001B. Similarly, 5x[1] = −52 when represented with 8 bits and, consequently, y[1] = −87 − (−52) = −35 in this case. This simple example should suffice to illustrate the importance of the filter realization when the precision is finite. There are many structures for obtaining realizations of digital filters. The most popular will be presented in the sequel.\n\n\n3.14.1  Structures for FIR filters\n\n\n\n                                   &lt;div class=\"center\" \n\n\n\n\n\n\n\n\n\n\n\n\n(a) Direct form\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Symmetric\n\n\n\n\n\n\n\nFigure 3.53: Two structures for FIR realizations."
  },
  {
    "objectID": "ak_dsp_bookse45.html",
    "href": "ak_dsp_bookse45.html",
    "title": "45  Multirate Processing",
    "section": "",
    "text": "3.15  Multirate Processing\n\n\n\nMany modern DSP systems adopt distinct sampling rates along the processing chain, using interpolators and decimators. Two basic blocks of these multirate processing systems are the upsampler and downsampler.\n\n\n\n\n\n3.15.1  Upsampler and interpolator\n\n\n\nThe upsampler has an output q[m] = x[m∕L] for any m that is an integer multiple of L and 0 otherwise. It is represented by\n\n\n\n\n\n\n x[n]→ ↑ L  →q[m]. \n\n\n\n\n\n\nNote that distinct indexes m and n are used to emphasize the sampling rates are not the same.\n\n\n\nThe upsampling by L is not a time-invariant operation (in spite of being linear), so the system is not LTI. When the input is a WSS process, the output process cannot be assumed WSS.\n\n\n\nThe Fourier transform of the upsampler output is given by:\n\n\n\n\n\n\n\n\n\n Q(ejΩ) = X(ejLΩ), \n\n\n(3.94)\n\n\n\n\n\n\nwhich corresponds to scaling the abscissa Ω by the factor L such that Q(ejΩ) is a compressed version34 of X(ejΩ).\n\n\n\nFigure 3.63 provides an example where the original spectrum X(ejΩ) is an ideal lowpass filter with cutoff frequency Ωc = π∕4 rad. After upsampling by L = 4, the resulting Q(ejΩ) has not a lowpass spectrum anymore. In fact, the new “high frequency” components in Q(ejΩ) are required to enable the amplitudes of the upsampler output q[m] to vary from/to zero between two original samples of the (relatively “smooth”) input x[n].\n\n\n\n\n\n\n\n\nFigure 3.63: Original spectrum X(ejΩ) (top) and its upsampled version Q(ejΩ) = X(ejLΩ) with L = 4 (bottom). For convenience, the abscissa was normalized by π such that the value (in red) of X(ejΩ) at Ω = 2π rad (f = 2) appears in Ω = π∕2 rad (f = 0.5) for Q(ejΩ).\n\n\n\n\n\nNotice that there is no aliasing or change in spectrum amplitude involved in this process, but within [−π,π[ rad, Q(ejΩ) has L versions of what was the original X(ejΩ) in the range [−π,π[ rad as emphasized in Figure 3.64. In this example, with L = 4, the equivalent of four replicas of the original spectrum can be found within [−π,π[ rad: the ones centered at f = −0.5,0,0.5 and half of the replicas in f = −1 and 1.\n\n\n\n                                   &lt;div class=\"center\" \n\n\n\n\n\n\n\n\n\nFigure 3.64: Zoom of the bottom plot of Figure 3.63: due to the upsampling by &lt;mi L = 4, &lt;mi Q(&lt;mrow &lt;mi e&lt;mrow &lt;mi jΩ) has four replicas of the original lowpass spectrum."
  },
  {
    "objectID": "ak_dsp_bookse46.html",
    "href": "ak_dsp_bookse46.html",
    "title": "46  Applications",
    "section": "",
    "text": "3.16  Applications\n\n\n\n Application 3.1. Implementing a “universal” analog filter using the PC sound board and Playrec. This application practices the system illustrated in Figure 3.7, using a PC to implement the system H(z) and the sound board for A/D and D/A conversions. It relies on a sound library to continuously read input samples from the ADC and send output samples to the DAC. There are many libraries for doing that, such as Java Sound, which has the advantage of a clean API and running on Windows, Linux and other operating systems. But a requirement for this example was to implement the signal processing in Matlab and Octave. Because of that, Playrec [url1pre] was adopted. The code based on Playrec is not optimized for this application because it continuously and dynamically allocates and frees memory. A more optimized implementation would use something as a circular buffer. But Playrec is flexible in the sense that allows using all Matlab/Octave powerful functions and can be compiled to both. In our case, the experiments were done using Playrec compiled for Octave on Linux. The reader is referred to Playrec’s web site at [url1pre] for installation instructions and it is assumed from now on that Playrec has been already installed and tested.\n\n\n\nListing 3.38 lists the first lines of the initialization code, where the sampling frequency Fs is defined. Hence, the application bandwidth is Fs∕2. It should be mentioned that the values of the two first variables in Listing 3.38 may vary among different computers. The user must use Playrec’s scripts select_play and select_rec to investigate the proper identifiers (ID’s) in his/her system.\n\n\n\nListing 3.38: MatlabOctaveThirdPartyFunctions/ak_universalChannelInitialization.m\n\n\nplayDeviceID=10; %the device ID, choose it using select_play... \nrecDeviceID=10; %the device ID, choose it using select_rec... \nplayChan=1; %number of channels used for playback \nrecChan=1; %number of channels used for recording \n5Fs=44100; %sampling frequency \ndebug=1; %use 1 to show the number of skipped samples or 0 otherwise \n%other definitions follow below:  ...\n  \n\n\nListing 3.39 provides an example of filtering using Playrec. Note that the first instruction is to run Listing 3.38 to initialize Playrec. Just after that, a Butterworth filter of order 30 is designed, with cutoff frequency\n\nFs∕4 Hz, which corresponds to Ω = π∕2 rad (or 0.5 after normalizing by π, as required by Matlab/Octave). The array memory is created in the next line, to store the samples in the filter’s memory. The actual filtering processing is in line 23, within an eternal loop, and repeatedly updates memory.\n\n\n\nListing 3.39: MatlabOctaveThirdPartyFunctions/ak_universalChannel1.m\n\n\n%Example of LTI channel implemented as H(z)=B(z)/A(z), below \nak_universalChannelInitialization %initialize sound card acquisition \n%(script above defines Fs and sound recording/playback parameters) \n[B,A]=butter(30,0.5); %design some filter to play the role of channel \n5[ans, memory] = filter(B,A,zeros(1,100)); %initialize filter's memory \nwhile true %script runs until interrupted (by CTRL + C, for example) \n    %-1 is to record the same number of samples in playSampleBuffer \n    newPageNumber = playrec('playrec', playSampleBuffer, playChan,... \n        -1, recChan); \n10    pageNumList = [pageNumList newPageNumber]; %add to end of list \n    if(runMaxSpeed) %consumes more CPU \n        while(playrec('isFinished', pageNumList(1)) == 0) \n        end \n    else %less CPU-demanding \n15        playrec('block', pageNumList(1)); %blocks until recording \n    end \n    recSamples = playrec('getRec', pageNumList(1)); %get rec. samples \n    playrec('delPage', pageNumList(1)); %delete the current page \n    pageNumList = pageNumList(2:end); %exclude page number from list \n20    if (~isempty(recSamples)) %DSP code section \n        %Your DSP code must go here, inside the 'if' instruction. The \n        %recSamples and playSampleBuffer store the input and output \n        %channel samples, respectively. \n        [playSampleBuffer, memory] = filter(B,A,recSamples,memory); \n25    end \n    if debug == 1 %send to stdout (if outside the loop, it would not \n        fprintf('%d samples skipped!\\n', ... %print due to CTRL+C) \n            playrec('getSkippedSampleCount')); \n    end \n30end \nplayrec('delPage'); %clean exit, but not executed due to user CTRL+C\n  \n\n\nExpanding the setup in Application 1.4 to use two computers, with audio cables to connect the ADC and DAC of the first to the DAC and ADC of the second, respectively, it is possible to implement and test several DSP algorithms. For example, while one computer implements the analog filter based on H(z), Audacity can be used to playback a file with a signal x[n] composed by the sum of two sinusoids to be the filter input, and simultaneously obtain the filter output y[n].\n\n\n\nAnother example is suggested in Listing 3.40, which shows how to add random noise to the filter’s output.\n\n\n\nListing 3.40: MatlabOctaveThirdPartyFunctions/ak_universalChannel2.m\n\n\n    if (~isempty(recSamples)) %DSP code section \n        %Your DSP code must go here, inside the 'if' instruction. The \n        %recSamples and playSampleBuffer store the input and output \n        [playSampleBuffer, memory] = filter(B,A,recSamples,memory); \n25        noiseSamples = randn(size(noiseSamples)); %new noise samples \n        playSampleBuffer = playSampleBuffer + noiseSamples;%add noise \n    end\n  \n\n\nNote that the variable debug in Listing 3.38 can be used to evaluate if the system is capable of not dropping samples (given the chosen Fs and DSP processing).   □\n\n\n\n Application 3.2. Implementing a “universal” analog filter using the PC sound board and Java code. This application discusses another implementation of a universal channel. Instead of the Playrec Matlab/Octave code adopted in Application 3.1, Java is used here. The advantage of using Playrec is that the channel can be based on virtually all Matlab/Octave available functions. However, in many cases the channel just needs to implement a filter. In this situation, the task of compiling Playrec can be avoided. The\nportability of Java makes easier to run the filter using the provided code, which can have its source code modified to add noise, etc. Alternatively, the already compiled DigitalFilter.jar can be executed, which requires only having Java installed on your machine.\n\n\n\nThe DigitalFilter Java project can be found at folder Code/Java_Language of the companion software. The code can be executed from a command prompt, going to folder Code/Java_Language/DigitalFilter/dist and typing\n\n\njava -jar DigitalFilter.jar\n\n\nwhich will open the GUI depicted in Figure 3.66. The assumed sampling frequency is Fs = 44.1 kHz, which can be modified by editing the source code and recompiling.\n\n\n\n\n\n\n\n\nFigure 3.66: Screenshot of the DigitalFilter GUI after user informed the coefficients of the filter obtained with Matlab/Octave command [B,A]=butter(4,0.5).\n\n\n\n\n\nThe GUI has two text areas to specify the numerator B(z) (top area) and denominator A(z) (bottom area) of a filter’s system function H(z) = B(z)∕A(z). The coefficients must be separated by a blank space. Figure 3.66 shows the coefficients of an IIR filter obtained with Matlab/Octave command [B,A]=butter(4,0.5). The difference equation is implemented using a Direct Form II Transposed, as the filter function in Matlab/Octave.\n\n\n\nAfter clicking Change filter and then the Start button, the signal acquired by the sound board ADC will be continuously filtered by H(z) and the filter output sent to the sound board DAC, according to the scheme depicted in Figure 3.7.\n\n\n\n\n\n\n\n\nFigure 3.67: Result of experiment with Listing 3.41 and soundboards of two computers: an analog bandpass filter implemented with the canonical interface of Figure 3.7.\n\n\n\n\n\nFigure 3.67 shows the plot obtained with the Matlab code of Listing 3.41 (a modified version of Listing 1.26). Two computers were used and two audio cables connected the ADC (mic in) of the first computer to the DAC (speaker out) of the second computer and vice-versa. The first computer executed the DigitalFilter Java code while the second executed Listing 3.41. The gains (volumes) of both computers were adjusted to avoid signal saturation. The filter that the Java code implemented was obtained with the command [B,A]=cheby1(4,1,[0.2 .5]) (the coefficients are not the ones in Figure 3.66), which corresponds to a bandpass filter.\n\n\n\nListing 3.41: MatlabOnly/snip_systems_realtimeLoopback.m\n\n\nFs = 44100;   %sampling rate, must be the same as Java code \nnumSamples = Fs/2; %number of samples for DAC and ADC of soundboard \nwhile 1 %eternal loop, break it with CTRL+C \n    outputSignal=randn(numSamples,1); %generate white Gaussian noise \n5    wavplay(outputSignal, Fs, 'async');  %non-blocking playback \n    inputSignal  = wavrecord(numSamples, Fs, 'int16'); %record \n    subplot(211), plot(inputSignal); axis tight %graph in time domain \n    subplot(212), pwelch(double(inputSignal)); %in frequency domain \n    drawnow %Force the graphics to update immediately inside the loop \n10end\n  \n\n\nThe shape of this bandpass filter can be observed in Figure 3.67 but the stopband attenuation is not as good as the theoretical one (obtained with freqz, for example). A caveat of the experiment can be observed from the top plot of Figure 3.67, which shows the signal acquired by the ADC of the second computer. The zero-valued samples in the middle of the noise signal are due to Matlab not being able to output WGN samples (Example 1.50 discusses WGN), record the input signal and performing the extra processing without this delay. A workaround is to use another software to output the WGN and use Matlab only to record the samples, process them and plot the results.\n\n\n\n\n\n\n\n\nFigure 3.68: Result similar to Figure 3.67 but without silence intervals in acquired signal (top plot).\n\n\n\n\n\nFigure 3.68 illustrates the result of a second experiment, where Audacity was used to send WGN samples to the DAC of the second computer. This can be conveniently done by using Audacity’s menu Generate and choosing Noise -&gt; White. Then, the option Loop Play of menu Transport can be used to conveniently play the noise in an eternal loop. The second computer also executed Listing 3.41 but with its fourth and fifth lines commented out such that Matlab simply reads from the ADC and shows the plots. It can be seen that there are no silence gaps as occurred in Figure 3.67.\n\n\n\nThe user is invited to implement a system with larger attenuation in the stopband. One aspect is to use H(z) with larger order. Another aspect is to get a better estimate of the spectrum by changing the values of numSamples and input parameters of psd (or pwelch) in Listing 3.41. For example, it may help to increase the number of FFT points as discussed in Chapter 4.    □\n\n\n\n Application 3.3. Relating power in continuous and discrete-time when the reconstruction filter has unitary energy.\n\n\n\nIn Section 1.11, a ZOH reconstruction was assumed. Alternatively, if the reconstruction filter has an impulse response h(t) with unitary energy, a reasoning similar to the one adopted in Section 1.11 leads to the conclusion that the power is Pc = Pd∕Ts. Because of this, some textbooks denote Pd as the average energy (Joules) instead of power (Watts). This ambiguity can also be interpreted as a consequence of the fact that a discrete-time signal x[n] does not have explicit information about time. Hence, the interpretation of its power in Watts requires a model or assumption for the reconstruction filter h(t), which must incorporate the whole reconstruction process, including filters and amplifiers. In this text the assumed model is zero-order hold reconstruction.    □\n\n\n\n Application 3.4. Minimum phase systems. A LTI discrete-time system H(z) is causal and stable only if all its poles are inside the unit circle, but these two requirements do not impose restrictions on the zeros. The zeros of a causal and stable H(z) can be anywhere in the Z plane. In many cases, it is useful to impose that the zeros must also be inside the unit circle. For a continuous-time system H(s), the equivalent requirement is to have all zeros inside the left-part of the S plane. A minimum-phase system has this property.\n\n\n\nA LTI is minimum-phase if both the system\n\nH itself and its inverse 1∕H are causal and stable. Assuming, discrete-time, because the poles of the inverse 1∕H(z) are the zeros of H(z), only a system with all zeros inside the unit circle has a causal and stable inverse.\n\n\n\nA non-minimum-phase system Hnon(z) can always be transformed into a minimum-phase Hmin(z) that has the same magnitude, i. e., |Hnon(z)| = |Hmin(z)|. This is achieved by replacing the zeros of Hnon(z) that are outside the unit circle by their conjugate reciprocals.35 That is, replace all zeros z0 with |z0| &gt; 1 by 1∕z0∗. For example, if z0 = 2ejπ∕2 is a zero of Hnon(z), it should be replaced by 1∕z0∗ = 0.5ejπ∕2. The function ak_forceStableMinimumPhase.m implements this method and also decreases the magnitude of zeros on top of the unit circle.\n\n\n\n\n\n\n\n\nFigure 3.69: Frequency responses of Hnon(z) and its minimum-phase counterpart Hmin(z).\n\n\n\n\n\nListing 3.42: MatlabBookFigures/figs_systems_minphase\n\n\n\nclf \nB_roots = [2*exp(j*pi/4) 2*exp(-j*pi/4) ... %define roots \n    0.7*exp(j*pi/8) 0.7*exp(-j*pi/8)]; \nB=poly(B_roots) %construct B(z) of H(z)=B(z)/A(z) \n5A=poly([0.4 0.5 0.6 0.5j -0.5j 0.3j -0.3j]); %arbitrary \nBmin = ak_forceStableMinimumPhase(B) %move zeros inside \n[H,w]=freqz(B,A); %frequency response for original H(z) \n[Hmin,w]=freqz(Bmin,A); w=w/pi; %for minimum phase H(z) \nsubplot(211), plot(w,abs(H),w,abs(Hmin),'--'), axis tight \n10legend1 = legend('non-min','min-phase'), subplot(212)\n  \n\n\nFigure 3.69 was obtained with Listing 3.42. Note that the magnitude is the same but the phase is distinct, with Hmin(z) having smaller values.\n\n\n\nMinimum-phase systems have interesting properties. As the name indicates, among all systems with a given magnitude, the minimum-phase is the one with the phase closest to zero. Figure 3.69 illustrates this behavior, which can also be observed via the group delay, as in Figure 3.70.\n\n\n\n\n\n\n\n\nFigure 3.70: Group delay for the non-minimum and minimum phase systems of Figure 3.69.\n\n\n\n\n\nBesides, the impulse response h[n] of a minimum-phase system has its energy concentrated toward time n = 0 more than any other causal signal having the same magnitude spectrum.    □\n\n\n\n Application 3.5. Distinct ways of specifying the passband “ripple” / deviation. There are several ways of specifying the maximum “ripple” or deviation at the passband of a filter. For example, it can be specified in dB or linear scale, which are called here Rp and Dp, respectively (as suggested by Matlab). If one assumes a lowpass filter with a unitary gain at DC, the maximum linear deviation Dp can be assumed to be within [1 − Dp,1 + Dp] or [1 − Dp,1]. The latter option is particularly useful if the filter magnitude monotonically decreases as for the Butterworth filters and is assumed here, meaning that the frequency response is 1 at DC and decreases in the passband to a value not smaller than 1 − Dp. For example, a specification can be that the passband magnitude does not deviate 5% of the gain at DC. In this case, Dp = 0.05 and the minimum magnitude at passband is 1 − Dp = 0.95. Often, this value is specified in dB as\n\n\n\n\n\n\n Rp = −20log ⁡ 10(1 − Dp). \n\n\n\n\n\n\nIf the deviation is within [1 − Dp,1] in linear scale, then it is confined to [−Rp,0] in dB.\n\n\n\nAt the stopband, the magnitude in linear scale Ds is also a “deviation” because the reference value is 0 (the desired magnitude at stopband). In other words, for the stopband, the deviation coincides with the magnitude itself and its value in\ndB is given by\n\n\n\n\n\n\n Rs = −20log ⁡ 10Ds. \n\n\n\n\n\n\nOften, the filter magnitude has a maximum value of 0 dB and the values of Rp and Rs are positive.\n\n\n\nMatlab uses the same convention adopted here in its routines. This is exemplified in Listing 3.43, which designs an IIR Butterworth.\n\n\n\nListing 3.43: MatlabOctaveCodeSnippets/snip_systems_IIR_design.m\n\n\nWp=0.3; %normalized passband frequency (rad / pi) \nWs=0.7; %normalized stopband frequency (rad / pi) \nRp=1; %maximum passband ripple in dB \nRs=40; %minimum stopband attenuation in dB \n5[n,Wn] = buttord(Wp,Ws,Rp,Rs); %find the Butterworth order \n[B,A]=butter(3*n,Wn); %design the Butterworth filter \nzp=exp(j*pi*Wp); zs=exp(j*pi*Ws); %Z values at Wp and Ws \nlinearMagWp = abs(polyval(B,zp)/polyval(A,zp)) %mag. at Wp \nlinearMagWs = abs(polyval(B,zs)/polyval(A,zs)) %mag. at Ws \n10D_p = 1-linearMagWp %linear deviation at Wp (linear) \nR_p = -20*log10(1-D_p) %deviation (ripple) at Wp (dB) \nD_s = linearMagWs %linear deviation at Ws (linear) \nR_s = -20*log10(D_s) %deviation (ripple) at Ws (dB)\n  \n\n\nThe filter has order n=4, D_p=0.0044, R_p=0.0380, D_s=1.0e-006 and R_s=119.9987. Note that the requirements are met with slack. As a sanity check, adding the line\n\n\n[n2,Wn2] = buttord(Wp,Ws,0.0380,119.9987); %find Butterworth order\n\n\nat the end of the previous code would give the same order n2=n=4 and cutoff frequency Wn2=Wn as the previous invocation to buttord.    □\n\n\n\n Application 3.6. Influence of the poles quality factor on frequency response. The goal here is to illustrate how the pole Q-factor influences a frequency response.\n\n\n\n\n\n\n\n\nFigure 3.71: Magnitudes of frequency responses for two resonators with H(s) as in Eq. (3.29). The top plot corresponds to poles p = −300 ± j4000 with Q ≈ 6.6854 and |H(ω)|ω=4000 = 16.52 dB. The bottom plot corresponds to p = −3 ± j4000 with Q ≈ 666.67 and |H(ω)|ω=4000 = 56.48 dB. The two data tips indicate approximately 3-dB frequencies.\n\n\n\n\n\nFirst, Listing 3.44 shows how to calculate Q for a second-order system in Matlab/Octave using Eq. (3.40).\n\n\n\nListing 3.44: MatlabOctaveCodeSnippets/snip_systems_Qfactor.m\n\n\np1=-300+j*4000; p2=-300-j*4000; %pole and its conjugate \nA=poly([p1 p2]); %convert it to a polynomial A(s) \nomega_n = sqrt(A(3)); alpha = A(2)/2; %use definitions \nQ = omega_n / (2*alpha) %calculate Q-factor\n  \n\n\nIn this case, the poles are located at the frequency  ± 4000 rad/s and have Q ≈ 6.6854. For this example, the 3-dB bandwidth is defined (approximately) by ω1 = 3678 and ω2 = 4276.3 rad/s, which gives BW = (ω2 − ω1)∕(2π) ≈ 95.2 Hz. If we change the poles to  − 3 ± j4000, the Q-factor increases to Q ≈ 666.67 and BW decreases to approximately 6 Hz. Figure 3.71 compares these two situations.\n\n\n\nThe approximations used in analog filter design differ in the strategy for locating the poles (and zeros). Typically, the approximations that use poles with larger Q-factor have better magnitude but worse phase responses. The Listing 3.45 compares the poles and their Q values for the Butterworth, Chebyshev (type 1) and Cauer (or elliptic).\n\n\n\nListing 3.45: MatlabOctaveFunctions/ak_showQfactors.m\n\n\nfunction ak_showQfactors \nN=6; %order of filter \nWc=100; %cutoff frequency \ndisp('- Butterworth filter:'); \n5[z,p,k]=butter(N,Wc,'s'); %design Butterworth \ncalculateQfactors(p) %calculate and display Q-factors \ndisp('- Chebyshev 1 filter:'); \nRp=1; %maximum ripple at passband \n[z,p,k]=cheby1(N,Rp,Wc,'s'); %design Chebyshev type 1 \n10calculateQfactors(p) %calculate and display Q-factors \ndisp('- Elliptic filter:'); \nRs=30; %minimum attenuation at stopband \nWp=Wc; %elliptic natural frequency equals cutoff frequency \n[z,p,k]=ellip(N,Rp,Rs,Wp,'s'); %design Elliptic filter \n15calculateQfactors(p) %calculate and display Q-factors \nend % end function ak_showQfactors \nfunction calculateQfactors(p) %p has the poles \np=p(find(imag(p)&gt;0)); %exclude real poles and complex conjugates \nN=length(p); %number of complex poles with non-zero imaginary parts \n20for i=1:2:N \n    pole = p(i); \n    Q = abs(pole) / (2*(-real(pole))); \n    disp(['Poles=' num2str(real(pole)) ' +/- j' ... \n        num2str(imag(pole)) ' =&gt; Q=' num2str(Q)]); \n25end %end for \nend %end function calculateQfactors\n  \n\n\nThe output of Listing 3.45 is the following:\n\n- Butterworth filter:\nPoles=-96.5926 +/- j25.8819 =&gt; Q=0.51764\nPoles=-70.7107 +/- j70.7107 =&gt; Q=0.70711\nPoles=-25.8819 +/- j96.5926 =&gt; Q=1.9319\n- Chebyshev 1 filter:\nPoles=-23.2063 +/- j26.6184 =&gt; Q=0.76087\nPoles=-16.9882 +/- j72.7227 =&gt; Q=2.198\nPoles=-6.2181 +/- j99.3411 =&gt; Q=8.0037\n- Elliptic filter:\nPoles=-34.9934 +/- j49.0885 =&gt; Q=0.86137\nPoles=-9.154 +/- j92.2792 =&gt; Q=5.0651\nPoles=-1.3808 +/- j100.0259 =&gt; Q=36.2244\n\n\n\nThese results indicate that elliptic filters are more aggressive with respect to pole positioning (the poles are closer to the jω axis) and have sharper magnitude responses. Butterworth filters, on the other hand, have poles with lower Q than Chebyshev and elliptic filters.\n\n\n\n\n\n\n\n\nFigure 3.72: Poles (left) and magnitude |H(ω)| (right) for the sixth-order Butterworth filter of Listing 3.45. The top plots are obtained with the pair of poles with the smallest Q, the middle plots with four poles and the bottom plots with all six poles.\n\n\n\n\n\n\n\n\n\n\nFigure 3.73: Similar to Figure 3.72, but with a sixth-order Chebyshev type 1 filter. Note the poles are closer to the jω axis than for the Butterworth filter and there are ripples in the passband (one peak for each pole).\n\n\n\n\n\nFigure 3.72 and Figure 3.73 compare the Butterworth and Chebyshev approximations using sixth-order filters. Both lead to all-poles transfer functions, because there are no finite zeros (all zeros are at ω = ±∞). Finite zeros offer an additional degree of freedom but typically create additional abrupt changes in phase.\n\n\n\n\n\n\n\n\nFigure 3.74: Zero-pole plot (top) and magnitude in dB (bottom) for the sixth-order elliptic filter of Listing 3.45. This filter has zeros at ω = ±104.6303,  ± 116.8868,  ± 242.7541 rad/s, each one imposing a “valley” in the magnitude response, as the one indicated for ω = 104.6303.\n\n\n\n\n\nThe elliptic filters use finite zeros and achieve very short transition regions. Figure 3.74 illustrates a sixth-order elliptic filter to be compared to Figure 3.72 and Figure 3.73 (their bottom-most plots).   □\n\n\n\n Application 3.7. Estimating the frequency response of a sound board using a cable. The idea here is to estimate an impulse response δ(t) and then obtain a frequency response H(f) = F{δ(t)}, continuing the tests conducted in Application 1.4 (the same topic in be further discussed in Application 4.5).\n\n\n\nNote that shifting a signal in time does not alter the magnitude of the spectrum. But choosing the “time origin” is important for the spectrum phase. For the specific signal described in Application 1.4, the first task is then to isolate one of the four “impulses” (recall they are just gross approximations to δ(t)). Here it is chosen the segment that starts at sample n = 11026, which is when the second impulse δ[n] is positioned.\n\n\n\n\n\n\n\n\nFigure 3.75: Sound system phase frequency response ∠H(f) estimated from an impulse response.\n\n\n\n\n\nListing 3.46 estimates the phase from the estimated H(f) considering the response to the second impulse δ[n] at n = 11026.\n\n\n\nListing 3.46: MatlabOctaveCodeSnippets/snip_systems_phase_estimation.m\n\n\n[h,Fs,b]=readwav('impulseResponses.wav'); \nnstart=11026; %when second \\delta[n] occurs in impulses.wav \nnend=22050; %segment ends before the third \\delta[n] in impulses.wav \nh=double(h(nstart:nend)); %segment and cast h to double \n5N=length(h)-1; %it's convenient to make N an even number to use N/2 \nH=fft(h,N); %calculate FFT \np = unwrap(angle(H(1:N/2+1))); %unwrap the phase for positive freqs. \nf=Fs/N*(0:N/2)/1000; %create abscissa in kHz. Fs/N is the bin width \nhandler=plot(f,p); xlabel('f (kHz)'),ylabel('\\angle{H(f)} (rad)') \n10k1=1500; k2=4000; makedatatip(handler,[k1,k2]); %choose any 2 points \n%Calculate derivative as the slope. Convert from kHz to rad/s first: \ngroupDelayInSeconds = -atan2(p(k2)-p(k1),2*pi*1000*(f(k2)-f(k1)))\n  \n\n\nFigure 3.75 shows the obtained phase. Because the phase is linear, the group delay can be estimated as suggested in Listing 3.28, by choosing two points and calculating the line slope in line 12. In this case the result was 45.573 ms, which is consistent with the estimate conducted in time-domain in Application 1.4.    □\n\n\n\n Application 3.8. Interpreting the impulse response for line topology identification in DSL. In DSL, loop qualification consists in determining if a given line under analysis is able to support the desired DSL service. Identifying the topology of a given telephone line means identifying its line sections and is helpful for the loop qualification. These sections can be serial sections or bridge taps. Roughly speaking, a bridge tap is a derivation that is not terminated and causes unwanted signal reflections (echoes). Both types of sections have essentially two attributes: the length of the section and the type of cable used, which here is assumed to be entirely described by the gauge (or equivalently, the diameter) of the conductors. Therefore, the topology of a line under analysis can be considered as a set Θ =  {𝜃1,𝜃2,…,𝜃ns}, where the subset 𝜃k, with k = 1,2,...,ns, represents the k-th line section from a port at the central office to the port at the costumer premises equipment, and ns is the number of sections that composes the subscriber line. Each subset 𝜃k is composed by the parameters 𝜃k =  {τk,lk,ϕk} where τk ∈{“serial”,“bridge tap”}, lk, and ϕk are, respectively, the type of section, its length, and the cable gauge.\n\n\n\nFor many practical lines, the impulse response is composed by a series of positive (or upward) pulses delayed in time. Each succeeding pulse is generated by the part of the signal transmitted through the line discontinuities and multi-reflections suffered by the input\nsignal. Each “path” taken by the input signal into the line defines a pulse in the output.\n\n\n\nThe following discussion will suggest how the impulse response can be interpreted. Figure 3.76(a) (top) depicts an example of a topology with six line sections and two bridge taps in the third and fifth line section, respectively. Figure 3.76(a) (bottom) also shows the first four (strongest) possible paths that the input signal can follow, from port 1 to port 2. Figure 3.76(b) shows the corresponding (simulated) impulse response h(t) of the line topology depicted in Figure 3.76(a). The relation between the signal paths and the pulses in the h(t) can be observed from the arrival times of the positive pulses, t1 to t4.\n\n\n\n\n\n\n\n\n\n\n\n(a) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) \n\n\n\n\n\n\n\nFigure 3.76: DSL line topology and corresponding impulse response: (a) from top to bottom: topology and some of the possible signal paths; (b) corresponding impulse response with points indicating the estimated time of arrival of the pulses.\n\n\n\n\n\nFor example, the length of path A can be estimated by processing h(t) to extract the time of arrival tA of the first pulse. Having the time, the path length is obtained by considering that the signal propagates at velocity vp = 0.67c, where c ≈ 3 × 108 m/s is the speed of light in vacuum. For example, assume the total serial length 500 + 300 + 350 + 850 = 2,000 m of the line is not known and must be automatically estimated by the DSL service provider. Assume an estimate of h(t) was available and tA = 9.35 μs is obtained by signal processing (in our case, by inspection of Figure 3.76(b)). Hence, the estimate for the total length is LA = tAvp ≈ 1,879.4 m. The length of the bridge taps can also be estimated along this line of reasoning, assuming LA has already been estimated.\n\n\n\nThe presence of each bridge tap originates a new path and, as a consequence, a new pulse in the impulse response. In the example, the corresponding paths are B and C, which correspond to the arrival times tB e tC) of the second and third pulses, respectively. Given that the shortest bridge tap (the fifth section) has length l5, the total length traveled by its corresponding pulse can be written as LB = LA + 2l5. Thus, the length of the first bridge tap can be estimated by l5^ =  (tB−tA) 2 vp. Use the same reasoning to estimate the length of the longest bridge tap (third section).\n\n\n\nTopology identification is an involved problem. For example, the path D is caused by multiple reflections, which can cause several problems. And the presented analysis has restrictions. For example, it is not possible to determine the position of bridge taps from the impulse response. If the position of the bridge taps in Figure 3.76(a) change, the path traveled by the signal remains the same and, consequently, its signature on h(t) does not change.   □\n\n\n\n Application 3.9. Understanding and compensating the delay imposed by a system. All practical systems impose some delay or distortion to signals. For example, in communication systems the channel is typically frequency-selective, meaning that distinct frequency components of the signal get attenuated and delayed in a way that the output signal is a distorted version of the input signal. Besides this effect, systems impose a delay (the output is observed only after an interval of time from the instant the input was generated, as studied in Application 1.5) that is often characterized by its group-delay.\nBetter understanding how to predict this delay when the system is LTI is the subject of the following discussion.\n\n\n\n\n\n\n\n\n\n\n\n(a) Impulse response\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Group delay\n\n\n\n\n\n\n\nFigure 3.77: Impulse response and group delay of IIR filter obtained with [B,A]=butter(8,0.3). The impulse response was truncated to 50 samples.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Input signal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) FFT of the input\n\n\n\n\n\n\n\nFigure 3.78: Input signal x[n] and its FFT.\n\n\n\n\n\nAn example of filtering is provided in the sequel using an 8-th order IIR filter obtained and analyzed with Listing 3.47.\n\n\n\nListing 3.47: MatlabOctaveCodeSnippets/snip_systems_filtering.m\n\n\n[B,A]=butter(8,0.3); %IIR filter \nh=impz(B,A,50); %obtain h[n], truncated in 50 samples \ngrpdelay(B,A); %plots the group delay of the filter \nx=[1:20,20:-1:1]; %input signal to be filtered, in Volts \n5X=fft(x)/length(x); %FFT of x, to be read in Volts \ny=conv(x,h); %convolution y=x*h\n  \n\n\nThe truncated impulse response of this filter is depicted in Figure 3.77(a) and the group delay in Figure 3.77(b). An input signal x with a triangular shape was generated and is depicted in Figure 3.78 together with its FFT.\n\n\n\n\n\n\n\n\nFigure 3.79: Input x[n] and output y[n] signals obtained via convolution with truncated h[n].\n\n\n\n\n\nThe result of the convolution y=conv(x,h) is shown in Figure 3.79. The input has peaks at n = 19 and n = 20, such that x[19]=x[20]=20, while the output peaked at n = 25 with y[25]=19.7. This delay of 5 samples could be inferred by observing that the peak of h in Figure 3.77(a) is located at sample n = 6. A more accurate analysis can be done via the filter’s group delay. In the case of a linear-phase FIR, the group delay is constant (the same number of samples) in the whole passband. But the current IIR has a frequency dependent group delay and the spectrum of the input signal needs to be analyzed via Figure 3.78.\n\n\n\nNote that most of the input signal power is concentrated at DC and the FFT coefficient X[k] for k = 1, which corresponds to an angle Ω = 2π∕40 = 0.1571 rad, given that length(x)=40 is the FFT length. From Figure 3.77(b) or directly calculating gd=grpdelay(B,A,[0 0.1571]), it can be observed that the group delay τg(Ω) of the filter for components at k = 0 and k = 1 is τg(0) = 5.03 and τg(0.1571) = 5.1043 samples, respectively. When using Figure 3.77(b), note that Ω should be normalized and the value read at Ω∕π = 0.05. The group delay at the frequencies of interest is approximately 5 samples, which explains the delay imposed by the filter in Figure 3.79.\n\n\n\n\n\n\n\n\nFigure 3.80: Input x[n] and output y[n] signals obtained via filter.\n\n\n\n\n\nInstead of using convolution, the output was obtained with y=filter(B,A,x). In this case, the vector y is the one depicted in Figure 3.80, which should be compared to Figure 3.79. Because Nx = 40 and Nh = 50 samples, the convolution in Figure 3.79 led to Ny = 40 + 50 − 1 = 89 samples. In contrast, Figure 3.80 shows that filter makes sure that Ny = Nx = 40 samples. To visualize the tail of the convolution, one should add zero samples to the end of x. For example, the extra 20 samples in y=filter(B,A,[x zeros(1,20)]) are enough to visualize the “triangle” in y.\n\n\n\n\n\n\n\n\nFigure 3.81: Output y[n] aligned with the input x[n] and corresponding error x[n] − y[n].\n\n\n\n\n\nIn some situations it is necessary to compare the error between the input and filtered output signals. But this requires compensating the filter delay and eliminating the convolution tail. Figure 3.81 was obtained with:\n\n\n\nListing 3.48: MatlabOctaveCodeSnippets/snip_systems_compensate_grpdelay.m\n\n\nx=[1:20,20:-1:1]; %input signal to be filtered, in Volts \n[B,A]=butter(8,0.3); %IIR filter \nh=impz(B,A,50); %truncate h[n] using 50 samples \ny=conv(x,h);  %convolution y=x*h \n5x=x(:); y=y(:); %force both x and y to be column vectors \nmeanGroupDelay=5 %estimated \"best\" filter delay \ny(1:meanGroupDelay)=[]; %compensate the filter delay \ny(length(x)+1:end)=[]; %eliminate convolution tail \nmse=mean( (x-y).^2 ) %calculate the mean squared error \n10SNR=10*log10(mean(x.^2)/mse) %estimate signal/noise ratio \nstem(0:length(x)-1,x); hold on %input \nstem(0:length(y)-1,y,'r') %output aligned with the input \nstem(0:length(y)-1,x-y,'k') %e.g., calculate error after alignment\n  \n\n\nThe mean squared error (MSE) in this case is 0.0363, which is equivalent to an SNR of 35.97 dB.\n\n\n\nDecreasing the filter cutoff frequency to Ω = 0.1 rad via [B,A]=butter(8,0.1) and adjusting the variable meanGroupDelay to 16 samples (meanGroupDelay=17 leads to a slightly better result) makes the SNR decrease to 23.22 dB because more high frequency components of x are filtered out.    □\n\n\n\n Application 3.10. Taking the filter memory in account. When using filter in block processing, it is important to take the filter’s memory in account. By default, this memory is assumed to be zero (the system is relaxed), but this can be modified with the command filtic, which allows to set the filter’s initial conditions.\n\n\n\n\n\n\n\n\nFigure 3.82: Filtering in blocks of N = 5 samples but not updating the filter’s memory.\n\n\n\n\n\nInstead of using y=filter(B,A,x) to completely filter x in one function call, it is useful to know how to segment the input x in blocks of N samples and repeatedly invoke filter until filtering all blocks. This is illustrated in Listing 3.49.\n\n\n\nListing 3.49: MatlabOctaveCodeSnippets/snip_systems_filtering_blocks.m\n\n\nx=[1:20,20:-1:1]; %input signal to be filtered, in Volts \n[B,A]=butter(8,0.3); %IIR filter \nN=5; %block length \nnumBlocks=floor(length(x)/N); %number of blocks \n5Zi=filtic(B,A,0); %initialize the filter memory with zero samples \ny=zeros(size(x)); %pre-allocate space for the output \nfor i=0:numBlocks-1 \n    startSample = i*N + 1; %begin of current block \n    endSample = startSample+N -1; %end of current block \n10    xb=x(startSample:endSample); %extract current block \n    [yb,Zi]=filter(B,A,xb,Zi);%filter and update memory \n    y(startSample:endSample)=yb; %update vector y \nend\n  \n\n\nThese commands obtain the same results of Figure 3.80 because the filter memory is updated via [yb,Zi]=filter(B,A,xb,Zi). However, a simple modification from this command to yb=filter(B,A,xb), which disregards the filter memory leads to the result in Figure 3.82. In this case, the filter has zeroed memory at the beginning of each block of N = 5 samples. The strong distortion on y[n] in this case indicates that it is crucial to take care of updating the filter memory when dealing with signal processing in blocks.    □\n\n\n\n Application 3.11. Processing sample blocks using convolution in matrix notation.\n\n\n\nAs Eq. (3.6) indicates, the convolution between two finite-length signals can be represented in matrix notation. However, in many applications, block processing is adopted, where one of the signals has an infinite duration and is segmented into successive blocks of N samples, as in Eq. (1.8).\n\n\n\nListing 3.50 illustrates the extra manipulations that are required to use a matrix notation for the convolution of each block and get the correct convolution result.\n\n\n\nListing 3.50: MatlabOctaveCodeSnippets/snip_systems_matrixBlockConvolutions.m\n\n\nx=transpose(1:1000); %(eventually long) input signal,as column vector \nh=ones(3,1); %non-zero samples of finite-length impulse response \nNh=length(h); %number of impulse response non-zero samples \nNb=5; %block (segment) length \n5Nbout = Nb+Nh-1; %number of samples at the output of each block \nNx = length(x); %number of input samples \nhmatrix = convmtx(h,Nb); %impulse resp. in matrix notation \nbeginNdx = 1; %initialize index for first sample of current block \ny = zeros(Nh+Nx-1,1); %pre-allocate space for convolution output \n10for beginNdx=1:Nb:Nx %loop over all blocks \n    endIndex = beginNdx+Nb-1; %current block end index \n    xblock=x(beginNdx:endIndex); %block samples, as column vector \n    yblock = hmatrix * xblock; %perform convolution for this block \n    y(beginNdx:beginNdx+Nbout-1) = y(beginNdx:beginNdx+Nbout-1) + ... \n15        yblock; %add parcial result \nend \nplot(y-conv(x,h)) %compare the error with result from conv\n  \n\n\nThe basic idea is that the end samples (“tails”) of the partial convolution results need to be properly added to the final result. In the example of Listing 3.50, the impulse response has Nh=3 non-zero samples and the block length is Nb=5. The convolution matrix hmatrix has\ndimension 7 × 5 and each partial convolution result corresponds to Nbout=7 samples. For instance, the last two samples of yblock of the first block need to be added to the first two samples of yblock obtained of the second block, and so on. This is similar to the overlap-add method illustrated in Listing 3.9 but without the FFT-based processing.    □\n\n\n\n Application 3.12. Power and energy at the output of a LTI system. In many applications it is important to estimate the power at the output of a system. The following result is valid for LTI systems.\n\n\n\n\n  Theorem 3.  Power at the output of a discrete-time LTI system.\n\n\n\n\n\n\n\n\n\n Py = PxEh + 2∑ i=0L ∑ j=0,j≠iLh ihjRx(j − i) \n\n\n(3.97)\n\n\n\n\n\n\nwhere Rx(j − i) = 𝔼[⟨x[n],x[n − (j − i)]⟩] is the empirical (time-averaged) autocorrelation and Eh = ∑ ⁡ i=0Lhi2 is the energy of h[n]. If Rx(k) = 0,∀ ⁡k and k≠0, then Py = PxEh.\n\n\n\nProof sketch: Assume a short impulse response h[n] = h0 + h1δ[n − 1] + h2δ[n − 2] with L = 2. Take three consecutive output values starting at an arbitrary time instant, such as n = 5, for example. These values will be y[5] = x[5]h0 + x[4]h1 + x[3]h2, y[6] = x[6]h0 + x[5]h1 + x[4]h2 and y[7] = x[7]h0 + x[6]h1 + x[5]h2. Calculating\nthe energy Ey for these three samples is enough to provide insight on the solution:\n\n\n\n Ey  = y2[5] + y2[6] + y2[7]     = h02(x2[5] + x2[6] + x2[7]) + h 12(x2[4] + x2[5] + x2[6])     +h22(x2[3] + x2[4] + x2[5]) + 2h 0h1(x[5]x[4] + x[6]x[5] + x[7]x[6])     +2h0h2(x[5]x[3] + x[6]x[4] + x[7]x[5]) + 2h1h2(x[4]x[3] + x[5]x[4]     +x[6]x[5]).    \n\nTaking all samples in account and using expectation, one can obtain a relation between the output and input energies Ey and Ex, respectively:\n\n\n\n\n\n\n Ey = ExEh + 2∑ i=0L ∑ j=0,j≠iLh ihj⟨x[n],x[n − (j − i)]⟩ \n\n\n\n\n\n\nor, alternatively\n\n\n\n\n\n\n Py = PxEh + 2∑ i=0L ∑ j=0,j≠iLh ihjRx(j − i) \n\n\n\n\n\n\ngiven that P = E∕N.\n\n\n\nIf the input x[n] does not have correlation among its samples, then it is a “white” signal (see Section 1.12.1.0) with Rx(k) = 0 for k≠0, such that\n\n\n\n\n\n\n\n\n\n Py = PxEh. \n\n\n(3.98)\n\n\n\n\n\n\n  □\n\n\n\n\n\n\n\nFor example, assume that h[n] = 1 − 0.5δ[n − 1], such that Eh = 1.25 and Py = PxEh + 2h0h1Rx(1). Assuming the input is AWGN with power Px = 1, then Eq. (3.98) leads to Py = PxEh = 1.25 . The following code illustrates this fact:\n\n\nN=1000000; x=randn(1,N); h=[1 -0.5]; y=filter(h,1,x); \nEh=sum(h.^2), Px=mean(x.^2), Py=mean(y.^2)\n\n\nHowever, if x[n] is not white, Rx(k) will impact the result.\nFor the same filter, if x[n] = 1,∀ ⁡n, then R(1) = 1 and Px = 1, such that Py = 0.25. As a third example, consider the samples of a binary x[n] are i.i.d with equiprobable 0 and 1 values. In this case, R(1) = 1∕4 and Px = 0.5, such that Py = 0.5 × 1.25 + 2(−0.5) × 1∕4 = 0.375. The following commands\n\n\nN=1000000;x=round(rand(1,N));h=[1 -0.5];y=filter(h,1,x); \nEh=sum(h.^2), Px=mean(x.^2), Py=mean(y.^2)\n\n\ncan confirm the results.   □\n\n\n\n Application 3.13. The delay and attenuation imposed by a channel. Here the LTI system is assumed to be a communication channel. When the channel has a flat magnitude and a linear phase (or, equivalently, a constant group delay) over the bandwidth of the transmitted signal, the received signal is a delayed version of the transmitted signal. In this special case, the delay can be described by a single value. In general however, each frequency component is delayed by a different amount and this relation is described by the group delay function in Eq. (3.46).\n\n\n\nAs discussed, roughly, the group delay is the time between the channel’s initial response and its peak response or, in other words, the group delay of a filter is a measure of the average delay of the filter as a function of frequency. Hence, for an arbitrary (non-constant) group delay, distinct frequency components of the transmitted signal get delayed by different delays, the signal gets distorted and there is not a single delay value to characterize the transmission. In practical communication systems, an estimator keeps tracking at the receiver of the best time instant to compensate the delay provoked by the channel. And in simulations, it is common to resort to a brute force approach, where this “optimum” delay compensation instant is searched by trying distinct values in a range and choosing the one that minimizes a figure of merit such as the bit error rate.\n\n\n\nWhen the channel simulation requires filtering to shape the magnitude of the signal spectra but allows linear phase, it is convenient to use FIR filters with linear phase because the delay they impose is well-defined as indicated in Figure 3.49. Hence, in many cases the LTI channel is assumed to be causal and represented by a finite-length impulse response h[n] = h0δ[n] + h1δ[n − 1] + … + hLδ[n − L] with L + 1 non-zero values. The integer L is the channel order because in the Z-domain, H(z) = h0 + h1z−1 + … + hLz−L. Assuming a linear phase FIR of L-th order, where L is even (or equivalently, the filter has an odd number of coefficients), the delay is simply\n\nL∕2.\n\n\n\nRecall that, for FIR filters, the impulse response coincides with the filter coefficients. Adopting vectors, the impulse response can be represented by h=[h0 h1 … hL] and the command y=filter(h,1,x) used because the numerator B(z) and denominator A(z) of H(z) = B(z)∕A(z) are h and 1, respectively.\n\n\n\n\n\n\n\n\nFigure 3.83: Linear phase FIR channel obtained with h=fir1(10,0.8). The output signal was scaled to have the same power as the input.\n\n\n\n\n\nFor example, consider the linear phase channel obtained with h=fir1(10,0.8). Figure 3.83 depicts its impulse response, group delay and the output y[n] for an input x[n] representing symbols  + 1,−1,+1,+1,−1,−1 with an oversampling of L = 4 samples per symbol and a shaping pulse with four samples equal to 1.\n\n\n\nFigure 3.83 allows to observe that the input gets delayed by the group delay of h, which in this case is 5. Note that the input peak at x[0] corresponds to the output sample y[5]. This simple signal delay happens because because h in this example was chosen to be symmetric and have a linear phase, such that the group delay is 5 samples for all frequencies as indicated in Figure 3.83. In terms of seconds, a group delay of 5 samples corresponds to 5Ts = 5∕Fs, where Fs is given in Hz. In this case, the first 5 samples of the output should be eliminated for aligning with x[n]. For example, the commands y(1:5)=[], x(end-4:end)=[] could be used to align the corresponding vectors and keep them with the same number of samples.\n\n\n\nHowever, in a more general scenario, the group delay varies with frequency, such as for the non-symmetric FIR filter illustrated in Figure 3.50.\n\n\n\nTypically a channel attenuates the input signal and decreases its power. When the channel is LTI, the output power Py is related to the input power Px as informed by Theorem 3 (page §).\n\n\n\nWhen comparing the input x[n] and output y[n] waveforms, it is useful to know the ratio g = Py∕Px and multiply x[n] by g in order to compare signals with equivalent amplitude ranges. However, in practice, it is hard to determine g and typically an adaptive automatic gain control (AGC) is employed.\n\n\n\nAnother example of compensating the delay and gain imposed by a channel is discussed in the sequel.\n\n\n\nIf useful, one can mimic a perfect automatic gain control (AGC) and force the output to have the same power as the input with the following commands Px=mean(x.^2), Py=mean(y.^2), y = sqrt(Px/Py)*y.\n\n\n\n\n\n\n\n\nFigure 3.84: IIR channel obtained with [B,A]=butter(5,0.8). The output signal was scaled to have the same power as the input.\n\n\n\n\n\nSimilar to the procedure for generating Figure 3.83, an IIR filter was simulated to obtain Figure 3.84. In this case the group delay is less than 1 for frequencies Ω &lt; 0.47π and the peak of h[n] is for n = 1. Observing the graphs of x[n] and y[n], a good alignment would be obtained by shifting y[n] one sample to the left, i. e., y[n + 1] if manipulating algebraically or y(1)=[] if using Matlab/Octave.    □\n\n\n \n\n35 The reciprocal of a complex number c is 1∕c."
  },
  {
    "objectID": "ak_dsp_bookse47.html",
    "href": "ak_dsp_bookse47.html",
    "title": "47  Comments and Further Reading",
    "section": "",
    "text": "We have explained that an impulse response h(t) or h[n] is capable of fully representing the behavior of a LTI system because this is the case in current engineering problems. But as discussed in [?] and references therein, it is possible to define abstract examples where the LTI cannot be described by an impulse response. The physical dimension, unit and interpretation of the impulse response are discussed in [?,?].\n\n\n\nThe group delay can assume negative values and, more than that, the channel output signal may look like as anticipated with respect to the input signal, as if the group delay was “negative”. This issue is discussed online, e. g. [url3grp], and also in [?], where an interesting example is provided.\n\n\n\nIt should be noted that the powerful Matlab function yulewalk can be used to find an IIR filter that approximates an arbitrary magnitude response.\n\n\n\nSome useful links about analog filter design can be found at [url3fid].\n\n\n\nThere are several topologies for analog filters. For example, at [url3fto] one can find a discussion about a better topology than Sallen-Key."
  },
  {
    "objectID": "ak_dsp_bookse48.html",
    "href": "ak_dsp_bookse48.html",
    "title": "48  Exercises",
    "section": "",
    "text": "3.1. When the input x1[n] = δ[n] + δ[n − 1] is applied to a linear system, the observed output is y1[n] = 4δ[n − 1] + δ[n − 2]. Inform the expression of the output y2[n] when the input is x2[n] = 3δ[n] + 3δ[n − 1]. 3.2. When the input x1[n] = δ[n] is applied to a LTI system, the output is y1[n] = 3δ[n] + 2δ[n − 1] + δ[n − 2]. What is the output y2[n] when the input is x2[n] = 5δ[n] + 2δ[n − 1]? 3.3. When the input x1[n] = δ[n] is applied to a system, the output is y1[n] = 3δ[n] + 2δ[n − 1] + δ[n − 2], and when the input is x2[n] = 2δ[n − 1], the output is y2[n] = 6δ[n − 1] + 2δ[n − 2] + 2δ[n − 3]. Can this system be LTI? 3.4. A time-invariant system has output y1(t) = δ(t − 1) − δ(t − 4) when the input is x1(t) = δ(t). Inform the expression of the output y2(t) when the input is x2(t) = δ(t − 3). 3.5. The input-output relations that describe three given systems are: 1) y[n] = C(x[n])2, where C = 2ejπ∕2, 2) y[n] = (n − 1)x[n] and 3) y[n] = 3x[n − 1] + 2πx[n − 3]. Classify whether or not these systems are: a) causal, b) linear, c) memoryless, d) stable in BIBO sense and e) time-invariant. 3.6. A discrete-time LTI is characterized by the difference equation y[n + 2] = 5x[n + 2] + 2x[n + n0], where x[n] and y[n] are the input and output signals, respectively. Inform a value for n0\nthat makes the system causal. 3.7. What is the convolution matrix H for an impulse response h[n] = 3δ[n] + 2δ[n − 1] + δ[n − 2] when the input signals are: a) column vectors of dimension 3, b) row vectors of dimension 4? 3.8. Four continuous-time signals: x1(t) = ej4t, x2(t) = e−j4t, x3(t) = cos ⁡ (4t) and x4(t) = e(−1+j4)t, are individually transmitted over a causal system H(s) = 1 s+10. What are the respective outputs y1(t),…,y4(t)? 3.9. Four discrete-time signals: x1[n] = 5ej2n, x2[n] = e−j2n, x3[n] = cos ⁡ (2n) and x4[n] =  (3ej2) n, are individually transmitted over the causal system H(z) = 1 1−0.6z−1. What are the respective outputs y1[n],…,y4[n]? 3.10. What is the result of the periodic convolution between a pulse train x(t) with pulses with duty cycle of 2 s, amplitude of 5 V and a period of 10 s with itself? Assume x(t) is an even signal. 3.11. The following commands were used to calculate y1 and y2:\n\n\nN=3;M=4;x=1:M;h=[0.4,-0.1];y1=conv(x,h);y2=ifft(fft(x,N).*fft(h,N));\n\n\n\nWhat is the minimum value of N to make y1 and y2 coincide? b) Generalize the previous answer for an arbitrary value of M and length(h). 3.12. A band-limited signal x(t) was sampled obeying the sampling theorem with Fs = 10 Hz and the result is xs(t) = 4δ(t) + 3δ(t − 0.1) − 2δ(t − 0.4). Provide the\n\nexact expression of x(t). Hint: this expression is given in terms of scaled and shifted sincs. 3.13. An AM radio signal x(t) has a bandwidth of 10 kHz at baseband and is centered at fc = 600 MHz (the passband signal occupies 20 kHz). Use undersampling to obtain a discrete-time baseband version xb[n] of this signal via digital signal processing based on a signal x[n] obtained with the minimum sampling frequency that you think is possible assuming realistic (non-ideal) filters. Describe the whole scheme informing the adopted: a) analog anti-aliasing filter, b) sampling frequency, c) any required digital filtering and/or frequency digital down-conversion, and d) phase inversion compensation (in case the signal replica at positive frequency fc was moved to a lower frequency than the replica at  − fc). 3.14. A band-limited even and real-valued discrete-time signal x[n] has the spectrum X(ejΩ) described in Figure 3.85 with Ω1 = π∕4 and Ω2 = π∕2 rad. It is converted to a continuous-time signal x(t) using the scheme depicted in Block (3.27) with Fs = 1 kHz and a ZOH modeled by h(t) = u(t) − u(t − Ts). a) Carefully draw |X(f)|, where X(f) = F{x(t)} in the range  − 3 to 3 kHz. b) Calculate X(f) (magnitude and phase) at f = 0,100 and 1200 Hz. c) Repeat the calculation in b) but now considering that x[n]\nis a critically sampled signal with Ω1 = π∕4 but Ω2 = π rad.\n\n\n\n\nFigure 3.85: Spectrum of a band-limited even and real-valued signal x[n].\n\n\n\n\n3.15. Assume the DUT in the circuit of Figure B.34 is the SAW filter described in Table 3.4 (assume its typical parameters). Both the source\n\n\nZS\n\nand load\n\n\nZL\n\nimpedances are 100 \n\n\nΩ\n\n. a) If the input signal\n\n\nx(t)\n\n(denoted as\n\n\nV s\n\nin Figure B.34) is a sinusoid of frequency 70 MHz and power 4 dBm, what is the power over the load and a reasonable expression for the output signal\n\n\ny(t)\n\n? b) If\n\n\nx(t) = 3cos ⁡ (ω1t) + 6cos ⁡ (ω2t)\n\nwith\n\n\nω1 = 2π × 70\n\nand\n\n\nω2 = 2π × 76.5\n\nMrad/s, what is a reasonable expression for the output\n\n\ny(t)\n\n? 3.16. If the input to the filter characterized by Table 3.5 is\n\n\nx(t) = 4cos ⁡ (2πf1t + π∕4) + 8cos ⁡ (2πf2t)\n\n, where\n\n\nf1 = 455\n\nand\n\n\nf2 = 500\n\n kHz, what is a reasonable expression for the output\n\n\ny(t)\n\n? 3.17. A non-causal analog lowpass filter has an ideal magnitude with unitary gain for frequencies\n\n\n − 5\n\nto 5 rad/s and zero otherwise. The phase of this filter is linear and given by\n\n\ne−j8ω\n\n. What is the output of this filter when the input is\n\n\nx(t) = ∑ ⁡ k=0∞0.5k sin ⁡ (1.5kt)\n\n? 3.18. A FIR filter with linear phase has an impulse response\n\n\nh[n] = 3δ[n] − 5δ[n − 1] + 7δ[n − 2] − 5δ[n − 3] + 3δ[n − 4]\n\n. Find its system function\n\n\nH(z)\n\nand draw a realization of this filter with the minimum number of multipliers.\n3.19. a) Design two filters\n\n\nH1(s)\n\nand\n\n\nH2(s)\n\n, both described by\n\n\n(ωn2)∕(s2 + 2ζωns + ωn2)\n\n, with damping factors\n\n\nζ = 0.707\n\nand\n\n\n0.9\n\n, respectively. In the case of\n\n\nH2(s)\n\n, assume a natural frequency\n\n\nωn = 1600\n\n rad/s and a tolerance\n\n\n𝜖 = 0.05\n\n. In both cases, the settling time must be approximately 3 ms. b) Plot and compare the graphs of the output waveforms of both systems to the step function\n\n\nu(t)\n\n. 3.20. For the filter\n\n\nH(z)\n\nobtained with [b, a]=butter(3,1/3): a) find the values of\n\n\nH(z)\n\nfor\n\n\nz = 0.5 + 0.5j,3 + 2j,0,−1,3\n\nand\n\n\n0.5 + 0.5j\n\n. b) Using freqz.m, find the maximum value of the magnitude response and in which frequency this maximum occurs. c) What are the first five samples of the impulse response\n\n\nh[n]\n\n? d) Is this a FIR or IIR? e) Is this a lowpass or highpass? 3.21. For the filter\n\n\nH(s) = 100 s2+102s+100\n\n(obtained with [Bs,As]=butter(2,10,’s’)); a) what are the values in Hertz of its natural and cutoff frequencies? Now consider that\n\n\nH(s)\n\nis the anti-aliasing filter of a DSP system operating with a sampling frequency of\n\n\nFs = 4\n\n Hz and there is a strong interferer tone with 1 dBm at 5 Hz that contaminates the signal before the filter\n\n\nH(s)\n\n. b) At what frequency and power this interferer will appear within the band\n\n\n[0,Fs∕2[\n\n? c) And what if the interferer has the same power of 1 dBm but frequency of 50 Hz? 3.22. a) Using syntax and structure similar to Listing 3.2, list the algorithm of a digital filter implementing the system function\n\n\nH(z)\n\nprovided by [Bz,Az]=butter(3,0.2). b) When\n\n\nH(z)\n\nis used in the canonical system of Figure 3.7 with\n\n\nFs = 44.1\n\n kHz and ideal analog filters, what is the corresponding cutoff frequency in Hertz of the overall\nsystem? 3.23. The filter\n\n\nH(s) = 1 s3+2s2+2s+1\n\nwas obtained with the command [b, a]=butter(3,1,’s’) and has cutoff frequency of 1 rad/s. Obtain a filter\n\n\nG(s)\n\nthat corresponds to a scaled version of\n\n\nH(s)\n\nwith a new cutoff frequency of\n\n\n1200π\n\nrad/s such that both have the same gain at DC. a) Inform the linear gains\n\n\n|H(ω)|\n\nand\n\n\n|G(ω)|\n\nfor both filters at frequencies\n\n\nω = 0,1,1200π\n\nand 2000 rad/s. b) What are these gains in dB? 3.24. A filter\n\n\nH(s) = 125 s3+10s2+50s+125\n\nhas a cutoff frequency of 5 rad/s. Obtain a filter\n\n\nG(s)\n\nthat corresponds to a version of\n\n\nH(s)\n\nscaled in frequency with a cutoff frequency of 3 rad/s.\n\n\nG(s)\n\nshould also have a gain of 3 dB at DC. 3.25. a) Use the six methods of Table 3.6 to convert\n\n\nH(s) = 0.13∕(s2 − 0.4s + 0.13)\n\ninto\n\n\nH(z)\n\nassuming\n\n\nTs = 0.1\n\n s and compare their frequency responses with the original one (in continuous-time) b) Repeat the procedure for\n\n\nTs = 1\n\n s. c) Discuss what methods were most impacted by the increase of\n\n\nTs\n\n. 3.26. Given a filter\n\n\nH(s) = (0.1s2 + 71)∕(s2 + 11s + 71)\n\nand assuming\n\n\nFs = 50\n\n Hz, a) find\n\n\nH(z)\n\nusing the bilinear transform, b) compare the frequency responses of\n\n\nH(s)\n\nand\n\n\nH(z)\n\nafter D/C conversion using the same abscissa in rad/s, c) can you find a frequency\n\n\nωm &gt; 0\n\nin rad/s for which the values of\n\n\nH(s)\n\nand\n\n\nH(z)\n\nare the same?\n3.27. A filter\n\n\nH(s) = (0.1s2 + 71)∕(s2 + 11s + 71)\n\nshould be converted to\n\n\nH(z)\n\nusing the bilinear. The value of\n\n\nH(s)\n\nat\n\n\ns1 = j8.24\n\nand\n\n\ns2 = j5\n\nare\n\n\ng1 ≈ 0.708e−j1.5366\n\nand\n\n\ng2 ≈ 0.9554e−j0.8743\n\n, respectively. a) Find\n\n\nH1(z)\n\nand\n\n\nH2(z)\n\nthat when implemented in a hardware using\n\n\nFs = 50\n\n Hz leads to the same values\n\n\ng1\n\nand\n\n\ng2\n\nat\n\n\nωd = 8.24\n\nand 5 rad/s, respectively, which correspond to the frequencies\n\n\nΩ = ωd∕Fs = 0.1648\n\nand 0.1 rad. b) For comparison, show the values of\n\n\nHi(ej0.1648)\n\nand\n\n\nHi(ej0.1)\n\n, for\n\n\ni = 1\n\nand 2. c) To note the degrees of freedom, use frequency scaling such that\n\n\nG(s)|s=s2 = H(s)|s=s1 = g1\n\nand then find\n\n\nG(z)\n\nsuch that when implemented in a hardware using\n\n\nFs = 50\n\n Hz leads to the same value\n\n\ng1\n\nat\n\n\nωd = 5\n\n rad/s. 3.28. The filter\n\n\nHs(s) = (0.1s2 + 71)∕(s2 + 11s + 71)\n\nshould be converted to\n\n\nHz(z)\n\nusing the bilinear such that the value of\n\n\nHs(jω)\n\nat\n\n\nω = 10\n\n rad/s is the same value\n\n\nHz(ejΩ)\n\nwill have at\n\n\nΩ = π∕4\n\n rad. a) find the value of\n\n\nFs\n\nthat allows to directly convert\n\n\nHs(s)\n\ninto\n\n\nHz(z)\n\nwithout the traditional pre-warping step, b) assuming an arbitrary value of\n\n\nF ′s = 0.5\n\n Hz, find the pre-warped version of\n\n\nHs(s)\n\nthat will generate the correct\n\n\nHz(z)\n\nvia bilinear with\n\n\nF ′s\n\n. 3.29. A product adopted an analog filter\n\n\nH(s) = 125×106 s3+103s2+500×103s+125×106\n\n, which was designed with [B,A]=butter(3,500,’s’) to have a cutoff frequency of 500 rad/s. This filter must be substituted by an equivalent system\n\n\nHe(s)\n\nthat relies on a digital filter\n\n\nH(z)\n\n. The anti-aliasing and reconstruction filters are ideal. The ADC and DAC converters should operate at\n\n\nFs = 800\n\n Hz. Design a digital filter\n\n\nH(z)\n\nwith the bilinear transform, such that\n\n\nHe(s)\n\nhas the same cutoff frequency of\n\n\nH(s)\n\n. 3.30. Design a causal FIR\n\n\nH(z)\n\nusing the windowing method. The filter must have 5 coefficients, cutoff frequency\n\n\nπ∕6\n\n rad and the adopted window must be Hann’s. The gain at DC must be unitary. The command hanning(5) in Matlab/Octave, returns [0.25, 0.75, 1.00, 0.75, 0.25]. 3.31. A filter was obtained with [b, a]=butter(3,1/3). a) Draw the block diagram representation for realizations using direct form I, direct form II, transposed direct form II, cascade of SOS and parallel using SOS. b) Using any programming language, implement the direct form II. Assume that there is a function called readAD() that reads a sample from the ADC and another writeDA() that writes a sample to the DAC, and that the processing loop is correctly invoked according to the chosen sampling frequency. In other words, complete the following code (this example is in C):\n\n\nmain( ) { \n  int x, y; \n  ... \n  while (1) { //eternal loop \n5    x = readAD( ); \n    ... \n    writeDA(y); \n  } \n  ... \n10}"
  },
  {
    "objectID": "ak_dsp_bookse49.html",
    "href": "ak_dsp_bookse49.html",
    "title": "49  Extra Exercises",
    "section": "",
    "text": "3.1. Develop and list the source code of a Matlab/Octave routine that implements the overlap-save method using a structure similar to Listing 3.9. 3.2. Using FilterPro as in Section 3.3.2.0 or similar software, design a second-order lowpass Butterworth filter H1(s) with passband frequency of 5 kHz. Show the schematics. Adopt commercial components with 10% of tolerance. What is the exact value of the passband frequency? Does it coincide with 5 kHz? After that, execute another design to obtain H2(s), trying to improve the attenuation using an approximation other than Butterworth’s. Comparing H1(s) and H2(s), what can you say on how good are their magnitudes and phases? 3.3. Design a digital Butterworth filter H(z) using the bilinear transform. The cutoff and rejection filters are π∕4 and 2π∕3, respectively. The linear gain at DC should be 1 and the minimum gain at the passband should be 0.95. The maximum gain at the rejection band is 0.01. This filter can be obtained with the commands:\n\n\n\nListing 3.51: MatlabOctaveCodeSnippets/snip_systems_buttord.m\n\n\nGs=20*log10(0.01) \nGp=20*log10(0.95) \nWp=1/4 \nWs=2/3 \n5[n, Wc] = buttord(Wp, Ws, -Gp, -Gs) \n[b, a]=butter(n,Wc)\n\n\n\n\nWhat are the gain and phase at frequencies π∕4 and 2π∕3 rad? b) What is the difference equation that corresponds to H(z)? 3.4.  The bilinear transform Eq. (3.69) was obtained by expanding the numerator and denominator of z = esTs∕2∕e−sTs using a first order Taylor expansion. Compare it with the expansion of z = esTs ≈ 1 + sTs.\n\nWhat are the reasons for using the former expansion? An alternative point of view is to calculate the series expansion of ln ⁡ z for the matched Z-transform s = ln ⁡ (z)∕Ts. Check the expansion for x &gt; 0 at [url3iit]. 3.5. Study the algorithm used to design [b, a]=butter(2,1/4) including the pre-warping stage. 3.6. Using the function fir1, compare the FIR filters with cutoff frequency π∕4 rad for the windows Hamming, Hann and rectangular. The commands can be\n\n\nbham=fir1(30,1/3,hamming(31)); \nbhan=fir1(30,1/3,hanning(31)); \nbret=fir1(30,1/3,rectwin(31));\n\n\nand freqz can help. How do they compare with respect to: attenuation, decay, side lobe and width of the main lobe. 3.7.  Observe that fir1 in Matlab uses a modified cutoff frequency fc. Instead of fc being the frequency of half the power at passband, for fir1 fc is the frequency of half the gain at passband. In other words, considering a filter with gain |H(0)| = G at DC, fir1 provides a filter with gain |H(fc)|≈ 0.5G (which corresponds to 20log ⁡ 100.5 ≈−6 dB), instead of the commonly adopted  − 3 dB value of |H(fc)|≈ G∕2. For the Hamming window, design an alternative to fir1 that allows the user to specify the cutoff frequency as the one in which |H(fc)|≈ G∕2. 3.8. The Kaiser window is very flexible. Study the project below that uses Fs = 11025 Hz, passband [0,1000] Hz, and rejection band [1200,Fs∕2]. The minimum attenuation at rejection band is 40 dB and the maximum ripple at the passband is 5% (i. e., the filter can go from 1 to 0.95 at the passband).\n\n\n[n,w,beta,ftype]=kaiserord([1000,1200],[1,0],[0.05,0.01],11025); \nfreqz(fir1(n,w,ftype,kaiser(n+1,beta),'noscale'),1,[],11025);\n\n\nDesign an IIR Butterworth that meets the requirements and compare the order with the Kaiser FIR filter. 3.9. Generate realizations (waveforms) of a Gauss-Markov process. Recall that a first-order Gauss-Markov process obeys s[n] = as[n − 1] + ν[n],n ≥ 0, where ν[n] is an i. i. d. Gaussian noise. Generate realizations of s[n] with N = 100 samples each, using ν[n] with unitary power, and for the following values of a: 0,.1,.9,.99,.5,2,3. Provide a qualitative description of the difference between |a| &lt; 1 and |a| &gt; 1. Calculate the autocorrelation of s[n] in terms of a (or obtain it from references such as [?] or the Web). 3.10. Study the anti-aliasing filters at [url3min]. 3.11. Using FilterPro, execute the project of a stopband using default options, which should lead to an order 6 and cutoff frequency of 1 kHz. a) What does it mean “Min GBW reqd.” in the end of the “Schematic”? b) how to interpret the Group Delay plot? c) Why each stage of SOS has f0, BW and Q? d) Use “Edit” to re-do the project using Chebyshev with a maximum of 1 dB ripple in the passband instead of Bessel. How did that impact the “Group delay”? 3.12. Study the advantages and disadvantages when comparing Butterworth, Chebyshev and Bessel responses. 3.13. When cascading the 2nd order stages, why the stages with lower quality factor Q are placed first by Filter Pro? 3.14. Compare the MFB and Sallen-Key topologies. Can you find a more sophisticated topology in the literature? If yes, describe it. 3.15. Briefly explain what sensitivity means."
  },
  {
    "objectID": "ak_dsp_bookch4.html",
    "href": "ak_dsp_bookch4.html",
    "title": "Spectral Estimation Techniques",
    "section": "",
    "text": "4.1  To Learn in This Chapter  4.2  Introduction  4.3  Windows for spectral analysis  4.3.1  Popular windows  4.3.2  Leakage  4.3.3  Picket-fence effect  4.3.4  Figures of merit applied to windows  4.3.5  Alternative representation of discrete-time sinusoids  4.3.6  Example of combined effects of leakage and picket-fence  4.3.7  Example of using windows in spectral analysis  4.3.8  Estimating sinusoid amplitude and correction of scalloping loss  4.4  The ESD, PSD and MS Spectrum functions  4.4.1  Energy spectral density (ESD)  4.4.2  Power spectral density (PSD)  4.4.3  Fourier Modulation Theorem Applied to PSDs  4.4.4  Mean-square (MS) spectrum  4.5  Filtering Random Signals and the Impact on PSDs  4.5.1  Response of LTI systems to random inputs  4.5.2  Continuous-time signals with a white PSD and their filtering  4.5.3  Discrete-time signals with a white PSD and their filtering  4.6  Nonparametric PSD Estimation via Periodogram  4.6.1  Discrete-time PSD estimation using the periodogram  4.6.2  Relation between MS spectrum and periodogram  4.6.3  Periodogram of periodic or energy signals  4.6.4  Examples of PSD and MS spectrum estimation  4.6.5  Estimating the PSD from Autocorrelation  4.7  Nonparametric PSD Estimation via Welch’s method  4.7.1  The periodogram variance does not decrease with N  4.7.2  Welch’s method for PSD estimation  4.8  Parametric PSD Estimation via Autoregressive (AR) Modeling  4.8.1  Spectral factorization  4.8.2  AR modeling of a discrete-time PSD  4.8.3  AR modeling of a continuous-time PSD  4.8.4  Yule-Walker equations and LPC  4.8.5  Examples of autoregressive PSD estimation  4.9  Time-frequency Analysis using the Spectrogram  4.9.1  Definitions of STFT and spectrogram  4.9.2  Wide and narrowband spectrograms  4.10  Applications  4.11  Comments and Further Reading\n 4.12  Exercises  4.13  Extra Exercises"
  },
  {
    "objectID": "ak_dsp_bookse50.html",
    "href": "ak_dsp_bookse50.html",
    "title": "50  To Learn in This Chapter",
    "section": "",
    "text": "Interpret the power spectral density (PSD)\n\n\nDistinguish PSD and the mean-square (power) spectrum (MS spectrum)\n\n\nUse FFT to estimate the PSD and MS spectrum\n\n\nPerform spectrum analysis with a computer\n\n\nDistinguish a spectrum analyzer and a vector network analyzer (VNA)\n\n\nPractice using windowing functions for spectral analysis"
  },
  {
    "objectID": "ak_dsp_bookse51.html",
    "href": "ak_dsp_bookse51.html",
    "title": "51  Introduction",
    "section": "",
    "text": "This chapter presents a brief introduction to spectral analysis. The main goal of spectral analysis is to estimate how the signal power is distributed over frequency. It is a very broad area with many applications. Beware that definitions and jargon vary over the literature! The discussion here will only focus on the important task of estimating the power spectrum (also called power spectral density).\n\n\n\nSpectral analysis differs from frequency estimation or spectral line analysis. The latter assumes there are few frequencies that need to be estimated while, in contrast, spectral analysis aims at estimating the whole spectrum. When comparing the two, it is intuitive that having only few frequencies of interest is beneficial. If that is the case, there are many frequency estimation algorithms that outperform a strategy based on, for example, estimating the spectrum via an FFT and picking its magnitude peaks.\n\n\n\nThere are specific frequency estimation methods for a single tone, multi-tone and multi-harmonic signals, for example. In the latter case, the signal of interest is composed by a sum of harmonically related sinusoids. This text concentrates in spectral analysis and the reader is encouraged to get information elsewhere about frequency estimation methods if he/she faces the task of finding a small set of frequencies that compose a given signal of interest.\n\n\n\nThe spectral analysis algorithms can be broadly divided in parametric and nonparametric. The former category encompasses algorithms that assume a model for the signal under analysis and estimate the parameters of this model. Examples of such models are autoregressive (AR), moving-average (MA) and ARMA (combination of the two), which can be seen as stochastic processes with realizations obtained by passing white noise through digital filters of types all-poles IIR 1∕A(z), FIR B(z), and an IIR with finite zeros B(z)∕A(z), respectively.\n\n\n\nFor these parametric models, the “parameters” are the coefficients of the corresponding filter and the noise power. The nonparametric algorithms try to estimate the spectrum without imposing that the signal of interest was generated by a model. This text discusses the following spectral analysis methods:\n\n &lt;ul class='itemize1'&gt;\n &lt;li class='itemize'&gt;&lt;span class='ec-lmbx-10x-x-109'&gt;Nonparametric&lt;/span&gt;: PSD estimation with the periodogram and Welch’s algorithm\n &lt;/li&gt;\n &lt;li class='itemize'&gt;&lt;span class='ec-lmbx-10x-x-109'&gt;Parametric&lt;/span&gt;: estimation of an AR model via linear predictive coding (LPC)&lt;/li&gt;&lt;/ul&gt;\n                                                                          \n                                                                          \n\n\nBecause of their importance in spectrum estimation, a brief review on windows is presented in the sequel."
  },
  {
    "objectID": "ak_dsp_bookse52.html",
    "href": "ak_dsp_bookse52.html",
    "title": "52  Windows for spectral analysis",
    "section": "",
    "text": "Windows were briefly discussed in the context of FIR design in Section 3.13.5. Here they are applied in spectral analysis to better control trade offs such as resolution and leakage.\n\n\n\nWindows are essential to model the extraction of a finite-duration segment of a signal with a potentially infinite duration. For example, an FFT is restricted to operate on N signal samples and even if the user is not aware or explicitly modeling the windowing operation, obtaining a segment of N samples from a longer signal x[n] is equivalent to multiplying x[n] by a rectangular window w[n] of duration N samples.\n\n\n\nWindowing in time domain is often performed by\n\n\n\n\n\n\n\n\n\n xw[n] = x[n]w[n], \n\n\n(4.1)\n\n\n\n\n\n\nas a pre-processing step. Then, for example, the FFT of the windowed signal xw[n] is calculated (the FFT of the original signal x[n] would be unfeasible due to its long duration).\n\n\n\nAs informed by Eq. (3.12), this multiplication x[n]w[n] in time-domain corresponds to a periodic convolution Xw(ejΩ) = X(ejΩ)⊛W(ejΩ) in frequency-domain between their respective spectra X(ejΩ) and W(ejΩ).\n\n\n\nReduced resolution and leakage are the two primary effects on the spectrum as a result of applying a window to the signal. The resolution of the overall spectral analysis\nis primarily influenced by the width of the main-lobe of the window, which is inversely proportional to the window (and consequently signal) duration. The degree of leakage depends on aspects of the window DTFT such as the relative amplitude of the main-lobe with respect to the amplitudes of the sidelobes. An FFT-based spectral analysis resolution also depends on the FFT resolution Δf given by Eq. (2.32). Given a windowed signal xw[n] with duration of N samples, the FFT resolution can always be increased via zero-padding. Using a FFT size larger than N decreases Δf, but cannot recover the leakage that occurred due to windowing.\n\n\n\nThe convolution in Eq. (3.12) may not significantly alter Xw(ejΩ) with respect to the original X(ejΩ) in case W(ejΩ) is relatively narrow compared to variations in X(ejΩ). However, because the tradeoff between narrower bandwidths and longer duration over time (see the Fourier transform of a square pulse in Eq. (B.54), for example), this condition requires the window w[n] to be sufficiently long (i. e., N must be sufficiently large). In the extreme (and ideal) situation of windowing not altering the original X(ejΩ), the window DTFT would be W(ejΩ) = δ(Ω + k2π),k ∈ ℤ. But this is equivalent to having w[n] with infinite duration and, obviously, not performing windowing at all.\n\n\n\n\n\n4.3.1  Popular windows\n\n\n\nFive of the most popular windows are described here.\n\n\n\n\n\n\n\n\nFigure 4.1: Selected windows w[n] of duration N = 32 samples in time-domain.\n\n\n\n\n\n\n\n\n\n\nFigure 4.2: The DTFT W(ejΩ) of the windows in Figure 4.1.\n\n\n\n\n\nFigure 4.1 shows some common windows for N = 32 samples in time-domain. Note that the maximum amplitude is equal to one. Figure 4.2 shows these windows in frequency-domain via their DTFTs.\n\n\n\n Example 4.1. Designing windows in Matlab/Octave. Listing 4.1 lists the commands to design windows with N = 256 samples using Matlab/Octave.\n\n\n\nListing 4.1: MatlabOctaveCodeSnippets/snip_frequency_windows.m\n\n\nN=256; %desired number of samples of window W \nW=rectwin(N); %Rectangular window \nW=hamming(N); %Hamming window \nW=hanning(N); %Hann window \n5W=kaiser(N,7.85); %Kaiser with beta=7.85 \nW=flattopwin(N); %Flat top window\n  \n\n\nAssuming discrete-time windows, the only input parameter for their design is their duration. The Kaiser windows (also called Kaiser-Bessel window) is the only exception among the five, because it also has a parameter β. For example, increasing β of a Kaiser window widens the main lobe and decreases the amplitude of the sidelobes.    □\n\n\n\n\n\n\n\n\nFigure 4.3: The DTFT of windows in Figure 4.1 with their values normalized such that |W(ejΩ)| = 1 for Ω = 0 rad.\n\n\n\n\n\nIt is common to normalize a window such that its DTFT has a value equal to one at DC. This was done to generate Figure 4.3. The normalization can use the fact that the DC value W(z)|z=1 coincides with the sum of the window coefficients. For example, a normalized Hamming window can be obtained with w=hamming(32);w=w/sum(w).\n\n\n\n Example 4.2. Equations describing popular windows. The Hamming window adopted here is given by\n\n\n\n\n\n\n\n\n\n w[n] = 0.54 − 0.46cos ⁡   ( 2πn N − 1 ),   n = 0,… ⁡ ,N − 1, \n\n\n(4.2)\n\n\n\n\n\n\nwhile the Hann window is\n\n\n\n\n\n\n\n\n\n w[n] = 0.5 − 0.5cos ⁡   ( 2πn N − 1 ),   n = 0,… ⁡ ,N − 1. \n\n\n(4.3)\n\n\n\n\n\n\nNote that according to Eq. (4.3), the first and last samples (n = 0 and N − 1) are zero and this version of the Hann window is called “periodic”. In Python, the commands import numpy and w = numpy.hanning(5) return [0.,0.5,1.,0.5,0.]. In Matlab/Octave, the command hanning(6,’periodic’) returns [0;0.25;0.75;1.00;0.75;0.25] while the commands hanning(5) and hanning(5,’symmetric’) return [0.25;0.75;1.00;0.75;0.25], without the zero-valued samples.\n\n\n\nThe flat-top window is\n\n\n\n w[n]  = 0.21557895 − 0.41663158cos ⁡   ( 2πn N − 1 ) + 0.277263158cos ⁡   ( 4πn N − 1 ) (4.4)    − 0.083578947cos ⁡   ( 6πn N − 1 ) + 0.006947368cos ⁡   ( 8πn N − 1 ),   n = 0,… ⁡ ,N − 1.    \n\nThe Kaiser windows for N odd, such that M = (N − 1)∕2 is an integer, is\n\n\n\n\n\n\n\n\n\n w[n] = I0  (β1 −  (n−M M ) 2)  I0  (β) ,   n = 0,…,N − 1, \n\n\n(4.5)\n\n\n\n\n\n\nwhere I0(x) is the 0th order Bessel function of the first kind, which is calculated in Matlab/Octave with besseli(0,x), as indicated in the following commands:\n\n\nN=13; beta=7.85; %produce same window as w=kaiser(N,beta): \nn=0:N-1; M=(N-1)/2; %auxiliary variables \nw=abs(besseli(0,beta*sqrt(1-((n-M)/M).^2)))/abs(besseli(0,beta)); \nw=w/max(w); %normalize to have maximum value equal to 1\n\n\nthat create a Kaiser window (for N odd).   □\n\n\n4.3.2  Leakage\n\n\n\nLeakage is a very important aspect on spectral analysis using FFT. To understand leakage one must remember that the multiplication x[n]w[n] in time-domain, between the signal x[n] and a window w[n], corresponds to the convolution in frequency-domain between their respective spectra X(ejΩ) and W(ejΩ).\n\n\n\nThe spectra of the windows in Figure 4.2 indicate that the values of the original spectrum X(ejΩ) will be surely altered if X(ejΩ) is convolved with any window. This happens because the spectrum W(ejΩ) of a finite-duration window can be very distinct from the ideal impulse δ(Ω) in frequency domain.\n\n\n\nTo make this point clear, consider that X(ejΩ) is the DTFT of a cosine of amplitude A and frequency Ω1, such that the impulses of area Aπ are located at  ±Ω1 + k2π,k ∈ ℤ. Assuming the window DTFT W(ejΩ) has value B at Ω = 0 (DC), according to Eq. (3.12), the windowing operation will lead to a convolution result with shifted versions of W(ejΩ) centered at  ±Ω1 with a peak value of 0.5(A × B).\n\n\n\nThe result of the convolution caused by windowing may not significantly impact the original spectrum X(ejΩ) in case W(ejΩ) is relatively narrow compared to variations in X(ejΩ). But this condition implies that the window w[n] is sufficiently long (i. e., N must be sufficiently large). As mentioned, the ideal situation, where windowing would not alter the original X(ejΩ), is to have W(ejΩ) as an impulse in the range  − π &lt; Ω ≤ π (i. e. W(ejΩ) = δ(Ω + k2π),k ∈ ℤ), but this implies that w[n] has infinite duration and all samples of x[n] are available.\n\n\n\nHence, even if W(ejΩ) is\nnarrow compared to X(ejΩ), the convolution Xw(ejΩ) = X(ejΩ)∗W(ejΩ) may have non-zero (“leaked”) values in frequencies where the original signal X(ejΩ) was zero.\n\n\n\nNote that many times, leakage is erroneously associated to using FFTs to perform spectral analysis. Even when dealing with DTFTs, without the inherent discretization of frequency provoked by FFTs, windowing the spectrum X(ejΩ) creates “frequency leaks” that show up on the DTFT Xw(ejΩ). Hence, leakage is associated to the DTFTs and should not be confused with the picket-fence effect caused by sampling the DTFT via a FFT, as discussed in the sequel.\n\n\n\n\n\n4.3.3  Picket-fence effect\n\n\n\nConceptually, the result of taking the FFT of a signal segment is the consequence of two steps: a) obtaining Xw(ejΩ), a version of X(ejΩ) with leakage due to windowing, and then b) discretizing Ω according to the frequency grid imposed by the FFT.\n\n\n\nThe consequence of discretizing Ω via an FFT is sometimes called picket-fence effect, which corresponds to observing the DTFT of the signal under analysis just at multiples of Δf from Eq. (2.32). These effects appear in a combined way in FFT-based spectral analysis, such as in Application 2.11.\n\n\n\nIt is useful to distinguish the two concepts: spectral leakage due to windowing and the picket-fence due to discretization of Ω according to FFT frequency grid.\n\n\n\n\n\n4.3.4  Figures of merit applied to windows\n\n\n\nFour of the most important parameters of a window are:\n\n &lt;ul class='itemize1'&gt;\n &lt;li class='itemize'&gt;width of its main lobe,\n                                                                          \n                                                                          \n &lt;/li&gt;\n &lt;li class='itemize'&gt;amplitude of its sidelobes,\n &lt;/li&gt;\n &lt;li class='itemize'&gt;rate of sidelobe decay as frequency increases,\n &lt;/li&gt;\n &lt;li class='itemize'&gt;and scallopping loss.&lt;/li&gt;&lt;/ul&gt;\n\n\nEach of them are discussed in the sequel. There are other figures of merit of windows, and the reader is invited to check the references about windows in Section 4.11.\n\n\n\n Example 4.3. Evaluating windows using Matlab’s wintool. Matlab’s wintool (Window Design & Analysis Tool) GUI is useful to explore trade offs in windowing. One can observe, for example, that increasing β of a Kaiser window widens the main lobe and decreases the amplitude of the sidelobes. It also calculates the leakage factor, relative sidelobe attenuation and main lobe width.    □\n\n\n\nWindows differ in their main-lobe width, which is typically inversely proportional to N, the length of the window. Thus, it is possible to achieve a better resolution in frequency by increasing N, but more signal samples are needed and sometimes they are not available.\n\n\n\nAnother important window parameter is the highest sidelobe level, which should have a small value. When a window is used for FIR design, the smaller the sidelobe amplitude with respect to the amplitude of the main lobe, the higher the filter attenuation in the stopband. From the spectra in Figure 4.3, Table 4.1 can be obtained, which indicates the difference in dB between the main lobe and the highest sidelobe amplitudes. It should be noticed that, for the Hamming window, the highest sidelobe is the third one. The first sidelobe is localized in the frequency equal to 0.46 rad and has a level of  − 50.53 dB when compared to the main-lobe level.\n\n\n\n\n\n\n\nTable 4.1: Difference in dB between the window main lobe and highest sidelobe amplitudes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWindow \n\n\nRectangular \n\n\nHann \n\n\nHamming \n\n\nKaiser (β = 7.85) \n\n\nFlat top \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDifference (dB) \n\n\n\n−13\n\n\n\n−32\n\n\n\n−43\n\n\n\n−57\n\n\n\n−68\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                                                                                                                                                                                                                                                                                                                                                               &lt;/div&gt;\n                                                                          \n                                                                          \n\n\n\nWhen comparing spectral components that are close to each other, the adopted windows must have good resolution and a low highest sidelobe level. When the spectral components are far apart, having a large sidelobe decay can be more important.\n\n\n\nThis sidelobe fall-off indicates how the peak amplitude of the sidelobes decay with frequency and, for example, is a motivation for using the Hann window. The side-lobe fall-off is dependent on the continuity of the window w(t) in time-domain and its derivatives. For example, the Hann window is continuous, and besides, its first derivative is continuous too. Thus, its transform falls off at 1∕ω3 or, equivalently, at  − 18 dB per octave. In contrast, the rectangular and Hamming windows, which are not continuous at their endpoints, have a  − 6 dB per octave fall-off.\n\n\n\nWhen using FFT to estimate the amplitude or power of a sinusoid, the combined effect of picket-fence and leakage creates a variation on the estimated sinusoid amplitude called “scalloping”. For most windows, the so-called scalloping loss is worst when the frequency is half-way the two neighboring bins. Listing 4.2 allows to observe the scalloping loss when using the rectangular window for obtaining the FFT of a cosine.1\n\n\n\n Example 4.4. When a DTFT has impulses, does this create a problem for an FFT-based analysis?. A DTFT can have impulses such as, for example, the DTFT of a sinusoid. One question that may arise then is: -If the FFT simply performs sampling of the DTFT, what is the FFT value when exactly sampling an impulse? Should this value go to infinite? The answer requires noting that the impulse in the DTFT appears because x[n] is an infinite-long signal (e. g., an eternal sinusoid). But an FFT has to be applied to a signal with a finite number N of samples. Therefore, the FFT is always applied to a windowed signal xw[n], with a DTFT Xw(ejΩ) that does not have impulses.   □\n\n\n4.3.5  Alternative representation of discrete-time sinusoids\n\n\n\nGiven that the bin width of an N-points DFT (or FFT) is 2π N  radians, the angular frequency associated to the FFT bin k is\n\nΩk = k2π N  radians, as given by Eq. (2.35).\n\n\n\nWhen performing the spectral analysis of a sinusoid x[n] = cos ⁡ (k2π N n) using an N-points FFT, the components of “positive” and “negative” frequencies of x[n] will be mapped to the FFT bins k and (N − k), respectively. Therefore, it is useful to generalize the signal expression to x[n] = cos ⁡ (α2π N n), where α ∈ ℝ is a real number that allows to obtain any desired value for the angular frequency Ωc = α2π N .\n\n\n\nTo check the periodicity of x[n], one can use Eq. (1.39) and write\n\n\n\n\n\n\n\n\n\n  m N0 = Ωc 2π = 2π N α 2π = α N, \n\n\n(4.6)\n\n\n\n\n\n\nwhich is a ratio m∕N0 of two integers when α∕N is a rational number (α∕N ∈ ℚ). For instance, assume the signal of interest is x[n] = cos ⁡ ((100π∕256)n), with α = 50 and N = 256, where N is the adopted FFT size. In this special case, in which α ∈ ℤ, the fundamental period is N0 = 128 samples, given that simplifying the fraction α∕N leads to m∕N0 = 25∕128, and this cosine is represented by the FFT bins k = 50 and k = 256 − 50 = 206.\n\n\n\nAs another example, consider the signal x[n] = cos ⁡ ((61π∕256)n) and an FFT\nsize N = 256. In this case, α = 30.5 and α∉ℤ, but α∕N ∈ ℚ and can be written as a ratio 30.5∕256 = 61∕512 of two integers m = 61 and N0 = 512. The signal x[n] has a period of N0 = 512 samples but only N = 256 of its samples are being analyzed via the FFT. In this case, the FFT does not “see” a pure sinusoid. The component of x[n] corresponding to the positive frequency will reside between the bins k = 30 and k = 31, while the negative-frequency component will be located between bins k = 224 and 225. Leakage will occur for both components and will be more visible in their neighbor bins.\n\n\n\n\n\n4.3.6  Example of combined effects of leakage and picket-fence\n\n\n\nIt is informative to revisit the result of an FFT when the original input signal is a discrete-time sinusoid x[n] = Acos ⁡ (Ωcn) after windowing. Note that the eternal cosine DTFT is X(ejΩ) = Aπ[δ(Ω + Ωc) + δ(Ω −Ωc)] within the range  − π ≤Ω ≤ π, then repeated in multiples of 2π.\n\n\n\nAssuming that Xw(ejΩ) denotes the DTFT of the windowed cosine, the final FFT result depends on the two effects:\n\n\n\nf 1.\n\n\nLeakage: Xw(ejΩ) is the periodic convolution between X(ejΩ) and the window DTFT W(ejΩ). Assuming a rectangular window w[n] with non-zero amplitudes from n = 0,1,…,N − 1, its spectrum is\n\n\n\n\n\n\n\n\n W(ejΩ) = sin ⁡   (NΩ 2 )   sin ⁡   (Ω 2 )  e−jΩ(N−1) 2 . \n\n\n(4.7)\n\n\n\n\n &lt;!-- l. 338 --&gt;&lt;p class='noindent'&gt;Recalling that the periodic convolution of Eq. (&lt;a href='ak_dsp_bookse34.html#x43-127007r12'&gt;3.12&lt;!-- tex4ht:ref: eq:windowingConvolution  --&gt;&lt;/a&gt;) has a factor\n &lt;!-- l. 338 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt; &lt;mfrac&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;\n2π, the resulting DTFT is\n\n\n\n\n\n\n\n\n\n Xw(ejΩ) = A 2  [W(ej(Ω+Ωc)) + W(ej(Ω−Ωc))] \n\n\n(4.8)\n\n\n\n\n &lt;/dd&gt;&lt;dt class='enumerate-enumitem'&gt;&lt;span class='ec-lmbx-10x-x-109'&gt;f&lt;/span&gt;\n\n\n\nPicket-fence: The FFT Xw[k] has a resolution of ΔΩ = (2π)∕N and corresponds to samples Xw[k] = Xw(ejΩ)|Ω=kΔΩ of the DTFT Xw(ejΩ) at frequencies Ω = kΔΩ.\n\n\n\n\nThe FFT values Xw[k] for the windowed cosine xw[n] = x[n]w[n], with Ωc = αΔΩ = α(2π)∕N and α ∈ ℝ, is obtained substituting Eq. (4.7) in Eq. (4.8):\n\n\n\n\n\n\n\n\n\n\n Xw[k] = A 2  [sin ⁡   ((k + α)π) sin ⁡   ((k+α)π N )  e−j ((N−1)π N (k+α)) + sin ⁡   ((k − α)π) sin ⁡   ((k−α)π N )  e−j ((N−1)π N (k−α))] . \n\n\n(4.9)\n\n\n\n\n\n\nFrom Eq. (4.7), for Ω = 0 rad, the value of W(ejΩ) can be found using L’Hospital’s rule, which leads to W(ejΩ)|Ω=0 = N. In the special case of Ωc = 0, the signal x[n] = A has a constant amplitude, its DTFT is Xw(ejΩ) = AW(ejΩ) and the FFT DC bin Xw[k]|k=0 = Xw[0] = AN. Recall from Eq. (2.44), that one needs to normalize the DFT by N in order to associate the DFT coefficients to the amplitude in Volts of the signal components in time-domain. In this case, the normalized DC value is Xw[0]∕N = A. Similarly, when Ωc≠0, the two DTFT peaks at Ω = ±Ωc have amplitude AN∕2. Normalizing the peak values by N, leads to an amplitude of A∕2 for each.\n\n\n\nTwo cases of interest are α = k ′ and α = k ′∕2, where k ′∈ ℤ, which correspond to Ωc centered on a bin and exactly in the middle of two bins, respectively.\n\n\n\nListing 4.2 compares the magnitudes of the DTFT and N = 32-points DFT for a signal x[n] = cos ⁡ (Ωcn), with Ωc = (2πα)∕N and α = 8.3. This windowed cosine is not bin-centered and the leakage is visible through the “picket-fence” corresponding to the DFT bins. The cosine has an angular frequency Ωc = 1.6297 rad and the closest FFT bin is k ′ = round(α) = 8. In this case, the DFT magnitude |Xw[k ′]| = 84.77 at k ′ = 8 would lead to an estimated\nfrequency Ω^c = 2πk ′∕N = 1.5708 rad. The DTFT magnitude peak at Ωc = 1.6297 rad is AN∕2 = 6 × 32∕2 = 96, such that the scalloping loss is 96 − 84.77 = 11.23.\n\n\n\nListing 4.2: MatlabOctaveCodeSnippets/snip_frequency_fftCosineExample.m\n\n\nfunction snip_frequency_fftCosineExample() \nclf, N=32; A=6; %clear figure, FFT length, cosine amplitude \nM=1000; %number of sample points imposing the DTFT resolution \nalpha=8.3; %specifies the cosine frequency \n5Wc=(alpha*2*pi)/N; %cosine frequency in radians \nW=linspace(0,2*pi,M); %frequency range \nXw=zeros(1,M); %DTFT values \nfor i=1:M %loop over frequencies and calculate DTFT values \n  Xw(i) = (A/2)*(rect_dtft(W(i)+Wc, N)+rect_dtft(W(i)-Wc, N)); \n10end \nn=0:N-1; x=A*cos(Wc*n); %generate N samples of the cosine \nXfft = fft(x); %calculate FFT with N points \ndisp(['Max(abs(FFT))=' num2str(max(abs(Xfft)))]) \ndisp(['Scalloping loss in DTFT=' num2str(A*N/2-max(abs(Xfft)))]) \n15plot(W/pi,abs(Xw)),hold on, stem([0:N-1]*(2*pi/N)/pi,abs(Xfft),'or') \nxlabel('Frequency \\Omega (rad) normalized by \\pi)') \nylabel('Magnitude'), legend('DTFT','DFT'),grid \nend \nfunction dtft_value = rect_dtft(Omega, N) %DTFT of rectangular window \n20  if Omega == 0 \n    dtft_value = N; %L'Hospital rule to correct NaN if 0/0 \n  else \n    W_div_2 = Omega/2; %speed up things computing only once \n    dtft_value=(sin(N*W_div_2)/sin(W_div_2))*exp(-1j*W_div_2*(N-1)); \n25  end \nend\n  \n\n\nIf one varies α in Listing 4.2 to α = 8 and 8.5, the maximum DFT amplitudes at k ′ = 8 are |Xw[k ′]| = 96 and 61.51, respectively. Therefore, the corresponding scalloping losses are 0 (the minimum possible value) and 96 − 61.51 = 34.49. Observe that when estimating the cosine amplitude, one would divide the FFT peak amplitude by N. For α = 8.5, this normalization would lead to an estimate of 2 × 61.51∕32 ≈ 3.84 Volts instead of the correct value of A = 6 V. This indicates how deleterious the combination of leakage and picket-fence can be.\n\n\n\n\n\n4.3.7  Example of using windows in spectral analysis\n\n\n\nTo illustrate the importance of windows, two distinct signals, x1[n] and x2[n], each composed by a stronger and a weaker sinusoids, will have their spectra estimated using a FFT with N = 256 points:\n\n &lt;ul class='itemize1'&gt;\n &lt;li class='itemize'&gt;\n &lt;!-- l. 389 --&gt;&lt;p class='noindent'&gt;Case 1: the stronger sinusoid is bin-centered - The first sequence is &lt;/p&gt;&lt;table class='equation-star'&gt;&lt;tr&gt;&lt;td&gt;\n                                                                          \n                                                                          \n &lt;!-- l. 390 --&gt;&lt;math class='equation' display='block' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;\n           &lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;[&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;]&lt;/mo&gt; &lt;mo class='MathClass-rel' stretchy='false'&gt;=&lt;/mo&gt; &lt;mn&gt;1&lt;/mn&gt;&lt;mtext class='qopname'&gt;cos&lt;/mtext&gt;&lt;mo&gt; ⁡&lt;!-- FUNCTION APPLICATION --&gt; &lt;/mo&gt;&lt;!-- nolimits --&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mn&gt;64&lt;/mn&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;mo class='MathClass-bin' stretchy='false'&gt;∕&lt;/mo&gt;&lt;mn&gt;256&lt;/mn&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt; &lt;mo class='MathClass-bin' stretchy='false'&gt;+&lt;/mo&gt; &lt;mi&gt;π&lt;/mi&gt;&lt;mo class='MathClass-bin' stretchy='false'&gt;∕&lt;/mo&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt; &lt;mo class='MathClass-bin' stretchy='false'&gt;+&lt;/mo&gt; &lt;mn&gt;100&lt;/mn&gt;&lt;mtext class='qopname'&gt;cos&lt;/mtext&gt;&lt;mo&gt; ⁡&lt;!-- FUNCTION APPLICATION --&gt; &lt;/mo&gt;&lt;!-- nolimits --&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mn&gt;76&lt;/mn&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;mo class='MathClass-bin' stretchy='false'&gt;∕&lt;/mo&gt;&lt;mn&gt;256&lt;/mn&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt; &lt;mo class='MathClass-bin' stretchy='false'&gt;+&lt;/mo&gt; &lt;mi&gt;π&lt;/mi&gt;&lt;mo class='MathClass-bin' stretchy='false'&gt;∕&lt;/mo&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt;&lt;mo class='MathClass-punc' stretchy='false'&gt;.&lt;/mo&gt;\n\n\n\n\n\n &lt;!-- l. 394 --&gt;&lt;p class='noindent'&gt;The idea is that the weaker cosine (of amplitude 1 V) is located in the center of the\n FFT bin &lt;!-- l. 394 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;mi&gt;k&lt;/mi&gt; &lt;mo class='MathClass-rel' stretchy='false'&gt;=&lt;/mo&gt; &lt;mn&gt;32&lt;/mn&gt;&lt;/mrow&gt;&lt;/math&gt;,\n and  the  stronger  is  located  in  the  center  of  bin  38.  Having  a  bin-centered\n sinusoid corresponds to choosing the FFT length coinciding with a multiple of\n the sinusoid period.\n &lt;/p&gt;&lt;/li&gt;\n &lt;li class='itemize'&gt;\n &lt;!-- l. 396 --&gt;&lt;p class='noindent'&gt;Case 2: the stronger sinusoid is not bin-centered - The second sequence is\n &lt;/p&gt;&lt;table class='equation-star'&gt;&lt;tr&gt;&lt;td&gt;\n &lt;!-- l. 397 --&gt;&lt;math class='equation' display='block' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;\n           &lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;[&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;]&lt;/mo&gt; &lt;mo class='MathClass-rel' stretchy='false'&gt;=&lt;/mo&gt; &lt;mn&gt;1&lt;/mn&gt;&lt;mtext class='qopname'&gt;cos&lt;/mtext&gt;&lt;mo&gt; ⁡&lt;!-- FUNCTION APPLICATION --&gt; &lt;/mo&gt;&lt;!-- nolimits --&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mn&gt;64&lt;/mn&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;mo class='MathClass-bin' stretchy='false'&gt;∕&lt;/mo&gt;&lt;mn&gt;256&lt;/mn&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt; &lt;mo class='MathClass-bin' stretchy='false'&gt;+&lt;/mo&gt; &lt;mi&gt;π&lt;/mi&gt;&lt;mo class='MathClass-bin' stretchy='false'&gt;∕&lt;/mo&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt; &lt;mo class='MathClass-bin' stretchy='false'&gt;+&lt;/mo&gt; &lt;mn&gt;100&lt;/mn&gt;&lt;mtext class='qopname'&gt;cos&lt;/mtext&gt;&lt;mo&gt; ⁡&lt;!-- FUNCTION APPLICATION --&gt; &lt;/mo&gt;&lt;!-- nolimits --&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mo class='MathClass-open' stretchy='false'&gt;(&lt;/mo&gt;&lt;mn&gt;75&lt;/mn&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;mo class='MathClass-bin' stretchy='false'&gt;∕&lt;/mo&gt;&lt;mn&gt;256&lt;/mn&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt;&lt;mi&gt;n&lt;/mi&gt; &lt;mo class='MathClass-bin' stretchy='false'&gt;+&lt;/mo&gt; &lt;mi&gt;π&lt;/mi&gt;&lt;mo class='MathClass-bin' stretchy='false'&gt;∕&lt;/mo&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;mo class='MathClass-close' stretchy='false'&gt;)&lt;/mo&gt;&lt;mo class='MathClass-punc' stretchy='false'&gt;.&lt;/mo&gt;\n\n\n\n\n\n &lt;!-- l. 401 --&gt;&lt;p class='noindent'&gt;In this case, the weaker cosine is located in the center of bin 32 but the stronger\n is located in the border of bins 37 and 38.&lt;/p&gt;&lt;/li&gt;&lt;/ul&gt;\n\n\nIn this example the goal is to distinguish the sinusoids and estimate their frequencies. As in many applications of spectral analysis, the amplitudes are irrelevant and what is taken into account is the relative difference between the signal components in frequency domain. For both x1[n] and x2[n], the difference in power between the sinusoids is 20log ⁡ 10(100∕1) = 40 dB. These sequences can be generated as in Listing 4.3.\n\n\n\nListing 4.3: MatlabOctaveCodeSnippets/snip_frequency_sequence_generation.m\n\n\nN=256; %number of samples available of x1 and x2 \nn=0:N-1; %abscissa \nkweak=32; %FFT bin where the weak cosine is located \nkstrong1=38; %FFT bin for strong cosine in x1 \n5weakSigal = 1*cos((2*pi*kweak/N)*n+pi/3); %common parcel \nx1=100*cos((2*pi*kstrong1/N)*n+pi/4) + weakSigal; %x1[n] \nkstrong2=37.5; %location for strong cosine in x2 \nx2=100*cos((2*pi*kstrong2/N)*n+pi/4) + weakSigal; %x2[n]\n  \n\n\n\n\n\n\n\nFigure 4.4: Comparison of spectra obtained with four windows in case both sinusoids are bin-centered (left plots) and not (right plots).\n\n\n\n\n\nThe commands that design the windows to pre-multiply x1 and x2 in Listing 4.3, before calling the FFT, are given in Listing 4.1.\n\n\n\nThe results with four windows from Listing 4.1 are shown in Figure 4.4. The flat top window was not used due to its poor frequency resolution. For example, the spectrum in Figure 4.4 for x2[n] (called x2 in the code) using Hamming was obtained with Listing 4.4.\n\n\n\nListing 4.4: MatlabOctaveCodeSnippets/snip_frequency_analysis.m\n\n\nN=256; n=0:N-1; %N is the number of available samples and FFT length \nx2=100*cos((2*pi*37.5/N)*n+pi/4) + cos((2*pi*32/N)*n+pi/3); %cosines \ndw=(2*pi)/N; %DFT spacing in radians \nw=-pi:dw:pi-dw; %abscissa for plots matched to fftshift \n5x=x2.*hamming(N)'; %perform windowing \nfactor=max(abs(fft(x))); %normalization to have stronger at 0 dB \nplot(w,fftshift(20*log10(abs(fft(x)/factor))));\n  \n\n\nThe graphs at the left column in Figure 4.4 correspond to estimated spectra of x1[n] (both sinusoids are bin-centered). For x1[n], it can be seen that the best result was obtained by a rectangular window while the other windows led to spectra with leakage (spurious power) near the cosines. Based solely on the results obtained for case 1, one could erroneously concluded that the rectangular window is always the best. However, as shown in the top-right graph, when the strong cosine is not bin-centered, the rectangular window miserably fails to detect the weaker cosine.\n\n\n\nIn Figure 4.4, the Hann window allows a marginal detection of the weaker cosine, while the Hamming window allows the detection of both cosines but contaminates the whole spectrum with spurious power along frequency just 50 dB below the power level of the strongest sinusoid. This is a consequence of the reduced side-lobe fall-off for the Hamming window, as indicated in Figure 4.3. The Kaiser window achieves the best result when one takes in account both cases (bin-centered and not).\n\n\n\n\n\n\n\n\nFigure 4.5: Individual spectra of the two sinusoids superimposed obtained using the Kaiser window.\n\n\n\n\n\n\n\n\n\n\nFigure 4.6: Individual spectra of the two sinusoids superimposed, obtained using the rectangular window.\n\n\n\n\n\nTo complement the analysis of estimating the spectrum of x2[n] (the stronger sinusoid is not bin centered), Figure 4.5 and Figure 4.6 show the individual spectra of the two sinusoids superimposed, obtained using the Kaiser and rectangular windows, respectively. Note that, Figure 4.4 shows exactly the combined effect of these individual spectra. Figure 4.6 clearly shows that the significant leakage of the stronger cosine caused by the rectangular window, completely “masks” the weaker cosine.\n\n\n\nNote that this was a specific application. For example, features such as side-lobe fall-off are important in other situations. A very general conclusion is to always use a window other than the rectangular when performing spectral analysis of a signal that is not guaranteed to have all components bin-centered.\n\n\n4.3.8  Estimating sinusoid amplitude and correction of scalloping loss\n\n\n\nIn some applications of FFT-based spectral analysis it is necessary not only detect the frequencies but also the amplitudes of the signal frequency components. In this case, the impact of windowing needs to be taken in account, by scaling the FFT of the windowed signal.\n\n\n\nFor instance, if one is calculating the harmonic distortion solely based on percentage values, such as: “- the third harmonic has 12% of the amplitude of the fundamental frequency component”, then mitigating the scalloping loss may not be needed. But if the analysis requires the absolute amplitude values of the harmonic components, the procedure to be discussed in the next paragraphs become important.\n\n\n\nFor convenience, we assume a complex exponential x[n] = Aej2πk ′n∕N, where A is the amplitude, N is the FFT-size and k ′ is the FFT bin in which x[n] resides. Note that x[n] is bin-centered. The windowed signal xw[n] = Aej2πk ′n∕Nw[n] is obtained by multiplying x[n] by a window w[n] with support of N samples. Due to the orthogonality of the FFT basis functions, all FFT coefficients Xw[k] will be zero but the one corresponding to bin k ′. This coefficient is given by:\n\n\n\n\n\n\n Xw[k ′] = ∑ n=0N−1x w[n]e−j2πk ′n∕N = A∑ n=0N−1w[n]ej2πk ′n∕Ne−j2πk ′n∕N = A∑ n=0N−1w[n]. \n\n\n\n\n\n\nTherefore, to obtain an estimate Â of the correct amplitude A from an FFT value Xw[k], one should use\n\n\n\n\n\n\n\n\n\n Â = |Xw[k]| ∑ n=0N−1w[n]. \n\n\n(4.10)\n\n\n\n\n\n\nNote that one divides the FFT result by N when wants to interpret it in Volts, but this is already incorporated in Eq. (4.10). If the division by N is already incorporated in a given software routine, one needs to multiply the FFT X[k] by N∕∑ ⁡ n=0N−1w[n] in order to compensate the later division by N.\n\n\n\nListing 4.5 illustrates the procedure for estimating the amplitude A = 6 V of a cosine. When using the rectangular window for a frequency Ωc located half-way two neighboring bins (alpha=8.5), the result is:\n\nMax(abs(scaled FFT))=1.9221\nScalloping loss = 1.0779\nCorrect amplitude (Volts) = 6\nEstimated amplitude (Volts) = 3.8442\nAmplitude error (Volts) = 2.1558\nAmplitude error (%) = 35.9301\n\n\n\nThe amplitude error in this case is 35.9%, which is due to the worst-case scalloping loss for the rectangular window in this situation of a sinusoid exactly half-way the bins.2\n\n\n\nListing 4.5: MatlabOctaveCodeSnippets/snip_frequency_scalloping.m\n\n\nwindow_choice = 1; %choose one among 3 possible windows \nclf, N=32; A=6; %clear figure, FFT length, cosine amplitude \nalpha=8.5; %specifies the cosine frequency \nWc=(alpha*2*pi)/N; %cosine frequency in radians \n5n=0:N-1; x=A*cos(Wc*n); %generate N samples of the cosine \nswitch (window_choice) \n  case 1 \n    this_window = rectwin(N); \n  case 2 \n10    this_window = hanning(N); \n  case 3 \n  this_window = flattopwin(N); \nend \namplitude_scaling = sum(this_window); %factor to mitigate scalloping \n15xw = x.* transpose(this_window); %multiply in time-domain \nXw_scaled_fft = fft(xw)/amplitude_scaling; %N-points FFT and scale it \nmax_fft_amplitude = max(abs(Xw_scaled_fft)); \ndisp(['Max(abs(scaled FFT))=' num2str(max_fft_amplitude )]) \nscalloping_loss=(A/2)-max_fft_amplitude; \n20disp(['Scalloping loss = ' num2str(scalloping_loss)]) \ndisp(['Correct amplitude (Volts) = ' num2str(A)]) \ndisp(['Estimated amplitude (Volts) = ' num2str(2*max_fft_amplitude)]) \namplitude_error = A - 2*max_fft_amplitude; \ndisp(['Amplitude error (Volts) = ' num2str(amplitude_error)]) \n25disp(['Amplitude error (%) = ' num2str(100*amplitude_error/A)]) \nak_impulseplot([A/2, A/2],[Wc,2*pi-Wc]/pi,[]); %plot cosine impulses \nhold on, stem([0:N-1]*(2*pi/N)/pi,abs(Xw_scaled_fft),'or') \nxlabel('Frequency \\Omega (rad) normalized by \\pi)') \nylabel('FFT magnitude and impulse area scaled by \\pi'), grid\n  \n\n\nIn case one chooses the flat-top window in Listing 4.5 (window_choice=3), the results are:\n\nMax(abs(scaled FFT))=2.9949\nScalloping loss = 0.0050728\nCorrect amplitude (Volts) = 6\nEstimated amplitude (Volts) = 5.9899\nAmplitude error (Volts) = 0.010146\nAmplitude error (%) = 0.16909\n\n\n\nThe flat-top window does not have a good frequency resolution but reaches an amplitude estimation error of only 0.17%. Because of that, it is widely adopt to calibrate equipment.3 The Hann window is a trade-off between frequency resolution and amplitude error, reaching 14.24% of amplitude error when one uses window_choice=2.\n\n\n\nAfter this short introduction to windowing, the next sections discuss three important functions in spectral analysis.\n\n\n \n\n1 Scalloping loss is better understood by interpreting FFT as a filter-bank, as in Application 4.4.\n\n \n\n2 Similar to the amplitudes, the absolute power values inferred from of an FFT-based analysis require correction  [?].\n\n \n\n3 See, e. g., the presentation by Matt Ettus at http://www.youtube.com/watch?v=wqKNUXDdIvU."
  },
  {
    "objectID": "ak_dsp_bookse53.html",
    "href": "ak_dsp_bookse53.html",
    "title": "53  The ESD, PSD and MS Spectrum functions",
    "section": "",
    "text": "4.4.1  Energy spectral density (ESD)\n\n\n\nThe ESD is a convenient tool for analyzing energy signals, informing how the total energy E is distributed over frequency. The (Parseval) Theorem 2 (at page §) for block transforms can be extended to continuous and discrete-time signals. For continuous-time, together with Eq. (1.40), one has\n\n\n\n\n\n\n\n\n\n E = ∫ −∞∞|x(t)|2dt = ∫ −∞∞|X(f)|2df, \n\n\n(4.11)\n\n\n\n\n\n\nstating that the energy is the same in both time and frequency domains. This suggests defining G(f) = |X(f)|2 as the ESD in continuous-time because G(f) describes how the signal energy E is spread over frequency and its integral is E. The adopted units for the ESD are Joules/Hz.\n\n\n\n Example 4.5. ESD of a real-valued exponential. Consider the signal x(t) = e−atu(t) in Volts, where a = 0.9. Its Fourier transform is X(f) = 1∕(j2πf + a) and the ESD in Joules/Hertz is\n\n\n\n\n\n\n G(f) = |X(f)|2 = 1 (2πf)2 + a2. \n\n\n\n\n\n\nThe signal energy is E = ∫ −∞∞|x(t)|2dt = ∫ −∞∞G(f)df ≈ 0.55556. Listing 4.6 illustrates how E can be estimated in time-domain and also using G(f). In this case, the range ] −∞,∞[ is simulated as [0,30] s and [−1000,1000] Hz in time and frequency-domain, respectively.\n\n\n\nListing 4.6: MatlabOctaveCodeSnippets/snip_frequency_exp_esd.m\n\n\n%% Generate time-domain signal \nTs=0.0001; %defines resolution in time \nt=0:Ts:30; %time axis \na=0.9; %constant \n5x=exp(-a*t); %signal \nsubplot(211), plot(t,x) \nxlabel('t (s)'); ylabel('x(t) amplitude') \n%% Generate ESD \nf=linspace(-2,2,1000); %frequency axis \n10Gf= 1 ./ ((2*pi*f).^2 + a^2); %ESD \nsubplot(212) \nplot(t,x), plot(f,Gf) \nxlabel('f (Hz)'); ylabel('ESD (J/Hz)') \n%% Confirm Parseval theorem \n15energy_equation = 1/(2*a); %theoretical \nenergy_time = sum(abs(x).^2)*Ts; %integration \ndf=0.0001; %defines resolution in frequency \nmin_f = -1000; %range of interest \nmax_f = 1000; \n20f=min_f:df:max_f; %frequency axis \nGf= 1 ./ ((2*pi*f).^2 + a^2); %recalculate ESD \nenergy_frequency = sum(Gf)*df; %integration \ndisp(['Theoretical energy =' num2str(energy_equation) ' Joules']) \ndisp(['Integration in time =' num2str(energy_time) ' Joules']) \n25disp(['Integration in frequency =' num2str(energy_frequency) ' J']) \n%% Energy in given frequency band \nmin_f = 0.2; max_f = 1; %range of interest \nf=min_f:df:max_f; %new frequency axis \nGf= 1 ./ ((2*pi*f).^2 + a^2); %recalculate ESD \n30energy_band = 2*sum(Gf)*df; %integration using 2 times \ndisp(['Energy in band =' num2str(energy_band) ' Joules'])\n  \n\n\nListing 4.6 also illustrates how G(f) can be conveniently used to estimate the energy within a given frequency band. In this example, the energy within the range from 0.2 to 1 Hz, considering both negative and positive frequencies, is Er = 2∫ 0.21G(f)df ≈ 0.16954 J.    □\n\n\n\n Example 4.6. ESD of a sinc in time-domain. Now consider the signal is x(t) = 400sinc(100t) in Volts. According to Eq. (B.55) and using the linearity of the Fourier transform to take in account a factor of 4, one has X(f) = 4 for |f|≤ 50 Hz and 0 otherwise. Hence, the ESD of x(t) is G(f) = 16 for |f|≤ 50 Hz and 0 otherwise. In this case, using the ESD instead of x(t), it is easy to conclude that the total signal energy is\n\nE = 16 × 100 = 1600 Joules.    □\n\n\n\nThere is a subtle issue when using radians per second instead of Hertz. In this case, G(ω) = |X(ω)|2 and the factor 1∕(2π) must be taken in account, such that:\n\n\n\n\n\n\n\n\n\n E = ∫ −∞∞|x(t)|2dt = 1 2π∫ −∞∞|X(ω)|2dω = 1 2π∫ −∞∞G(ω)dω. \n\n\n(4.12)\n\n\n\n\n\n\nAs a consequence of these definitions and taking in account that G(ω) = G(2πf), the unit of G(ω) is Joules/Hz, not Joules/(rad/s). This issue is similar to the one discussed in Section 2.5.4 for the Fourier transforms X(f) and X(ω).\n\n\n\n Example 4.7. The unit of G(ω) is Joules/Hz, not Joules/(rad/s). For instance, take the sinc signal of Example 4.6, which has a flat ESD given by G(f) = 16 J/Hz within the range [−50,50] Hz and zero otherwise. The representation G(ω) of this ESD in rad/s has the same constant value G(ω) = 16 but now over the range [−100π,100π] rad/s. For the sake of argument, let us assume G(ω) = 16 were interpreted as indicating values in J/(rad/s). Within the range [−100π,100π], one would have 16 × 200π = 3200π J. But we know the correct value is 1600 J.   □\n\n\n\nIt is then more adequate to interpret that G(ω) provides the same numerical values of G(f)\nin J/Hz, but when G(ω) is shown in a plot or integrated, its abscissa range is enlarged by a factor of 2π to indicate angular frequency ω = 2πf.\n\n\n\nOne could eventually adopt a definition G~(ω) = 1 2π|X(ω)|2 that incorporates the 1∕(2π) factor such that G~(ω) would be given in J/(rad/s). But this would break the connection between G(f) and G(ω) and is not adopted. This reasoning is also applied when dealing with the power spectral density.\n\n\n\nIn continuous-time spectral analysis, the linear frequency in Hertz is more convenient and angular frequencies are seldom adopted. But this is not an option in discrete-time processing, given that Ω is an angle (assumed in radians) and does not have a counterpart in Hertz. Hence, the ESD in discrete-time\n\n\n\n\n\n\n\n\n\n G(ejΩ) = |X(ejΩ)|2 \n\n\n(4.13)\n\n\n\n\n\n\nis given in Joules per normalized frequency rad/(2π), such that\n\n\n\n\n\n\n\n\n\n E = ∑ n=−∞∞|x[n]|2 = 1 2π∫ &lt;2π&gt;|X(ejΩ)|2dΩ. \n\n\n(4.14)\n\n\n\n\n\n\n Example 4.8. Creation of digital frequency Dhertz (DHz) such that the unit of G(ejΩ) is Joules/DHz. Assume a normalized digital frequency 𝔽 defined as\n\n\n\n\n\n\n\n\n\n 𝔽=def Ω 2π, \n\n\n(4.15)\n\n\n\n\n\n\nwhere Ω is the discrete-time angular frequency in radians. The digital linear frequency 𝔽 is also an angle but we will interpret its unit as Dhertz (Dhz), the digital Hertz. The continuous-time Hertz can be seen as the number of occurrences in one second, while Dhertz is interpreted as the number of occurrences between two neighboring discrete-time samples n and n + 1. Similar to ω0 = 2πf0 = 2π∕T0, the discrete-time versions are Ω0 = 2π𝔽0 = 2π∕N0.\n\n\n\nAn analogy follows. The continuous-time signal x(t) with fundamental period T0 = 4 s has frequency f0 = 1∕T0 = 0.25 Hz. The interpretation is that in one second, there are 0.25 occurrences of a period of x(t). A discrete-time signal x[n] with fundamental period N0 = 4 samples has frequency 𝔽 = 1∕N0 = 0.25 Dhz. The interpretation is that between the occurrence of two consecutive samples of x[n], there are 0.25 occurrences of its period. Instead of Dhz, Matlab/Octaveuses the notation “per sample”.\n\n\n\nUsing Eq. (4.15), one can have an ESD G(ej𝔽) with unit Joules / Dhertz and the following property:\n\n\n\n\n\n\n\n\n\n E = ∫ 01G(ej𝔽)d𝔽. \n\n\n(4.16)\n\n\n\n\n\n\nSimilar to what was discussed in Example 4.7, the units of both G(ej𝔽) and G(ejΩ) is Joules / Dhertz, with the abcissas being 𝔽 in Dhz and Ω in radians, respectively.   □\n\n\n\nIn most cases, the signal under analysis has infinite energy, such as a power signal (e. g. a periodic signal) or realizations of a stationary random process. Therefore, the main interest in spectral analysis relies not on the ESD but on the PSD, which is the next subject. Noticing from Eq. (4.11) and (4.14) that squared magnitudes provide the energy distribution over frequency, to obtain the power distribution one can intuitively consider dividing the ESD by “time”, via a normalization factor that converts energy into power.\n\n\n\n\n\n4.4.2  Power spectral density (PSD)\n\n\n\nThe PSD has the important property that the average power P can be obtained in continous-time by\n\n\n\n\n\n\n\n\n\n Pc = ∫ −∞∞S(f)df, \n\n\n(4.17)\n\n\n\n\n\n\nwith S(f) in Watts/Hz, or in discrete-time by\n\n\n\n\n\n\n\n\n\n Pd = ∫ &lt;1&gt;S(ej𝔽)d𝔽, \n\n\n(4.18)\n\n\n\n\n\n\nwith S(ej𝔽) in Watts/Dhz (or Watts/“normalized frequency”).\n\n\n\nSimilar to the reasoning associated to Eq. (4.12), the version of Eq. (4.17) when the continuous-time PSD S(ω) is a function of ω = 2πf in rad/s is\n\n\n\n\n\n\n\n\n\n Pc = 1 2π∫ −∞∞S(ω)dω. \n\n\n(4.19)\n\n\n\n\n\n\nThe equivalent in discrete-time is\n\n\n\n\n\n\n\n\n\n Pd = 1 2π∫ &lt;2π&gt;S(ejΩ)dΩ. \n\n\n(4.20)\n\n\n\n\n\n\nIn all four equations, the PSDs describe the distribution of power over frequency or a normalized frequency.\n\n\n\n\n\nPSD of Random Signal\n\n\n\nRandom signals are important in many applications, such as in digital communications. One useful model for these signals is the wide-sense stationary (WSS) random process with autocorrelation R(τ).\n\n\n\nThe PSD S(f) for a continuous-time power signal x(t) corresponding to a realization of a WSS stochastic process X(t) is defined as\n\n\n\n\n\n\n\n\n\n S(f)=deflim ⁡  T→∞𝔼[|XT (f)|2] T , \n\n\n(4.21)\n\n\n\n\n\n\nwhere XT (f) is the Fourier transform of a truncated (windowed) version of x(t) with duration T. In other words, XT (f) = F{xT (t)} where xT (t) = x(t) for  − T∕2 ≤ t ≤ T∕2 or zero otherwise.\n\n\n\nAn alternative to obtaining a PSD via Eq. (4.21) is using the Wiener-Khinchin theorem,4 which states that the power spectral density (PSD) S(f) of a WSS process is the Fourier transform of the corresponding autocorrelation function, i. e.\n\n\n\n\n\n\n\n\n\n S(f) = F{R(τ)}. \n\n\n(4.22)\n\n\n\n\n\n\nSimilar expressions exist in discrete-time and even for deterministic signals.\n\n\n\nFor example, in discrete-time processing, the PSD is the DTFT of the autocorrelation function:\n\n\n\n\n\n\n\n\n\n S(ejΩ) = ∑ ℓ=−∞∞R[ℓ]e−jΩℓ, \n\n\n(4.23)\n\n\n\n\n\n\nwhere ℓ ∈ ℤ is the lag, and the inverse DTFT gives\n\n\n\n\n\n\n\n\n\n R[ℓ] = 1 2π∫ −ππS(ejΩ)ejΩℓdΩ. \n\n\n(4.24)\n\n\n\n\n\n\nSimilar to Eq. (4.21), the PSD S(ejΩ) for a discrete-time power signal x[n] corresponding to a realization of a WSS stochastic process X[n] is defined as\n\n\n\n\n\n\n\n\n\n S(ejΩ)=deflim ⁡   N→∞ 1 N𝔼  [ |∑ n=0N−1x[n]e−jΩn| 2] = lim ⁡   N→∞ 1 N𝔼  [ |XN(ejΩ)|2] . \n\n\n(4.25)\n\n\n\n\n\n\nwhere XN(ejΩ) is the DTFT of xN[n], a truncated version of x[n] obtained via a rectangular window of N non-zero samples. The unit of S(ejΩ) is Watts per normalized frequency Ω∕(2π).\n\n\n\nIn practice, there is a finite number of realizations of X[n] and often only one realization x[n] is available. Fortunately, ergodicity of the autocorrelation can be assumed in many cases and the ensemble averages substituted by averages taken over time (see Appendix B.19). Besides, the number of samples (the duration of x[n]) is often limited to a given value that can be determined, for example, by the time over which the process can be considered stationary. For example, in speech analysis applications, it is typically assumed the process of vowel production is quasi-stationary over segments with durations from 40 to 80 ms. With limited-duration signals, the challenge for spectral analysis is to obtain accurate estimates, as discussed in this chapter.\n\n\n\nFor both Eq. (4.21) and Eq. (4.25), windows other than the rectangular can be used. But the rectangular window is used in the definition of a PSD.\n\n\n\n\n\nPSD of Deterministic and Periodic Signal\n\n\n\n\n\n\nWhen adapted to deterministic signals, which do not require the expected value, Eq. (4.21) is written as S(f) = lim ⁡  T→∞|XT(f)|2 T , which is the ESD normalized by the time interval T, as expected. A special case of deterministic signals are the periodic ones.\n\n\n\nAssuming a continuous-time signal x(t) with period T0, its PSD is\n\n\n\n\n\n\n\n\n\n S(f) = ∑ k=−∞∞|c k|2δ(f − kF 0), \n\n\n(4.26)\n\n\n\n\n\n\nwhere ck are the Fourier Series coefficients and F0 = 1∕T0 is the fundamental frequency. Similarly, for S(ω), the expression is\n\n\n\n\n\n\n\n\n\n S(ω) = 2π∑ k=−∞∞|c k|2δ(f − kω 0), \n\n\n(4.27)\n\n\n\n\n\n\nwhere ω0 = (2π)∕T0 rad/s.\n\n\n\nIn summary, the PSD of a deterministic (non-random) periodic signal is composed by impulses with areas determined by the squared magnitude of Fourier series coefficients.\n\n\n\n Example 4.9. PSD of a continuous-time sinusoid. If x(t) = Acos ⁡ (2πfct), then S(f) = A2 4  [δ(f + fc) + δ(f − fc)] and the average power is ∫ −∞∞S(f)df = A2∕2.   □\n\n\n\nWhen considering a discrete-time periodic signal x[n], its PSD S(ejΩ) can be obtained by first considering an expression S ′(ejΩ) for the frequency range [0,2π[:\n\n\n\n\n\n\n\n\n\n S ′(ejΩ) = 2π∑ k=0N0 −1|X~[k]|2δ(Ω − kΩ 0), \n\n\n(4.28)\n\n\n\n\n\n\nwhere N0 is the period, Ω0 = (2π)∕N0 is the fundamental frequency, and X~[k] the DTFS of x[n]. Finally, the PSD S(ejΩ) is simply the periodic repetition of S ′(ejΩ):\n\n\n\n\n\n\n\n\n\n S(ejΩ) = ∑ p=−∞∞S ′(ej(Ω+p2π)). \n\n\n(4.29)\n\n\n\n\n\n\n Example 4.10. PSD of a discrete-time sinusoid. If x[n] = Acos ⁡ (Ω1n) (assume Ω1 obeys Eq. (1.39) to have x[n] periodic), then S ′(ejΩ) = πA2 2  [δ(Ω + Ω1) + δ(Ω −Ω1)] and\n\n\n\n\n\n\n S(ejΩ) = ∑ p=−∞∞πA2 2  [δ(Ω + Ω1 + p2π) + δ(Ω −Ω1 + p2π)] \n\n\n\n\n\n\nprovides its PSD, which has period 2π, as expected.   □\n\n\n\n\n\n4.4.3  Fourier Modulation Theorem Applied to PSDs\n\n\n\nIf Sx(f) is the PSD of a WSS random process X(t), the PSD Sy(f) of a new process Y (t) = X(t)ej2πfct is\n\n\n\n\n\n\n\n\n\n Sy(f) = Sx(f − fc). \n\n\n(4.30)\n\n\n\n\n\n\nSimilarly, if Y (t) = X(t)cos ⁡ (2πfct), then\n\n\n\n\n\n\n\n\n\n Sy(f) = 1 4  [Sx(f + fc) + Sx(f − fc)]. \n\n\n(4.31)\n\n\n\n\n\n\nTo observe why Eq. (4.30) (and, consequently, Eq. (4.31)) is true, recall the Wiener-Khinchin theorem of Eq. (4.22) and the autocorrelation definition of Eq. (1.56). Generally speaking, when modifying a WSS process X(t), the effect on its PSD can be obtained by checking how the modification affects its autocorrelation, and then relating this to frequency domain using Eq. (4.22). For example, multiplying X(t) by a scalar α corresponds to scaling its autocorrelation Rx(τ) by α2 and leads to a PSD α2Sx(f) given the linearity of the Fourier transform.\n\n\n\nAccording to this reasoning, a proof sketch of Eq. (4.30) follows:\n\n\n\n Sy(f)  = F{Ry(τ)}     = F{𝔼[Y (t + τ)Y ∗(t)]}     = F{𝔼[X(t + τ)ej2πfc(t+τ)X∗(t)e−j2πfct]}     = F{ej2πfcτ𝔼[X(t + τ)X∗(t)]}     = F{ej2πfcτR x(τ)}     = Sx(f − fc).    \n\nEq. (4.31) can be obtained by decomposing the cosine cos ⁡ (2πfct) = 1∕2(ej2πfct + e−j2πfct) into two complex exponentials and taking in account that the factor α = 1∕2 leads to the 1∕4 in the PSD expression. Eq. (4.31) allows to observe that the result of a signal multiplied by a cosine of unitary amplitude has half of the original signal power.\n\n\n\n\n\n4.4.4  Mean-square (MS) spectrum\n\n\n\nThe PSD is very useful especially when dealing with random signals and the power of the signal over a frequency range is obtained by integrating the PSD over that range. However, in some cases it is desired to use a function that allows to directly infer the average power of sinusoid components of a periodic signal, without the integration step. In these cases, the so-called MS or power spectrum is more convenient.5\n\n\n\nWhile in continuous-time the PSD S(f) unit is Watts/Hz, the mean-square spectrum is given directly in Watts and, assuming a periodic signal, corresponds to the squared magnitude of its DTFS:\n\n\n\n\n\n\n\n\n\n Sms[k] = |DTFS{x[n]}|2, \n\n\n(4.32)\n\n\n\n\n\n\nwith the property\n\n\n\n\n\n\n\n\n\n ∑ k=0N−1Sms[k] = P, \n\n\n(4.33)\n\n\n\n\n\n\nwhere P is the signal power and N is the FFT-size.\n\n\n\nRecall from the discussion associated to Eq. (2.44) that, if x[n] is periodic with fundamental period N0, its DTFS X~[k] can be obtained with an N0-point FFT:\n\n\n\n\n\n\n\n\n\n X~[k] = FFT{x[n]} N0 . \n\n\n(4.34)\n\n\n\n\n\n\nOne often uses Eq. (4.34) even for a non-periodic x[n], but then the result spectrum must be properly interpreted: as if the signal were a periodic version ∑ ⁡ p=−∞∞xN[n − pN] of the windowed version xN[n] of x[n] using N samples.\n\n\n\nIf the FFT size N is chosen as N = N0, the k-th FFT value corresponds exactly to the k-th DTFS coefficient, that is, they both represent the same frequency k(2π∕N) = k(2π∕N0). In case N≠N0, Eq. (4.33) is still a valid way to obtain the average power P, but the frequencies must be interpreted according to the FFT grid k(2π∕N).\n\n\n\nIn general, an estimate Ŝms[k] of the MS spectrum of a discrete-time signal x[n] can be obtained from its N-length windowed version xN[n] with\n\n\n\n\n\n\n\n\n\n Ŝms[k] = |DTFS{xN[n]}|2 =  |FFT{xN[n]} N |2. \n\n\n(4.35)\n\n\n\n\n\n\nAs mentioned, the power distribution of a deterministic signal such as a sum of sinusoids is more conveniently analyzed via its MS spectrum. However, in many practical engineering applications, there is some random noise involved and, even when the signal of interest is deterministic, its noisy version (such as sinusoids contaminated by AWGN) suggests using the PSD representation instead of the MS spectrum.\n\n\n \n\n4 See, e. g. [url3wie] and note that a realization x(t) of a WSS process X(t) is not square integrable and does not have a Fourier transform.\n\n \n\n5 Matlab (but not Octave) has a msspectrum spectral estimator and the private functions welch.m and computeperiodogram.m can be studied.\n\n                                                                           &lt;/div&gt;"
  },
  {
    "objectID": "ak_dsp_bookse54.html",
    "href": "ak_dsp_bookse54.html",
    "title": "54  Filtering Random Signals and the Impact on PSDs",
    "section": "",
    "text": "This section investigates the result of processing a random input signal through a system, and how the output PSD relates to the input.\n\n\n\n\n\n4.5.1  Response of LTI systems to random inputs\n\n\n\nAn important result of stochastic processes theory is that when the input of a LTI system is a WSS process X(t), the corresponding output is also a WSS process Y (t), as indicated below\n\n\n\n\n\n\n X(t)→ h(t) →Y (t), \n\n\n\n\n\n\nwhere h(t) is the impulse response of the LTI system.\n\n\n\nGiven that H(f) = F{h(t)} is the frequency response of the system, the relationship between the PSDs Sy(f) and Sx(f) of Y (t) and X(t), respectively, is given by\n\n\n\n\n\n\n\n\n\n Sy(f) = |H(f)|2S x(f). \n\n\n(4.36)\n\n\n\n\n\n\nSimilarly, if a discrete-time WSS random process X[n] is filtered by a LTI with impulse response h[n] as in\n\n\n\n\n\n\n X[n]→ h[n] →Y [n], \n\n\n\n\n\n\nthe PSD of the WSS output process Y [n] is given by\n\n\n\n\n\n\n\n\n\n Sy(ejΩ) = |H(ejΩ)|2S x(ejΩ), \n\n\n(4.37)\n\n\n\n\n\n\nwhere H(ejΩ) is the DTFT of h[n].\n\n\n\n\n\n4.5.2  Continuous-time signals with a white PSD and their filtering\n\n\n\nA signal model that is used in several applications is the white noise, previously discussed in Section 1.12.1.0. A realization ν(t) of a continuous-time white noise process has autocorrelation\n\n\n\n\n\n\n\n\n\n R(τ) = 𝔼[ν(t)ν(t − τ)] = N0 2 δ(τ) \n\n\n(4.38)\n\n\n\n\n\n\nand zero mean μ = 𝔼[ν(t)] = 0.\n\n\n\nUsing Eq. (4.22), the bilateral PSD of a white noise is\n\n\n\n\n\n\n\n\n\n S(f) = F  {N0 2 δ(τ)} = N0 2 ,   ∀ ⁡f. \n\n\n(4.39)\n\n\n\n\n\n\nWhen a unilateral representation is adopted, the white PSD is conveniently denoted as S(f) = N0. For instance, if N0 = 6 W/Hz is the constant level of a unilateral PSD, its bilateral representation has a constant level of N0∕2 = 3 W/Hz. Writing N0∕2 and N0 for the bilateral and unilateral PSDs, respectively, the average power over a given frequency band6 BW is always\n\n\n\n\n\n\n\n\n\n P = N0BW \n\n\n(4.40)\n\n\n\n\n\n\nfor both representations, given that in the case of a bilateral PSD one needs to multiply it by 2BW to find the average power.\n\n\n\nNote from Eq. (4.38) and Eq. (1.60) that the continuous-time white noise power is R(0) = ∞ because there is an impulse in τ = 0 (the area is N0∕2 but the amplitude goes to infinite). Another way of observing that the continuous time WGN has infinite power is looking at the frequency domain: its PSD is S(f) = N0 2 , such that from Eq. (4.17), the area under the curve goes to infinite.\n\n\n\nWith infinite power, a white noise cannot be measured (it would damage the measuring equipment!). But white noise is a very good model for many practical applications, with an implicit assumption that limits the frequencies to some bandwidth of interest and makes measurements and simulations feasible. In other words, the noise can have a flat PSD only over a given finite bandwidth but, for the purpose of the experiment / simulation, be conveniently modeled as white noise.\n\n\n\nBecause power can be written as P = σ2 + μ2, and μ = 0, the continuous-time white noise variance σ2 is infinite.\n\n\n\nIf the input X(t) is white noise with autocorrelation R(τ) = (N0∕2)δ(τ) and PSD N0∕2, Eq. (4.36) informs that the PSD at the output of the LIT system is\n\n\n\n\n\n\n\n\n\n Sy(f) = |H(f)|2N0 2 , \n\n\n(4.41)\n\n\n\n\n\n\n\ne., Sy(f) is a scaled version of |H(f)|2.\n\n\n\n\nA special case of white noise is the Gaussian or WGN, introduced earlier in Section 1.12.1.0.\n\n\n\n\n\n4.5.3  Discrete-time signals with a white PSD and their filtering\n\n\n\nIt is important to distinguish the continuous-time white noise, which has a flat bilateral PSD with value N0∕2 Watts/Hz and infinite power, from the discrete-time white noise that has finite power typically denoted by σ2. To emphasize the similarities and differences between continuous and discrete-time white noise, the goals here are to discuss how to: 1) generate a discrete-time white noise, 2) interpret the conversion of a continuous-time white noise into a discrete-time version and 3) its conversion back to continuous-time.\n\n\n\n\n\nDiscrete-time white noise\n\n\n\nAs mentioned, a discrete-time white process X[n] has its power 𝔼[X2[n]] = σx2 coinciding with the variance σx2 because the mean μx has to be zero for white processes. Moreover, its bilateral PSD level Sx(ejΩ) = σx2 Watts per normalized frequency (or Dhertz) coincides with its variance in Watts. To observe that, denote the constant PSD value as α and note from Eq. (4.20) that\n\n\n\n\n\n\n P = σ2 = 1 2π∫ &lt;2π&gt;S(ejΩ)dΩ = 1 2π2πα = α. \n\n\n\n\n\n\nIn other words, the constant PSD level α has the same numerical value of the power σ2 in Watts, but should be interpreted as having unit of Watts per normalized frequency, as discussed in Example 4.7 and Example 4.8. In case of a unilateral PSD, the constant value α = 2σ2 would be twice the power.\n\n\n\nTherefore, a discrete-time bilateral white PSD (with a constant level) can be denoted as:\n\n\n\n\n\n\n\n\n\n S(ejΩ) = N0 2 = σx2 = P. \n\n\n(4.42)\n\n\n\n\n\n\nThis result may be confusing because the values of power and spectral density coincide.\n\n\n\nDenoting as Sx(ejΩ) = σx2 the PSD of a white noise at the input of a filter H(ejΩ), Eq. (4.37) leads to an output PSD given by\n\n\n\n\n\n\n\n\n\n Sy(ejΩ) = σ x2|H(ejΩ)|2. \n\n\n(4.43)\n\n\n\n\n\n\nEq. (4.43) suggests that the output PSD Sy(ejΩ) has its shape |H(ejΩ)|2 imposed by the filter, given that the input PSD σx2 is simply a scaling factor.\n\n\n\n Example 4.11. Generation of uniform and Gaussian white noise using Matlab/Octave. A discrete-time white noise with i. i. d. samples is relatively easy to generate in Matlab/Octave using functions that implement random number generators. For example, x=rand(1,1000)-0.5 creates a signal with an autocorrelation that approximates an impulse at the origin and 1,000 samples that are uniformly distributed. The subtraction of 0.5 is necessary to make μ = 0. Similarly, a discrete-time version of WGN (Gaussian, not a uniform distribution) can be generated with x=randn(1,1000).   □\n\n\n\n\n\nConverting white noise from continuous to discrete-time\n\n\n\nThe signals discussed in the previous section were already created in discrete-time. But in some cases the discrete-time white noise is obtained from a continuous-time version or should be interpreted as such.\n\n\n\nFor example, assume ν(t) is a continuous-time WGN submitted to filtering by an ideal lowpass filter and A/D conversion, as illustrated by the following block diagram:\n\n\n\n ν(t) → h(t)→ A/D →ν[n].   \n\n\nFrom Eq. (4.36), the PSD at the output of h(t) is S(f) = |H(f)|2N0∕2. If the filter is ideal and the A/D conversion does not incur in aliasing, the PSD S(ejΩ) of ν[n] is a scaled and periodic version of S(f). Therefore, it is white in discrete-time.\n\n\n\nAssuming that h(t) is an ideal lowpass filter of bandwidth BW = Fs∕2 (gain equal to 1 over the band  − Fs∕2 to Fs∕2 Hz and zero otherwise), where Fs is the A/D sampling frequency, then the PSD of the continuous-time signal at the output of h(t) is S(f) = N0∕2 over BW and its power is P = (2BW)(N0∕2) = BWN0. Under the same assumptions of Eq. (1.51), the power values in discrete and continuous-time are the same, such that the power of ν[n] is denoted by its variance σ2 = BWN0.\n\n\n\n Example 4.12. Discretizing white noise. For example, given that white noise with N0∕2 = 3 W/Hz is converted to a discrete-time signal ν[n] via the cascade of an ideal lowpass filter h(t) with passband BW = Fs∕2 and an A/D process with Fs = 200 Hz, the power of the signal at the output of the ideal lowpass filter h(t) is\n\n\n\n\n\n\n BWN0 = Fs 2 N0 = 100 × 6 = 600W \n\n\n\n\n\n\nand the power of ν[n] is σ2 = BWN0 = 600 W.\n\n\n\n                                   &lt;div class='center'&gt;\n\n\n\n\n\n\n\n\nFigure 4.7: PSD Sx(f) of a continuous-time white noise with N0∕2 = 3 W/Hz (top) and its discrete-time counterpart Sx(ejΩ) obtained with Fs = 200 Hz (bottom). \n\n\n                                   &lt;/div&gt;\n                                                                          \n                                                                          \n\n\n\nFigure 4.7 depicts the PSD for both continuous and discrete-time signals. Note that the abscissa of Sx(ejΩ), which is periodic, is represented from 0 to 2π. As indicated by Eq. (4.20), the power σx2 can be obtained by integrating Sx(ejΩ) = 600 Watts per normalized frequency (Dhertz) over the range [0,2π[ and dividing the result by 2π.   □\n\n\n\n Example 4.13. Filtering white noise through systems with unitary-energy impulse responses. Another situation of interest is when white noise is filtered by an LTI that has impulse response h(t) with energy Eh = ∫ −∞∞|h(t)|2dt = 1 Joule, as depicted in:\n\n\n\n ν(t) → h(t) → x(t)→ A/D →x[n].    \n\nIn this case, the Parseval relation of Eq. (B.51) indicates that Eh = ∫ −∞∞|H(f)|2df = 1 and from Eq. (4.41), the power of x(t) is\n\n\n\n\n\n\n\n\n\n Px = ∫ −∞∞|H(f)|2N0 2 df = N0 2 . \n\n\n(4.44)\n\n\n\n\n\n\nTherefore, the power of x[n] is σ2 = N0∕2, independent of h(t) (or its transform H(f)). However, x[n] will not have a flat PSD unless h(t) is an ideal lowpass filter as in Example 4.12.   □\n\n\nGaussian signal filtered by LTI system remains Gaussian\n\n\n\nIf the system is LTI and its input is a realization of a WGN process, then it is well-known from random processes textbooks that the filter output is another Gaussian process. In other words, a Gaussian signal filtered by a linear system remains Gaussian but, in the general case, with a non-white PSD that was shaped by the filter.\n\n\n\nWhen the LTI is an ideal lowpass filter such as in Example 4.12, besides being Gaussian, the output is also flat within the filter passband (or “white” within this band). Hence, when the input is WGN, and h(t) an ideal lowpass filter with passband frequency Fs∕2 that has its output converted to discrete-time, the resulting ν[n] is a white and Gaussian process with PSD S(ejΩ) (again, “white” within the band  − π to π rad) and power σ2 = BWN0 Watts.\n\n\n\n\n\nConverting white noise from discrete to continuous-time\n\n\n\nThe conversion of a discrete-time white noise ν[n] into a continuous-time signal x(t) is depicted as:\n\n\n\n ν[n]→ D/A →h(t) → x(t).    \n\nSimilar to the previous discussion about white noise C/D conversion in this section, if h(t) is an ideal lowpass filter with passband frequency Fs∕2, x(t) has a flat PSD Sx(f) with value N0∕2 = σ2∕Fs, where σ2 is the power of ν[n] and Fs the sampling frequency. Note that Sx(f) = 0 for f &gt; |Fs∕2|. Therefore, x(t) is not a continuous-time WGN, but has a flat PSD within the band [−Fs∕2,Fs∕2].\n\n\n \n\n6 Unless otherwise stated, the bandwidth is assumed to be over non-negative frequencies.\n\n       &lt;/div&gt;"
  },
  {
    "objectID": "ak_dsp_bookse55.html",
    "href": "ak_dsp_bookse55.html",
    "title": "55  Nonparametric PSD Estimation via Periodogram",
    "section": "",
    "text": "The ESD, PSD and MS spectrum were defined without a discussion on how to estimate them. This section will exclusively concern this estimation using the FFT.\n\n\n\nThe periodogram Ŝ[k] is a classical approach for PSD estimation. There are distinct definitions in the literature and here, especially for consistency with Matlab/Octave, the periodogram is an approximation of S(f) and defined as\n\n\n\n\n\n\n\n\n\n Ŝ[k]=def 1 N BW|FFT{xN[n]}|2, \n\n\n(4.45)\n\n\n\n\n\n\nwhere xN[n] is the N-samples windowed version of x[n] and BW the assumed frequency bandwidth. If the window is other than the rectangular, Ŝ[k] is called the modified periodogram.\n\n\n\nWhen compared to the actual PSD S(f), notice that the periodogram Ŝ[k] may present aliasing in case xN[n] was obtained by sampling a continuous-time x(t). It may also present leakage due to windowing. Besides, the periodogram inherits the properties and pitfalls of an FFT, such as a frequency resolution ΔΩ = 2π∕N that may not be enough to distinguish peaks separated by less than ΔΩ due to the picket-fence effect.\n\n\n\nBased on the definition of Ŝ[k], the signal power P can be conveniently obtained by approximating the integral of Eq. (4.17). For instance, using the rectangle method one obtains\n\n\n\n\n\n\n\n\n\n P ≈Δf∑ k=0N−1Ŝ[k], \n\n\n(4.46)\n\n\n\n\n\n\nwhere Δf = BW∕N is the FFT frequency spacing.\n\n\n\nSome important facts related to this definition:\n\n &lt;ul class='itemize1'&gt;\n &lt;li class='itemize'&gt;The  periodogram  is  associated  by  definition  to  the  FFT  operation  and,\n consequently, operates in a finite-length discrete-time signal.\n &lt;/li&gt;\n &lt;li class='itemize'&gt;The  periodogram  input  is  a  discrete-time  signal,  but  frequencies  can  be\n conveniently interpreted in Hertz (or rad/s) via &lt;!-- l. 1041 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;mi&gt;ω&lt;/mi&gt; &lt;mo class='MathClass-rel' stretchy='false'&gt;=&lt;/mo&gt; &lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mi mathvariant='normal'&gt;Ω&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt;\n (Eq. (&lt;a href='ak_dsp_bookse7.html#x14-39001r22'&gt;1.22&lt;!-- tex4ht:ref: eq:freqdiscrete2continuous  --&gt;&lt;/a&gt;)) when the sampling frequency &lt;!-- l. 1041 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\n is specified.\n &lt;/li&gt;\n &lt;li class='itemize'&gt;The bandwidth &lt;!-- l. 1042 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mstyle class='text'&gt;&lt;mtext class='textrm' mathvariant='normal'&gt;BW&lt;/mtext&gt;&lt;/mstyle&gt;&lt;/math&gt;\n is   in   Hertz,   and   obtained   from   the   specified   sampling   frequency\n &lt;!-- l. 1042 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;.\n The &lt;!-- l. 1042 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mstyle class='text'&gt;&lt;mtext class='textrm' mathvariant='normal'&gt;BW&lt;/mtext&gt;&lt;/mstyle&gt;&lt;/math&gt;\n is assumed to be from &lt;!-- l. 1042 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt; &lt;mo class='MathClass-bin' stretchy='false'&gt;−&lt;/mo&gt; &lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo class='MathClass-bin' stretchy='false'&gt;∕&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/math&gt;\n to &lt;!-- l. 1042 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo class='MathClass-bin' stretchy='false'&gt;∕&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/math&gt;\n for a bilateral or &lt;!-- l. 1042 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/math&gt;\n to &lt;!-- l. 1042 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo class='MathClass-bin' stretchy='false'&gt;∕&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;/math&gt;\n for a unilateral periodogram.&lt;/li&gt;&lt;/ul&gt;\n                                                                          \n                                                                          \n\n\n\n\n4.6.1  Discrete-time PSD estimation using the periodogram\n\n\n\nThe periodogram is an approximation of the continuous-time PSD S(f), but one can “trick” the software routine and obtain an estimate of S(ejΩ) by imposing Fs = BW = 1 Hz. In this case, the numerical values of the periodogram Ŝ[k] provide an estimate Ŝ(ejΩ) of the discrete-time PSD S(ejΩ) as follows:\n\n\n\n\n\n\n\n\n\n Ŝ(ejΩ)| Ω=k(2π∕N) = Ŝ[k]. \n\n\n(4.47)\n\n\n\n\n\n\nHaving the periodogram calculated with Fs = 1, allows to interpret Ŝ[k] in Watts/Dhertz (normalized frequency), as discussed in Example 4.8.\n\n\n\nHowever, when invoking a periodogram software routine for discrete-time signals without specifying Fs, the assumed default value is Fs = 2π, not Fs = 1. The values of Ŝ[k] will differ in these two cases by 2π. But, independent on Fs, the signal average power can always be obtained with Eq. (4.46).\n\n\n\n\n\n4.6.2  Relation between MS spectrum and periodogram\n\n\n\nThe FFT-based estimation of both PSD and MS spectrum are closely related. From Eq. (4.35) and Eq. (4.45), one has:\n\n\n\n\n\n\n\n\n\n Ŝms[k] = BW N Ŝ[k] = ΔfŜ[k]. \n\n\n(4.48)\n\n\n\n\n\n\nBecause Ŝ[k] is a PSD estimate, the user of a PSD estimation routine that returns Ŝ[k] just needs to know the bin width Δf to be able to obtain the average power P from Ŝ[k], and/or individual values of Ŝms[k]. This reasoning is valid when P should be calculated over the whole frequency range, or shorter frequency intervals, because the amount of power Pk at a single FFT bin (the k-th bin) in a periodogram Ŝ[k] is\n\n\n\n\n\n\n\n\n\n Ŝms[k] = Pk = ΔfŜ[k]. \n\n\n(4.49)\n\n\n\n\n\n\nFor discrete-time signals assuming BW = 2π or BW = 1, Pk = (BW∕N)Ŝ[k]. In both continuous and discrete-time cases, P = ∑ ⁡ k=0N−1Pk.\n\n\n\nListing 4.7 illustrates how Eq. (4.48) can be used to obtain Ŝms[k].\n\n\n\nListing 4.7: MatlabOctaveCodeSnippets/snip_frequency_mssFromPeriodogram.m\n\n\nN=1024; n=(0:N-1)'; %number N of samples and column vector n \nA=10; x=A*cos((2*pi/64)*n); %generate cosine with period 64 \nSms = abs(fft(x)/N).^2; %MS spectrum |DTFS{x}|^2 \nBW = 1; %assumed bandwidth is 2*pi \n5S = abs(fft(x)).^2/(BW*N); %Periodogram (|FFT{x}|^2)/(BW * N) \nSms2=BW*S/N; %Example of obtaining MS spectrum from periodogram \nPower = sum(Sms) %Obtaining power from the MS spectrum \nPower2 = sum(S)*BW/N %Obtaining power from the periodogram\n  \n\n\nWhile the MS spectrum is a discrete function with the property specified by Eq. (4.33), the periodogram values Ŝ[k] ideally coincide with a continuous function of frequency: the PSD S(f) or S(ejΩ). Consequently, having an array of values Ŝ[k], the property that should be obeyed depends on the integral Eq. (4.17) or Eq. (4.20), respectively, not a summation as in Eq. (4.33).\n\n\n\nIt is recommended to observe Table B.1 and associated discussion, which summarizes an interesting analogy between the use of FFTs and histograms that helps understanding the difference betweeen Ŝ[k] and Ŝms[k].\n\n\n\n\n\nMatlab/Octave adopted conventions for periodograms\n\n\n\nIs should be noted that Matlab/Octave adopts the following:\n\n &lt;ul class='itemize1'&gt;\n &lt;li class='itemize'&gt;When the sampling frequency &lt;!-- l. 1172 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/math&gt;\n is not specified, it is assumed &lt;!-- l. 1172 --&gt;&lt;math display='inline' xmlns='http://www.w3.org/1998/Math/MathML'&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mrow&gt;&lt;mi&gt;F&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt; &lt;mo class='MathClass-rel' stretchy='false'&gt;=&lt;/mo&gt; &lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;π&lt;/mi&gt;&lt;/mrow&gt;&lt;/math&gt; Hz\n and Eq. (&lt;a href='ak_dsp_bookse53.html#x63-218001r17'&gt;4.17&lt;!-- tex4ht:ref: eq:psd_integration_continuousTime  --&gt;&lt;/a&gt;) is adopted.\n &lt;/li&gt;\n &lt;li class='itemize'&gt;When the signal is real, a unilateral PSD is adopted by default.\n &lt;/li&gt;\n &lt;li class='itemize'&gt;By default, the &lt;span class='ec-lmss-10x-x-109'&gt;periodogram.m &lt;/span&gt;function uses a power-of-two FFT-length with a\n minimum of 256, adopting zero-padding in case the input signal has less than\n 256 samples.\n &lt;/li&gt;\n &lt;li class='itemize'&gt;The periodogram values are converted to dB.&lt;/li&gt;&lt;/ul&gt;\n\n\nThese conventions are discussed in the sequel.\n\n\n\nA characteristic of Matlab/Octave is the adoption, by default, of a unilateral PSD when the signal is real and has a Hermitian-symmetric spectrum. In this case, only the nonnegative frequency range are used in the graphs and in the summation of the normalized periodogram to obtain P. For example, in continuous-time, instead of Eq. (4.46), one would have\n\n\n\n\n\n\n\n\n\n P = ∫ 0∞S(f)df ≈Δf∑ k=0(N∕2)+1Ŝ[k], \n\n\n(4.50)\n\n\n\n\n\n\nassuming N is even. Hence, the values of Ŝ[k] in a unilateral periodogram are twice the values in the corresponding bilateral representation, with exception of the DC and Nyquist frequencies. This can be seen in the result of Listing 4.8, which avoids zero-padding via the third input argument of periodogram.m.\n\n\n\nListing 4.8: MatlabOctaveCodeSnippets/snip_frequency_comparePeriodograms.m\n\n\nN=1024; n=(0:N-1)'; %number N of samples and column vector n \nA=10; x=A*cos((2*pi/64)*n); %generate cosine with period 64 \nBW = 2*pi; %assume defaut Fs=2*pi (bandwidth) in periodogram \nS = abs(fft(x)).^2/(BW*N); %Periodogram (|FFT{x}|^2)/(BW * N) \n5Nfft = N; %avoid zero-padding in periodogram below: \n[H,w]=periodogram(x,[],Nfft); %Using Matlab/Octave \nPower = sum(H)*BW/N %Power from unilateral periodogram \nPower2 = sum(S)*BW/N %Power from bilateral periodogram \nSunilateral = [S(1); 2*S(2:N/2); S(N/2+1)]; %from bi to unilateral \n10plot(w,H,'-o',w,Sunilateral ,'-x') %Compare periodograms\n  \n\n\nAs indicated in Listing 4.8, to obtain the power P from the periodogram, the proper normalization factor is the bin width Δf=BW/N, which in this case is Δf = 2π∕1024 because Fs was not specified when calling the periodogram.m routine and Fs = 2π is assumed.\n\n\n\nA minor detail is that when N is even and the periodogram is unilateral, w(end) is Fs∕2 and length(H) is (N∕2) + 1. If a bilateral periodogram\nis calculated, w(end) is (Fs∕2) −Δf and length(H) is N. Listing 4.9 illustrates this point.\n\n\n\nListing 4.9: MatlabOctaveCodeSnippets/snip_frequency_correctPeriodograms.m\n\n\nN=1024; n=(0:N-1)'; %number N of samples and column vector n \nA=10; x=A*cos((2*pi/64)*n); %generate cosine with period 64 \nFs=500; BW = Fs; %Fs = BW = 500 Hz \n[H,f] = periodogram(x,[],N,Fs); %Unilateral periodogram \n5[H2,f2] = periodogram(x,[],N,Fs,'twosided'); %Bilateral periodogram \nPower = (BW/N)*sum(H) %Power for unilateral periodograma \nPower2 = (BW/N)*sum(H2) %Power for bilateral periodograma \nf(end), f2(end), length(f), length(f2) %values for N even\n  \n\n\nAll the previous calculations (e. g., sum(H)) used the periodogram in linear scale. Another convention adopted by Matlab/Octave is to convert, by default, the periodogram values to dBW/Hz when the function periodogram.m is invoked without output parameters. However, as discussed in Appendix B.24, the periodogram unit is informed to be dB/Hz instead of dBW/Hz.\n\n\n\n\n\n4.6.3  Periodogram of periodic or energy signals\n\n\n\nA periodogram must be properly interpreted if applied to a periodic signal. Note that, as indicated in Section 4.4.2.0, while the PSD of a periodic signal is composed by impulses, the periodogram Ŝ[k] obtained from a finite-duration window does not contain impulses due to the convolution with the window spectrum.\n\n\n\nSimilarly, care must be exercised when a periodogram is applied to an energy signal. As discussed in Section 4.4.1, an energy signal should be associated to an ESD, not PSD. The periodogram of an energy signal such as x[n] = u[n] − u[n − 5], for example, is implicitly assuming that x[n] is just a windowed version of a periodic signal.\n\n\n\nBefore discussing the more advanced Welch method for PSD estimation, few examples of periodograms are provided in the next section to consolidate the concepts.\n\n\n\n\n\n4.6.4  Examples of PSD and MS spectrum estimation\n\n\n\n Example 4.14. Using periodogram.m to approximate a continuous-time PSD S(f) of a sinusoid. When the sampling frequency Fs is specified as input argument and without output arguments, the periodogram\nof Matlab/Octave indicates the abscissa in Hertz. For example, Listing 4.10 was used to generate Figure 4.8, which has the abscissa going from 0 to Fs∕2 Hz because the signal is real and the default periodogram is unilateral in this case.\n\n\n\nListing 4.10: MatlabOctaveCodeSnippets/snip_frequency_normalized_periodogram.m\n\n\nN=1024; A=10; x=A*cos(2*pi/64*(0:N-1));%generate cosine \nFs=8000; %sampling frequency in Hz \nrectWindow=rectwin(N); %rectangular window \nrectWindow=rectWindow(:); x=x(:); %make sure both are column vectors \n5[P,f]=periodogram(x,rectWindow,N,Fs); %periodogram \ndf = Fs/N; %FFT bin width to be used in power computation \nPower=sum(P)*df %calculate the average power \nplot(f/1000,10*log10(P)); grid, %convert to dBW/Hz\n  \n\n\n\n\n\n\n\nFigure 4.8: Periodogram of x[n] = 10cos ⁡ ((2π∕64)n) in dBW/Hz estimated by periodogram.m with a 1024-point FFT and assuming Fs = 8 kHz.\n\n\n\n\n\nThe signal power is P = 50 W. Figure 4.8 indicates a peak at f = 125 Hz, which corresponds to tone k = 16. This can be confirmed by observing that Δf = Fs∕N = 8000∕1024 ≈ 7.8125 Hz, such that fk = kΔf = 16 × 8000∕1024 = 125 Hz. Because there was no leakage in this case, the sinusoid power P = 50 W is completely residing in bin k = 16. From Eq. (4.49), the expected PSD value at this bin is Ŝ[k]|k=16 = P∕Δf = 50∕7.8125 = 6.4 W/Hz. Converting it to dBW leads to 10log ⁡ 106.4 ≈ 8.062 dBW/Hz, as indicated in Figure 4.8.\n\n\n\nOne aspect of PSD graphs in dB scale is that dynamic ranges larger than hundreds of dB should be associated to numerical errors, given that 10*log10(eps)=-156.5.    □\n\n\n\n Example 4.15. Discrete-time PSD of a sinusoid using the periodogram function. Listing 4.11 uses the Matlab/Octave periodogram function to estimate the discrete-time PSD of a cosine and compares it with a periodogram directly calculated based on the definition adopted in this text. Figure 4.9 depicts the result.\n\n\n\nListing 4.11: MatlabOctaveCodeSnippets/snip_frequency_periodogram.m\n\n\nN=16; alpha=2; A=10; n=0:N-1; \nx=A*cos((2*pi*alpha/N)*n); %generate cosine \nx=x(:); %Octave requires column vector \nBW=1; %it is a discrete-time signal, assume BW=1 Hz \n5Sk = (1/(N*BW))*abs(fft(x)).^2; %Periodogram as defined in text \nPower = (BW/N)*sum(Sk) %Obtain power from periodogram \nSmatlab=periodogram(x,[],N,BW,'twosided'); %from f=0 to 1 dHz \nM=length(Smatlab); %periodogram used zero-padding and unilateral PSD \ndhertz = (1/N)*(-N/2:(N/2-1)); %normalized frequency \n10Power2= (BW/N)*sum(Smatlab) %Power from periodogram function \nW=2*pi*dhertz; %convert abscissa to Omega in radians \nclf; subplot(211); h=stem(W,fftshift(Smatlab)); \nhold on; stem(W,fftshift(Sk),'xr')\n  \n\n\n\n\n\n\n\nFigure 4.9: Discrete-time PSD of x[n] = 10cos ⁡ ((2π∕8)n) in linear scale estimated with periodograms.\n\n\n\n\n\nListing 4.11 returns the value P = 50 W for both Power and Power2. The cosine angular frequencies are  ± 0.7854 rad as indicated in the top plot of Figure 4.9 for the positive frequency. The periodogram values at k = −2 and k = 2 are Ŝ[k]|k = ±2 = (P∕2)∕Δf = 400, given that Δf = BW∕N and BW = 1.\n\n\n\nThere was no leakage in the top plot of Figure 4.9. But in case one allows the periodogram.m routine to use zero-padding as in:\n\n\n[Smatlab,f]=periodogram(x,[],[],BW,'twosided'); %zero-padding\n\n\nthe result is the bottom plot of Figure 4.9. In this case, periodogram.m adopted zero-padding to reach N = 256 samples. This can be confirmed with\n\n\nPower3= (BW/256)*sum(Smatlab) %Power from periodogram function\n\n\nwhich returns Power3=50. Another discrepancy between the two plots in Figure 4.9 is that the bottom plot uses the range Ω ∈ [0,2π[ while the top plot adopted [−π,π[ via fftshift.m.   □\n\n\n\n Example 4.16. MS spectrum and PSD of a sum of sinusoids. A 1024-points DFT is adopted to inspect a signal composed by two discrete-time cosines that are not bin-centered and have amplitudes 10 and 1 V, as indicated in Listing 4.12.\n\n\n\nListing 4.12: MatlabOctaveCodeSnippets/snip_frequency_not_bin_cent_cos.m\n\n\nN=1024; n=(0:N-1)'; %number of samples and column vector \ndw=2*pi/N; %FFT bin width \nk1=115.3; k2=500.8; w1=k1*dw; w2=k2*dw;%define frequencies \nx=10*cos(w1*n) + 1*cos(w2*n); %generate sum of two cosines \n5Sms = abs(fft(x)/N).^2; %MS spectrum |DTFS{x}|^2 \n[S,w] = periodogram(x); %Periodogram, frequency w=0 to pi \nPower = sum(Sms) %estimate the power in Watts = A^2/2 \nk=0:N-1; %frequency index \nsubplot(211), h=plot(k,10*log10(Sms)); %MS spectrum (dB) \n10subplot(212), h2=plot(w,10*log10(S)); %Periodogram (dB)\n  \n\n\nFigure 4.10 depicts the obtained result. To obtain cosines that are not bin-centered, non-integer numbers k1=115.3 and k2=500.8 were specified to create the angular frequencies. Hence, each cosine has most of its power located at bins k = 115 and 501 as indicated by the data tips in Figure 4.10.\n\n\n\nThe top graph shows the bilateral MS spectrum with the positive frequencies (the ones that have associated data tips), from\n\nk = 0 to 512 preceding the “negative” frequencies from k = 513 to 1023. The function fftshift could be used to make the “negative” precede the positive frequencies. Note that the implicitly adopted rectangular window leads to considerable leakage. The bottom graph is a unilateral representation of the periodogram.\n\n\n\n\n\n\n\n\nFigure 4.10: Periodogram and MS spectrum for a sum of sinusoids. Both are in dB scale.\n\n\n\n\n\nThe Power calculated by the code was 50.5397 W while the theoretical result is 102∕2 + 12∕2 = 50.5. The small discrepancy is due to the fact that the sinusoids are not bin-centered and the time-domain signal x itself has power P = 50.5397 W.\n\n\n\nBin-centered cosines with power 50 and 0.5 W would lead to values 10log ⁡ 10(25) ≈ 13.98 dB and 10log ⁡ 10(0.25) ≈−6.02 dB at their corresponding frequencies in a bilateral MS spectrum. Given that BW = 2π and the FFT bin width is BW∕N = 2π∕1024 ≈ 0.0061, the theoretical values for the unilateral periodogram are 10log ⁡ 10(50∕0.0061) ≈ 39.11 and 10log ⁡ 10(0.5∕0.0061) ≈ 19.11. Figure 4.10 presents slightly smaller values for both MS spectrum and periodogram due to leakage. Because BW = 2π instead of BW = 1, the depicted values are not the ones of a discrete-time PSD S(ejΩ).\n\n\n\nAn interesting aspect is that the relative difference in power between the two sinusoids can be obtained in both periodogram and MS spectrum, leading to 37.79 − 18.63 ≈ 19.2 dB and 12.66 − (−6.497) ≈ 19.2 dB, respectively. In other words, the PSD does not allow directly reading the absolute power values but can inform the relation between powers of signal components that reside in specific bins.   □\n\n\n\n Example 4.17. Periodogram variance does not decrease with number of samples. Figure 4.11 illustrates the adoption of periodograms Ŝ[k] to estimate the discrete-time PSD S(ejΩ) of a white noise with power of 600 W. The estimation used N = 300 (top) and N = 3000 (bottom) samples of the noise signal. The code for signal generation and periodogram estimation for N=300 (the same code was used for N=3000 samples) is:\n\n\n\nListing 4.13: MatlabOctaveCodeSnippets/snip_frequency_noise_PSD.m\n\n\nN = 300; % number of samples and FFT size \npower_x = 600; %desired noise power in Watts \nFs = 1; %sampling frequency = BW = 1 Dhertz \nx=sqrt(power_x) * randn(1,N); %Gaussian white noise \n5actualPower = mean(x.^2) %the actual obtained power \n[Sk,F]=periodogram(x,[],[],Fs,'twosided'); %periodogram \nsubplot(211), plot(2*pi*F,Sk); %plot periodogram \nSx_th=power_x*ones(1,length(F)); %theoretical PSD \nmean(Sk) %in this case, it coincides with actualPower \n10disp(['Periodogram standard deviation=' num2str(std(Sk))]) \nhold on; h=plot(2*pi*F,Sx_th,'r:','lineWidth',3);\n  \n\n\nNote that due to the limited number N of samples, the actualPower values (e. g., 512.3 and\n601.0 W), may differ from the desired value power_x=600.\n\n\n\n\n\n\n\n\nFigure 4.11: Periodograms of a white noise with power equal to 600 W estimated with N = 300 (top) and N = 3000 (bottom) samples.\n\n\n\n\n\nRecall from Eq. (4.47), that in the case of Fs = 1, the periodogram coincides with the discrete-time PSD estimate Ŝ(ejΩ)|Ω=k(2π∕N).\n\n\n\nThis example illustrates that the periodogram values with Fs = 1 have an average mean(Sk) that coincides with actualPower (because Δf = 1∕N in this case). However, the variance of the estimate does not decrease with N. For N = 300 and 3000, the standard deviations are 593.1 and 599.4, respectively. This issue is discussed in Section 4.7, in which the Welch’s PSD estimation method is presented.    □\n\n\n\n Example 4.18. PSD of a sinusoid contaminated by AWGN. Special care must be exercised when interpreting a PSD that represents both broadband and narrowband signals. An example is when using the PSD to estimate the SNRs of sinusoids immersed in white noise. Figure 4.12 shows the periodogram of a cosine contaminated by AWGN generated with Listing 4.14.\n\n\n\nListing 4.14: MatlabOctaveCodeSnippets/snip_frequency_noisy_cosine.m\n\n\nN=1024; A=4; %# of samples and cosine amplitude of A Volts \nFs=8000; Ts=1/Fs; %sampling frequency (Hz) and period (s) \nf0=915; %cosine frequency in Hz \nnoisePower=16; %noise power in Watts \n5noise=sqrt(noisePower)*randn(1,N); \nt=0:Ts:(N-1)*Ts; %N time instants separated by Ts \nx=A*cos(2*pi*f0*t) + noise;%generate cosine with AWGN \n[P,f]=periodogram(x,[],N,Fs); %periodogram \nplot(f,10*log10(P)); %dB scale\n  \n\n\nListing 4.14 was used to generate the top plot with subplot(211). Then, the bottom plot was obtained by increasing N from 1024 to 16384. All the absolute values of the two periodograms differ due to their dependence on N.\n\n\n\n\n\n\n\n\nFigure 4.12: Periodograms of a cosine contaminated by AWGN at an SNR of  − 3 dB with an FFT of N = 1024 points (top plot) and 16384 (bottom). In this case the SNR cannot be inferred directly from the noise level.\n\n\n\n\n\nIt can be noticed from Listing 4.14 that the cosine power is Pc = A2∕2 = 8 W while the noise power is 16 W, leading to a SNR = 10log ⁡ 10(8∕16) ≈−3.01 dB. However, one may be tempted by Figure 4.12 to erroneously infer that SNR is positive given that the noise level (sometimes called noise floor) is more than 20 dB below the cosine peak for the top graph. Directly observing the ratio of powers in PSD graphs was valid in Figure 4.10, but in that case, the cosines were assumed to have all power confined in their respective bins. This is not the case with signals that have power spread over several bins and especially with white noise.\n\n\n\nBecause the white noise power is spread over all bins of a periodogram, the larger the number N of FFT bins, the less noise power each bin carries and the larger the difference between the sinusoid power (assumed to be confined in a single bin) and the noise floor. In Figure 4.10, the bottom plot shows that the cosine power is approximately 30 dB above the noise level when N is increased to 16384. However, in both plots, the SNR is  − 3 dB as attested by Listing 4.14.\n\n\n\nIn summary, the proper way of measuring the signal power is to account for the power in all bins that represent the signal.  □\n\n\n4.6.5  Estimating the PSD from Autocorrelation\n\n\n\nAn alternative way of estimating the PSD of a finite duration discrete-time signal is via the autocorrelation. The autocorrelation for such finite-duration signals can be obtained via Eq. (1.61), and then the PSD estimated with\n\n\n\n\n\n\n\n\n\n Ŝ(ejΩ) = ∑ k=−N+1N−1R^[k]e−jΩn. \n\n\n(4.51)\n\n\n\n\n\n\nAs previously done, an FFT can be used to obtain the values of Ŝ(ejΩ) at the FFT frequency grid.\n\n\n\n Example 4.19. PSD of filtered white-noise via its autocorrelation. Listing 4.15 illustrates how Eq. (4.51) can be used to generate Figure 4.13. The theoretical expression for the PSD in Figure 4.13 was obtained from Eq. (4.41).\n\n\n\nListing 4.15: MatlabOctaveCodeSnippets/snip_frequency_PSD_using_xcorr.m\n\n\nN = 3000; %total number of signal samples \nL = 4; %number of non-zero samples of h[n] \npower_x = 600; %noise power in Watts \nx=sqrt(power_x) * randn(1,N); %Gaussian white noise \n5h=ones(1,L); %FIR moving average filter impulse response \ny=conv(h,x); %filtered signal y[n] = x[n] * h[n] \nH=fft(h,4*N); %DTFT (sampled) of the impulse response \nSy_th=power_x*abs(H).^2; %theoretical PSD via sampling DTFT \nM=256; %maximum lag chosen as M &lt; N \n10[Ry,lags]=xcorr(y,M,'biased'); %estimating autocorrelation \nSy_corr=abs(fft(Ry)); %PSD estimate via autocorrelation\n  \n\n\n\n\n\n\n\nFigure 4.13: PSD of a filtered white noise x[n] estimated via the autocorrelation.\n\n\n\n\n\nNote that for an input signal with N samples, the function xcorr would, by default, output an autocorrelation with lags from  − N to N. However, to avoid noisy estimations at the autocorrelation tails, it is a good strategy to use a maximum lag M « N, if possible. There is a rule of thumb that suggests adopting M = N∕10. This was used in Listing 4.15, where Ry was calculated for a maximum lag of M=256 while the signal had N=3000 samples.   □\n\n\n\n Example 4.20. PSD of a discrete-time impulse. Let x[n] = Aδ[n − n0] be a discrete-time impulse with amplitude A that is multiplied by a rectangular window of N samples for FFT-based spectral analysis. For example, with N = 5 and A = 6 and x=[6, 0, 0, 0, 0] is the vector representing xw[n]. Because the PSD disregards the phase information, the value of n0 and the position of the peak with amplitude A within the windowed signal xw[n] is not relevant. For the given example, using x=[0, 0, 0, 6, 0] would lead to the same results. This signal xw[n] also has an autocorrelation that is an impulse and a white spectrum. Its PSD has a constant value of S(ejΩ) = P = 36∕5 = 7.2 W/Dhertz (normalized frequency), as indicated in Eq. (4.42). Listing 4.16 calculates the periodogram and MS spectrum.\n\n\n\nListing 4.16: MatlabOctaveCodeSnippets/snip_frequency_impulse_PSD.m\n\n\nN=5; %number of FFT points \nM=2; %maximum number of autocorrelation lags \nBW=1; %BW in normalized frequency (digital Hertz) \nx=[6 0 0 0 0]; %impulse signal truncated in N samples \n5Sms=abs(fft(x)/N).^2 %MS spectrum: all values are 2.25 Watts \nSk=periodogram(x,[],length(x),1,'twosided') %periodogram \n[R,lags]=xcorr(x,M,'biased'); %estimating autocorrelation \nSxcorr=abs(fft(R)); %PSD via autocorrelation \nPower=sum(Sms) %power (9 W) obtained from MS spectrum \n10Power2=(BW/N)*sum(Sk) %power (9 W) from \nPower3=(BW/N)*sum(Sxcorr) %power (9 W) from periodogram via xcorr\n  \n\n\nThe last commands obtain the average power equal to 7.2 W using three distinct alternatives. All elements of Sms have a value of 7.2∕5 = 1.44 W, given that the power was uniformly distributed over the five discrete frequency values. The bilateral periodogram was calculated assuming a bandwidth\n\nBW = 1 (in normalized frequency, or Dhertz) such that the returned value has all values equal to 7.2, coinciding with S(ejΩ) = 7.2 (see Eq. (4.47)).   □"
  },
  {
    "objectID": "ak_dsp_bookse56.html",
    "href": "ak_dsp_bookse56.html",
    "title": "56  Nonparametric PSD Estimation via Welch’s method",
    "section": "",
    "text": "This section discusses Welch’s method, one of the most popular nonparametric PSD Estimation method.\n\n\n\n\n\n4.7.1  The periodogram variance does not decrease with N\n\n\n\nIt may seem counterintuitive but, as Figure 4.11 indicates, increasing the number N of samples does not decrease the variance of the periodogram estimates Ŝ[k].\n\n\n\nIn fact, it is well-known in spectral analysis theory7 that the periodogram Ŝ[k] is an unbiased estimator of the true PSD S(f), i. e.,\n\n\n\n\n\n\n lim ⁡  N→∞𝔼[Ŝ[k]] = S(f). \n\n\n\n\n\n\nIn other words, the average periodogram converges to the right values as N increases.\n\n\n\nHowever, the variance of the periodogram estimator is approximately S2(f) even for large N. This is a consequence of a result that is valid under mild conditions:\n\n\n\n\n\n\n\n\n\n 𝔼[(Ŝ[k] − S(f))2] = S2(f) + R N, \n\n\n(4.52)\n\n\n\n\n\n\nwhere RN tends to zero when N →∞. In other words, the standard deviation of the periodogram is approximately as large as the spectrum it should estimate. Using windows such as Hamming and Hann can improve the spectral analysis, but do not help with respect to the variance of the periodogram.\n\n\n\nOne may expect that increasing the number N of samples would lead to a better estimate of the PSD. In fact, the estimation does improve, but with respect to its frequency resolution Δf = Fs∕N, not the variance.\n\n\n\n\n\n4.7.2  Welch’s method for PSD estimation\n\n\n\nThe main strategy to decrease the variance of a PSD estimation is to split the available data into K segments of M samples each, calculate the periodogram for each segment and then take their average. Methods such as Bartlett’s and Welch’s are based on this principle. The disadvantage of using less (M &lt; N) samples is that the frequency resolution Δf decreases (from Δf = Fs∕N to Fs∕M).\n\n\n\nThe main distinction between Bartlett’s and Welch’s methods is that the latter tries to combat the decrease in Δf by overlapping segments, such that for a given K, the value of M can be larger than M = N∕K, where N is the total number of available samples. For example, an overlap of 50% of segments of length M = 4 means that the first segment corresponds to samples with indices\n\nn = 0,1,2,3, the second segment has samples with n = 2,3,4,5, the third with n = 4,5,6,7 and so on.\n\n\n\nIn Matlab/Octave, the command Sx=pwelch(x) estimates a PSD via Welch’s method using default values (K = 8, Hamming window and an overlap of 50%). An example of a complete command is\n\n\n[Sx,w] = pwelch(x,window,Num_overlap,N_fft,Fs,'twosided')\n\n\nwhere window is the window, Num_overlap is the number of samples that are shared between two neighboring segments, N_fft is the number N of FFT points used for calculating the periodograms, Fs is the sampling frequency and ’twosided’ can be used to force pwelch to return values from 0 to 2π, even if the signal is real. The number M of samples per segment corresponds to the window length. The companion function ak_psd.m illustrates how to invoke pwelch.m and is convenient when the sampling frequency is given in Hz.\n\n\n\n\n\n\n\n\nFigure 4.14: PSD of a white noise x[n] with power equal to 600 W estimated by Welch’s method with M = 32 (top) and M = 256 (bottom) samples per segment.\n\n\n\n\n\n Example 4.21. Variance reduction with Welch’s method. Listing 4.17 provides an example of using pwelch for PSD estimation of the same signal as Figure 4.11 and leads to Figure 4.14. The top plot was obtained with Nfft=32 and then, the code was modified to use Nfft=256 and generate the bottom plot. Note that the total number of samples is N=3000 in both cases. The standard deviations among the periodograms were 46.5 and 132.0, respectively. The smaller Nfft, a larger number of periodograms is calculated and their averaging reduces the variance.\n\n\n\nListing 4.17: MatlabOctaveCodeSnippets/snip_frequency_pwelch.m\n\n\nN = 3000; %total number of signal samples \nNfft = 32; %segment length M and also FFT-length \nFs = 1; %assumed bandwidth for discrete-time signal \npower_x = 600; %noise power in Watts \n5x=sqrt(power_x) * randn(1,N); %Gaussian white noise \n[Sk,F]=pwelch(x,hamming(Nfft),[],Nfft,Fs,'twosided'); %Parameter [] \n%is because in Matlab it is num. samples while Octave is percent (%) \ndisp(['Periodogram standard deviation=' num2str(std(Sk))]) \nplot(2*pi*F,Sk) %periodogram with Fs=1 is discrete-time PSD \n10hold on, plot(w,power_x*ones(1,length(w)),'r') %PSD theoretical value\n  \n\n\nWhen comparing the plots in Figure 4.14 with the bottom plot in Figure 4.11, which also used N = 3000 samples, it is evident that averaging periodograms (for example, via Welch’s method) decreases the variance. Figure 4.14 also illustrates the tradeoff between variance reduction and frequency resolution as the top graph with ΔΩ = 2π∕32 is much smoother than the bottom one with ΔΩ = 2π∕256.\n\n\n\nA detail is that Listing 4.17 uses the default of the window overlap (the third argument of pwelch.m as []) because Octave assumes it is a percentage while Matlab assumes it is the number of samples.   □\n\n\n\n Example 4.22. Estimating the PSD of filtered white noise via Welch’s method.\n\n\n\n\n\n\n\n\nFigure 4.15: PSD of a filtered white noise x[n] estimated by Welch’s method.\n\n\n\n\n\nListing 4.18 illustrates an application of Sy(ejΩ) = σx2|H(ejΩ)|2 (Eq. (4.43)), where the system impulse response is h=[1,1,1,1] and H(ejΩ) is a sinc.\n\n\n\nListing 4.18: MatlabOctaveCodeSnippets/snip_frequency_filtered_noise_PSD.m\n\n\nN = 3000; %total number of signal samples \nL = 4; %number of non-zero samples of h[n] \npower_x = 600; %noise power in Watts \nx=sqrt(power_x) * randn(1,N); %Gaussian white noise \n5h=ones(1,L); %shaping pulse with square waveform \ny=conv(h,x); %filter signal x with filter \nNfft = 256; %segment length M and also FFT dimension \nFs = 2*pi; %sampling frequency for discrete-time PSD \n[Sy_pwelch,w]=pwelch(y,hamming(Nfft),[],Nfft,Fs,'twosided'); \n10H=fft(h,4*N); %DTFT (sampled) of the impulse response \nSy_th=power_x*abs(H).^2; %PSD theoretical expression \nplot(w,2*pi*Sy_pwelch); hold on %scale by 2*pi to get disc.time PSD \nplot(linspace(0,2*pi,length(H)),power_x*abs(H).^2,'r')\n  \n\n\nListing 4.18 was used to generate Figure 4.15, where one can notice that, even with the averaging used in pwelch, the variance continues proportional to the PSD magnitude, as indicated by Eq. (4.52).   □\n\n\n\n Example 4.23. Welch’s method using the Goertzel algorithm. Note that in Matlab, when the call to pwelch specifies the frequency points, pwelch can use the Goertzel algorithm instead of an FFT. An example is the following code:\n\n\nx=randn(1,10000); Fs=1e4; %some random vector and sampling freq. \nf=linspace(-Fs/2,Fs/2,1024); %create a frequency axis \nPx=pwelch(x,hamming(1024),512,f,Fs); %Octave has distinct syntax! \nplot(f,10*log10(Px)); %show PSD from -Fs/2 to Fs/2\n\n\nThe Goertzel algorithm can slow down the computations when compared to the FFT-based PSD estimation because it targets situations in which the frequencies are not uniformly spaced such as in DTMF detection algorithms (see Figure 4.32).    □\n\n\n \n\n7 See, e. g.,  [?,?] and [url4spe].\n\n                                                                                  &lt;/div&gt;"
  },
  {
    "objectID": "ak_dsp_bookse57.html",
    "href": "ak_dsp_bookse57.html",
    "title": "57  Parametric PSD Estimation via Autoregressive (AR) Modeling",
    "section": "",
    "text": "4.8  Parametric PSD Estimation via Autoregressive (AR) Modeling\n\n\n\nIn Section 3.10.1, the terms AR and MA (autoregressive and moving-average, respectively) were associated to filters. Here they will also denote WSS random processes that have white noise as the input to a LTI filter H(z) that is AR or MA, respectively.\n\n\n\nThe periodogram and Welch’s methods are categorized as “classical” or “nonparametric” spectral analysis. This section discusses a method from “modern” spectral analysis, which consists in indirectly estimating a PSD S(ejΩ) = |H(ejΩ)|2 by first estimating the parameters of the autoregressive model (or filter):\n\n\n\n\n\n\n\n\n\n H(z) = gzP   ∏ k=1P (z − pk) = g A(z) = g 1 + a1z−1 + a2z−2 + … + aP z−P , \n\n\n(4.53)\n\n\n\n\n\n\nwhich corresponds to Eq. (3.54), repeated here for the convenience of calling P (instead of N) the system order.\n\n\n\nSuch “parametric” spectral estimator has the advantage that there are potentially fewer parameters to be estimated when compared to the values of a PSD. For example, when using an M-point FFT for PSD estimation using Welch’s algorithm, M values need to be estimated. In contrast, the AR model requires estimating only P « M parameters and the gain g. On the other hand, if the assumed model is incorrect, the parametric estimator may lead to highly inaccurate results.\n\n\n\nThe goal of AR-based PSD estimation is to find H(z) = g∕A(z) that, when excited\nby white noise x[n] generates an output that has the same statistics of y[n], i. e.,\n\n\n\n\n\n\n x[n]→  &lt;mrow  &gt;g  &lt;mrow  &gt;A(z) →y[n]. \n\n\n\n\n\n\nHence, it is assumed that y[n] is a realization of an ergodic autoregressive random process of order P, denoted as AR(P).\n\n\n\nThe problem is posed mathematically as finding the FIR filter A(z) = 1 + ∑ ⁡ i=1P aiz−i that minimizes the energy ∑ ⁡ n|x[n]|2 of the output signal x[n] in\n\n\n\n\n\n\n y[n]→ A(z) →x[n]. \n\n\n\n\n\n\nThe filter A(z) = 1 −A~(z) is called the prediction-error filter because A~(z) = −∑ ⁡ i=1P aiz−i predicts an estimate y~[n] of y[n] as\n\n\n\n\n\n\n\n\n\n y~[n] = −∑ i=1P a iy[n − i], \n\n\n(4.54)\n\n\n\n\n\n\nbased on the P past samples of y[n]. This is depicted in Figure 4.16. The prediction error is x[n] = y[n] −y~[n] and, when A(z) is an optimum solution, x[n] has characteristics of white noise with power σx2. The signal x[n] is also called prediction error and, therefore, g2 is called the prediction error power.\n\n\n\n\n\n\n\n\nFigure 4.16: The prediction-error filter is A(z) = 1 −A~(z), where A~(z) provides a prediction y~[n] of the current n-th signal sample y[n], based on previous samples y[n − 1],…,y[n − P].\n\n\n\n\n\nAR-based modeling using Eq. (4.54) is widely used in speech coding and, because y~[n] in Eq. (4.54) is a linear combination of past inputs, it is called linear predictive coding (LPC).\n\n\n\nThe filter A(z) is often used to perform decorrelation of a signal (y[n] according to the adopted notation), while 1∕A(z) is called the synthesis filter because it can generate a signal (y[n]) with a given PSD from white noise (x[n]).\n\n\n\n Example 4.24. Experimenting with Matlab’s aryule function. Two examples of autoregressive modeling using Matlab are provided here. Similar commands can be used with Octave, as later suggested in Listing 4.24.\n\n\n\nThe function aryule.m returns the filter A and the power Perror of the white noise signal x[n], corresponding to A(z) and g2, respectively. When a white signal with power Perror is used as input of the synthesis filter 1∕A(z), the resulting signal has the power of y[n].\n\n\n\nAs an example, the result of the following code composes Table 4.2.\n\n\ny=(1:100)+randn(1,100); %signal composed by ramp plus noise \nfor P=1:4 %vary the LPC order \n  [A,Perror]=lpc(y,P) %estimate filter of order P \nend\n\n\n\n\n\n\nTable 4.2: LPC result for different orders P for a ramp signal with added noise.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOrder P\n\n\nFilter A(z)\n\n\nPerror (g2) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 \n\n\n[1.0, -0.9849] \n\n\n102.1144 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2 \n\n\n[1.0, -0.9883, 0.0035] \n\n\n102.1131 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3 \n\n\n[1.0, -0.9884, 0.0082, -0.0047] \n\n\n102.1109 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4 \n\n\n[1.0, -0.9884, 0.0082, -0.0138, 0.0092] \n\n\n102.1022 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                                                                                                             &lt;/div&gt;\n                                                                          \n                                                                          \n\n\n\nTable 4.2 illustrates that the LPC filter A(z) = 1 − 0.9849z−1 of order P = 1 can extract most of the correlation among the samples of the input signal y. For P &gt; 1, the coefficients ai for i &gt; 1 have relatively small values. Besides, Perror does not decrease significantly with P. A second example illustrates a situation where P = 2 outperforms P = 1. Listing 4.19 is similar to the previous one, but simulates an AR(2) process and allows to create Table 4.3.\n\n\n\nListing 4.19: MatlabOctaveCodeSnippets/snip_frequency_aryule.m\n\n\nN=1000; x=randn(1,N); %WGN with zero mean and unit variance \nPx=mean(abs(x).^2) %input signal power \nB=4; %filter numerator \nA=[1 0.5 0.98]; %filter denominator \n5Fs=1; %sampling frequency Fs = BW = 1 DHz \ny=filter(B,A,x); %realization of an AR(2) process \nPy=mean(abs(y).^2) %output power \nfor P=1:4 %vary the filter order \n    [Sy,f]=pwelch(y,rectwin(N),[],N,Fs,'twosided'); \n10    [Hthe,w]=freqz(B, A,'twosided'); \n    [Ahat,Perror]=aryule(y,P) %estimate filter of order P \n    [Hhat,w]=freqz(sqrt(Perror), Ahat,'twosided'); \n    clf, plot(2*pi*f,10*log10(Sy),'r'); hold on \n    plot(w,10*log10(Px*abs(Hthe).^2),'k--'); \n15    plot(w,10*log10(Px*abs(Hhat).^2),'b'); \n    legend('Welch','Theoretical','AR') \n    pause \nend\n  \n\n\n\n\n\n\nTable 4.3: LPC result for different orders P for an AR(2) realization.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOrder P\n\n\nFilter A(z)\n\n\nPerror (g2) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1 \n\n\n[1.0, 0.2566] \n\n\n513.6893 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2 \n\n\n[1.0, 0.5089, 0.9832] \n\n\n17.1227 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3 \n\n\n[1.0, 0.4601, 0.9579, -0.0496] \n\n\n17.0805 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4 \n\n\n[1.0, 0.4595, 0.9694, -0.0441, 0.0120] \n\n\n17.0781 \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                                                                                                             &lt;/div&gt;\n                                                                          \n                                                                          \n\n\n\nAs expected from the analysis of an AR(2) realization, Table 4.3 shows a drastic improvement when transitioning from P = 1 to 2, but without significant decrease in Perror afterward. Note that the correct filter was H(z) = 4∕(1 + 0.5z−1 + 0.98z−2), while the estimation for P = 2 was Ĥ(z) = 17.1227∕(1 + 0.5089z−1 + 0.9832z−2). Increasing the number of samples to N=10000 leads to the accurate estimation Ĥ(z) = 16.3780∕(1 + 0.5010z−1 + 0.9822z−2).    □\n\n\n4.8.1  Spectral factorization\n\n\n\nSpectral factorization8 is an important tool for generating a signal with a given discrete-time PSD S(ejΩ) via a rational system function H(z). Instead of using Fourier transforms, it is easier to work with the more general z transform S(z). Assuming a valid PSD is real and obeys S(ejΩ) ≥ 0,∀ ⁡Ω, if its corresponding S(z) is rational, it can be uniquely factored as\n\n\n\n\n\n\n\n\n\n S(z) = γ2H(z)H∗(1∕z∗), \n\n\n(4.55)\n\n\n\n\n\n\nwhere γ2 is the geometrical mean of S(ejΩ), H(z) is monic and loosely minimum-phase.\n\n\n\nTo obtain H∗(1∕z∗), first H(1∕z∗) can be obtained by substituting z by 1∕z∗ in\n\nH(z), and then its complex-conjugate is obtained according to the strategy discussed in Section B.3. For instance, suppose H(z) = (z − 2ej4)(z − 3 + j5)∕(z + 6e−j4), then H(1∕z∗) = ((1∕z∗) − 2ej4)((1∕z∗) − 3 + j5)∕((1∕z∗) + 6e−j4) and\n\n\n\n\n\n\n\n\n\n H∗(1∕z∗) = ((1∕z) − 2e−j4)((1∕z) − 3 − j5) ((1∕z) + 6ej4) = (z−1 − 2e−j4)(z−1 − 3 − j5) (z−1 + 6ej4) . \n\n\n(4.56)\n\n\n\n\n\n\nTo understand Eq. (4.55), it is also useful to note:\n\n &lt;ul class=\"itemize1\"&gt;\n &lt;li class=\"itemize\"&gt;\n &lt;!--l. 1795--&gt;&lt;p class=\"noindent\" &gt;Definition of &lt;span \nclass=“ec-lmri-10x-x-109”&gt;monic: signal that has unity-valued amplitude h[n]|n=0 = 1 at n = 0. In this case, the corresponding H(z) or H(z−1) is also monic.\n\n\n &lt;li class=\"itemize\"&gt;\n &lt;!--l. 1796--&gt;&lt;p class=\"noindent\" &gt;Definition of the geometrical mean &lt;/p&gt;&lt;table class=\"equation-star\"&gt;&lt;tr&gt;&lt;td&gt;\n &lt;!--l. 1797--&gt;&lt;math \nxmlns=“http://www.w3.org/1998/Math/MathML”\ndisplay=“block” class=“equation”&gt; γ2 = e [ 1 2π∫ &lt;2π&gt; ln ⁡  S(ejΩ)dΩ]. \n\n\n\n\n &lt;/li&gt;\n &lt;li class=\"itemize\"&gt;\n                                                                          \n                                                                          \n &lt;!--l. 1800--&gt;&lt;p class=\"noindent\" &gt;A minimum-phase &lt;!--l. 1800--&gt;&lt;math \nxmlns=“http://www.w3.org/1998/Math/MathML”\ndisplay=“inline” &gt;H(z) has all its poles and zeros inside the unit circle, while a loosely minimum-phase H(z) has at least one zero or pole on the unit circle and, if it is monic, can be written as\n\n\n\n\n\n\n\n\n\n H(z) = ∏ k=1M(1 − ckz−1) ∏ k=1N(1 − dkz−1),|ck|≤ 1,|dk|≤ 1, \n\n\n(4.57)\n\n\n\n\n &lt;!--l. 1805--&gt;&lt;p class=\"noindent\" &gt;where &lt;!--l. 1805--&gt;&lt;math \nxmlns=“http://www.w3.org/1998/Math/MathML”\ndisplay=“inline” &gt;ck and dk are zeros and poles, respectively, which are not outside the unit circle. H(z) is monic because its independent term is 1, i. e., H(z)|z→∞ = 1.\n\n\n\n\n\nNow the intuition behind Eq. (4.55) can be developed as follows. Suppose one wants to generate a discrete-time signal with a given PSD Sy(ejΩ), by filtering white noise with PSD Sx(ejΩ) and using Sy(ejΩ) = |H(ejΩ)|2Sx(ejΩ) from Eq. (4.37). Using the fact that multiplying a complex-number cej𝜃 by its conjugate ce−j𝜃 leads to its squared magnitude cej𝜃ce−j𝜃 = c2, one can write |H(ejΩ)|2 = H(ejΩ)H∗(ejΩ). Also,\n\n\n\n\n\n\n H∗(1∕z∗)| z=ejΩ = H∗(1∕e−jΩ) = H∗(ejΩ), \n\n\n\n\n\n\nsuch that\n\n\n\n\n\n\n |H(ejΩ)|2 =  H(z)H∗(1∕z∗)| z=ejΩ. \n\n\n\n\n\n\nThis shows that Eq. (4.37) can be obtained by the more general expression\n\n\n\n\n\n\n\n\n\n Sy(z) = H(z)H∗(1∕z∗)S x(z), \n\n\n(4.58)\n\n\n\n\n\n\nwhere H∗(1∕z∗) is the reflected transfer function of H(z).\n\n\n\nThe term reflected is adopted for H∗(1∕z∗) because its poles and zeros are at the conjugate-reciprocal locations of respective poles and zeros of H(z), i. e., they were reflected through the unit circle. Consider H(z) given by Eq. (4.57), then\n\n\n\n\n\n\n\n\n\n H∗(1∕z∗) = ∏ k=1M(1 − ck∗z) ∏ k=1N(1 − dk∗z). \n\n\n(4.59)\n\n\n\n\n\n\nA parcel (1 − ckz−1) of H(z) becomes (1 − ck∗z) in H∗(1∕z∗). Hence, a zero ck of H(z) turns into a zero (1∕c)∗ of H∗(1∕z∗). The magnitude |ck| turns into 1∕|ck|, while the phase is preserved. This happens for both zeros and poles. For example, the zeros of H∗(1∕z∗) in Eq. (4.56) are 0.5ej4 and (3 − j5)∕34, and the pole is  − (1∕6)e−j4. These are the reflected values of zeros and pole of H(z): 2ej4, 3 − j5 and  − 6e−j4, respectively. Therefore, if H(z) is loosely minimum-phase, then H∗(1∕z∗) is loosely maximum-phase. This allows to conveniently factor a rational |H(ejΩ)|2 into minimum and maximum phase systems. And because the minimum-phase H(z) is causal and stable, one can generate a process with a given PSD as described in the next paragraphs.\n\n\n\n Example 4.25. Generating the PSD of a first-order moving-average process. A first-order moving-average process, denoted as MA(1), can be described by the difference equation y[n] = b0x[n] + b1x[n − 1], where x[n] is white noise with PSD Sx(z) = σ2. The system function is H(z) = b0 + b1z−1. Using Eq. (4.58), its output PSD can be obtained by\n\n\n\n\n\n\n\n\n\n Sy(z) = (b0 + b1z−1)(b 0∗ + b 1∗z)σ2 =  [b 0b1∗z + |b 0|2 + |b 1|2 + b 0∗b 1z−1] σ2. \n\n\n(4.60)\n\n\n\n\n\n\nThe frequency response is Sy(ejΩ) =  [b0b1∗ejΩ + |b0|2 + |b1|2 + b0∗b1e−jΩ] σ2. Listing 4.20 provides an example with b0 = 1 + j3, b1 = −0.8 − j2 and σ2 = 1.\n\n\n\nListing 4.20: MatlabOctaveCodeSnippets/snip_frequency_ma1.m\n\n\nB=[1+3j -0.8-2j]; %MA highpass filter with complex coefficients \n%B=[1 0.8]; %MA lowpass filter with real coefficients \nx=randn(1,10000); %generate white noise \nPx=mean(abs(x).^2) %input signal power \n5Fs=1; %Fs = BW = 1 Dhz to obtain discrete-time PSD \ny=filter(B,1,x); %generate MA(1) process \n[Syy,f]=ak_psd(y,Fs); %find PSD via Welch's method \nplot(2*pi*f,Syy), hold on %plot estimated PSD in dBm/Dhz \nHmag2=(B(1)*conj(B(2))*exp(1j*2*pi*f))+sum(abs(B).^2)+ ... \n10    (conj(B(1))*B(2)*exp(-1j*2*pi*f)); \nplot(2*pi*f,10*log10(Hmag2/1e-3),'r')%theoretical, in dBm/Dhz \nxlabel('\\Omega (rad)'); ylabel('S(e^{j\\Omega})  dBm/Dhz'), axis tight\n  \n\n\nThe output process has a PSD shaped by the highpass filter. Uncommenting the second line in Listing 4.20 to adopt B=[1 0.8] leads to a lowpass output PSD.\n\n\n\nThe autocorrelation\n\n\n\n\n\n\n\n\n\n Ry[ℓ] =  (b0b1∗δ[ℓ + 1] + (|b 0|2 + |b 1|2)δ[ℓ] + b 0∗b 1δ[ℓ − 1])σ2. \n\n\n(4.61)\n\n\n\n\n\n\nis obtained via an inverse Fourier transform of Sy(z) and provides the average power of y[n] as Ry[0] = (|b0|2 + |b1|2)σ2.    □\n\n\n\n\n\n4.8.2  AR modeling of a discrete-time PSD\n\n\n\nEq. (4.37) informs that, if the input x[n] to a LTI system H(z) is white noise with a PSD consisting of a constant value Sx(ejΩ) = N0∕2, the output PSD is Sy(ejΩ) = N0∕2|H(ejΩ)|2. Recall that, for discrete-time PSDs, the white noise power σx2 coincides with the PSD level N0∕2 such that one can write Sy(ejΩ) = σx2|H(ejΩ)|2.\n\n\n\nAs discussed, in AR modeling, the PSD Ŝ(ejΩ) is obtained by first estimating A(ejΩ) = A(z)|z=ejΩ and g, and then using\n\n\n\n\n\n\n\n\n\n Ŝ(ejΩ) = |H(ejΩ)|2 = g2 |A(ejΩ)|2, \n\n\n(4.62)\n\n\n\n\n\n\nwhere it is assumed that x[n] has unit variance σx2 = 1. Alternatively, H(z) could be restricted to have a numerator equal to one, i. e. H(z) = 1∕A(z) and the squared gain g2 interpreted as the white noise power σx2 that allows to generate y[n] with power Py. This power is given by\n\n\n\n\n\n\n Py = 1 2π∫ &lt;2π&gt;Ŝ(ejΩ)dΩ = 1 2π∫ &lt;2π&gt; g2 |A(ejΩ)|2dΩ \n\n\n\n\n\n\nand can be solely controlled by H(z), specially by g.\n\n\n\nRecall from Eq. (3.98), that when the input signal to an LTI system is white, the output power is Py = Ehσx2, where Eh is the energy of the impulse response h[n] = Z−1{H(z)}. Listing 4.21 shows how Py = Ehσx2 can be used to relate Py and σx2 (or g2).\n\n\n\nListing 4.21: MatlabOctaveCodeSnippets/snip_frequency_lpcExample.m\n\n\nN=100; x=randn(1,N); %WGN with zero mean and unit variance \ny=filter(4,[1 0.5 0.98],x); %realization of an AR(2) process \nPy=mean(y.^2) %signal power \n[A,Perror]=aryule(y,2) %estimate filter via LPC \n5h=impz(1,A,500); %impulse response of 1/A(z) \nEh=sum(h.^2) %impulse response energy \nPy - (Perror*Eh) %compare Py with Perror*Eh\n  \n\n\nListing 4.22 compares a PSD estimated with Welch’s method and the one obtained via autoregressive modeling for a realization of an AR(2) process.\n\n\n\nListing 4.22: MatlabOctaveCodeSnippets/snip_frequency_AR_PSD.m\n\n\nN=100000; x=randn(1,N); %WGN with zero mean and unit variance \ny=filter(4,[1 0.5 0.98],x); %realization of an AR(2) process \n[A,Perror]=aryule(y,2) %estimate AR filter via LPC \nnoverlap=50; Nfft=2048; Fs=1; %pwelch input values \n5[Sp,f]=pwelch(y,[],noverlap,Nfft,Fs,'twosided');%PSD \n%Sp = 2*pi*Sp; %convert estimation into discrete-time PSD \n[H,w]=freqz(1,A,2048,'whole'); %get frequency response of H(z) \nShat=Perror*(abs(H).^2); %get PSD estimated via autoregressive model \nplot(2*pi*f,10*log10(Sp),w,10*log10(Shat)) %compare in dB \n10xlabel('\\Omega (rad)'); ylabel('S(e^{j\\Omega})  dBW/Dhz'), axis tight\n  \n\n\nWhen the power value σ2 is used in Listing 4.22, it is in fact representing the PSD level N0∕2 = σ2, which coincide in a discrete-time white PSD as indicated in Eq. (4.42). This is not the case for continuous-time PSDs, where the total power of a white noise will\nbe explicitly distributed over frequency to obtain its PSD, as discussed in the sequel.\n\n\n\n\n\n4.8.3  AR modeling of a continuous-time PSD\n\n\n\nIf the goal is to use a discrete-time signal y[n] to estimate a continuous-time PSD Ŝ(f), then the average power σx2 should be normalized by Fs to obtain Sx(f) = N0∕2 = σx2∕Fs and the frequency Ω mapped via ω = ΩFs or, equivalently, Ω = 2πfTs, such that A(f) = A(ejΩ)|Ω=2πfTs and\n\n\n\n\n\n\n\n\n\n Ŝ(f) = N0∕2 |A(ej2πfTs)|2. \n\n\n(4.63)\n\n\n\n\n\n\nFor example, if A(z) = 1 + 0.5z−1 + 0.98z−2 and Fs = 10 Hz, then\n\n\n\n\n\n\n A(ej2πfTs ) = 1 + 0.5e−j0.2πf + 0.98e−j0.4πf. \n\n\n\n\n\n\nAs illustrated in the last line of Listing 4.23, the normalization A(f) = A(ejΩ)|Ω=2πfTs can be obtained by simply changing the abscissa.\n\n\n\nListing 4.23 is similar to Listing 4.22 but aims at estimating a continuous-time PSD.\n\n\n\nListing 4.23: MatlabOctaveCodeSnippets/snip_frequency_AR_continuousPSD.m\n\n\nN=100000; x=randn(1,N); %WGN with zero mean and unit variance \nFs=600; %assumed sampling frequency in Hz \ny=filter(4,[1 0.5 0.98],x); %realization of an AR(2) process \n[A,Perror]=aryule(y,2) %estimate filter via LPC \n5noverlap=50; Nfft=2048; %pwelch input values \npwelch(y,[],noverlap,Nfft,Fs);% continuous-time PSD \n[H,w]=freqz(1,A,Nfft); %get frequency response of H(z) \nN0div2=Perror/Fs; %white noise PSD level is power / bandwidth \nShat=N0div2*(abs(H).^2); %get PSD estimated via AR model \n10Shat=[Shat(1); 2*Shat(2:end-1); Shat(end)]; %convert to unilateral \nhold on, plot(w*Fs/(2*pi),10*log10(Shat),'k') %compare in dB\n  \n\n\nThe command N0div2=Perror/Fs in Listing 4.23 converts the power Perror into a PSD level N0∕2, while Shat=N0div2*(abs(H).^2) uses the frequency response H obtained from H(z) = 1∕A(z) to implement Eq. (4.41).\n\n\n\nCare must be exercised with respect to normalization factors when comparing spectra obtained with distinct methods. For example, the bandwidth for normalizing the PSDs depends on the adoption of a unilateral or bilateral spectrum. Another detail is that, when converting from bilateral to unilateral representations and vice-versa, one needs to take in account that the values at DC and Nyquist frequencies should not be doubled, as done in Listing 4.23.\n\n\n\n\n\n4.8.4  Yule-Walker equations and LPC\n\n\n\nThere are many techniques to estimate the AR model 1∕A(z). Here the Yule-Walker equations are adopted given the existence of fast and robust algorithms for solving them, such as Levinson-Durbin.\n\n\n\nTo keep the discussion relatively short, the goal here is to practice how to use AR estimation to obtain the spectrum of a signal. The algorithms and their properties have been widely discussed in the parametric estimation literature and the reader is encouraged to use them and pursue further knowledge (see Section 4.11).\n\n\n\nBoth Matlab and Octave have several interrelated functions for AR modeling such as lpc, pyulear, aryule, levinson, etc. The first two will be discussed here.\n\n\n\nIf, for example, the Matlab command\n\n\n[A,Perror]=lpc(x,2) %estimate a second order filter\n\n\nreturns A=[1.0, -1.7196, 0.81] and Perror=10, then the filter H(z) = 10∕(1 − 1.7196z−1 + 0.81z−2) has poles at z = 0.9±j0.3 and, consequently, the corresponding PSD estimate Ŝ(ejΩ) has a peak at frequency Ω = 0.3 rad with value 10∕|A(ej0.3)|2 ≈ 3082.5.\n\n\n\nIn contrast, pyulear returns an estimation of Ŝ(f) and is useful when the intermediate step of dealing with g∕A(z) is not of interest.\n\n\n\nThe following two commands can be incorporated to the end of Listing 4.23:\n\n\nP=2; %AR filter order \n[Syule,f]=pyulear(y,P,Nfft,Fs); %Directly get the PSD, as via LPC \nplot(f,10*log10(Syule),'r') %compare in dB\n\n\nIt can be seen that pyulear gives the same PSD as the one obtained by first using lpc and then Eq. (4.63).\n\n\n\n\n\n4.8.5  Examples of autoregressive PSD estimation\n\n\n\nTwo examples will be used to address the issues of AR PSD estimation. In the first one, the signal y[n] = x[n]∗h[n] is generated as a realization of an autoregressive model, where x[n] is white Gaussian noise and h[n] is the impulse response of an all-poles IIR filter H(z) = 1∕A(z). In this case, the assumed model matches the actual signal. A mismatched condition is simulated in the second example, where H(z) = B(z) is a FIR filter.\n\n\n\n Example 4.26. Evaluating the PSD of an autoregressive random process. Listing 4.24 creates an all-poles H(z) of order P = 5 and creates an AR(5) signal y[n] with power Py = 3 W. It then estimates the PSD by solving Yule-Walker’s equations via the Levinson-Durbin\nalgorithm and compares it to a PSD estimate via Welch’s method and a theoretical expression, as shown in Figure 4.17.\n\n\n\nListing 4.24: MatlabOctaveCodeSnippets/snip_frequency_PSD_estimation.m\n\n\n%% generate some H(z) - case 1 - AR model (IIR) \np1=0.5; p2=0.3+1j*0.2; p3=0.3-1j*0.2; %defining the poles... \np4=0.9*exp(1j*2);p5=0.9*exp(-1j*2); %as complex conjugates \nAsystem=poly([p1 p2 p3 p4 p5]); %find H(z) denominator \n5Asystem=real(Asystem); %get rid of numerical errors \nBsystem=1; %H(z) numerator given that H(z) is all-poles \nh = impz(Bsystem,Asystem,200); %H(z) impulse response \n%% generate x[n] and y[n] \nFs=8000; %sampling frequency \n10Py_desired = 3; %power in Watts for the random signal y[n] \nS=100000; %number of samples for y[n] \nEh=sum(h.^2) %energy of the system's impulse response \nx=sqrt(Py_desired/Eh)*randn(S,1); %white Gaussian with given power \ny=filter(Bsystem,Asystem,x); %finally, generate y[n] \n15Px=mean(x.^2) %get power, to check if close to Py_desired/Eh \nPy=mean(y.^2) %get power, to check if close to Py_desired \n%% LPC analysis for estimating the PSD of y[n] \nP = 5; %assume we know the correct order of A(z) (matched condition) \n%if using Matlab, it is possible to adopt lpc instead of aryule \n20%[A,Perror]=lpc(y,P); %lpc solves Yule-Walker to estimate H(z) \n[A,Perror]=aryule(y,P); %solves Yule-Walker, both Matlab and Octave \n%note that Perror is approximately Py_desired/Eh, the power of x \nN0over2 = Perror/Fs; %value for the bilateral PSD Sx(w) \nN0=2*N0over2; %assumes a unilateral PSD Sy(w)=N0/|A(z)|^2 \n25Nfft=1024; %number of FFT points for all spectra \n[Hw,f]=freqz(1,A,Nfft,Fs); %frequency response H(w) from 1/A(z) \nSy=N0*(abs(Hw).^2); %unilateral PSD estimated via AR modeling \n[Swelch,f2]=pwelch(y,hamming(Nfft),[],Nfft,Fs); %Welch's PSD \n[Hsystem,f3]=freqz(Bsystem,Asystem,Nfft,Fs); %DTFT of assumed system \n30Sy_theoretical=(Px/(Fs/2)).*(abs(Hsystem).^2);%theoretical PSD \nplot(f2,10*log10(Swelch),f3,10*log10(Sy_theoretical),... \n    f,10*log10(Sy));%compare PSD from Welch, AR and theoretical\n  \n\n\nThe code above uses Eq. (3.98) to generate x[n] (implemented as vector x) with the proper power Py_desired/Eh.\n\n\n\n\n\n\n\n\nFigure 4.17: PSDs estimated from a realization y[n] of a autoregressive process. The model adopted for the AR-based estimation matches the one used to generate y[n].\n\n\n\n\n\nThe signal y in Listing 4.24 is a realization of an autoregressive process AR(5) of order 5. Hence, the model H(z) perfectly matches the process. Smaller values of P would potentially increase the estimation error while larger values lead to curves that are not as smooth as the theoretical one due to the extra (unnecessary) poles in the estimated H(z).\n\n\n\nIf zoomed, the theoretical and AR estimate curves present some discrepancy at the peak around 2546.5 Hz, which corresponds to the pair of poles p4 and p5 at frequencies Ω0 = ±2 rad (recall ω = ΩFs and in this case f0 = Ω0∕Fs∕(2π) ≈ 2546.5 Hz).   □\n\n\n\nThe next example presents a situation where the assumed AR model does not match the FIR filter used to generate y[n].\n\n\n\n Example 4.27. Evaluating the PSD of a moving average random process. Figure 4.18 was generated with the code figs_spectral_whitenoise.m, which is not repeated here, but was created according to the editions listed in Listing 4.25. As indicated, the code uses a FIR to create the signal y[n] corresponding to a MA(10) process.\n\n\n\nListing 4.25: MatlabOctaveCodeSnippets/snip_frequency_MA_process_PSD.m\n\n\nf=[0 0.25 0.3 0.55 0.6 0.85 0.9 1]; %frequencies \nAmp=[1 1 0 0 1 1 0 0]; %amplitudes \nM=10; %filter order \nBsystem=firls(M,f,Amp); %design FIR with LS algorithm \n5Asystem = 1; %the FIR filter has denominator equal to 1 \nh=Bsystem; %impulse response of a FIR coincides with B(z) \n... %here goes the code of previous example, up to P=5 \nP=20;%we do not know correct order of A(z). Use high value \n... %code of previous example continues from here\n  \n\n\n\n\n\n\n\nFigure 4.18: PSDs estimated from a realization y[n] of a moving average process that does not match the model adopted for the AR-based estimation.\n\n\n\n\n\nThe code snippet indicates that the FIR has order M=10 but even using an AR model of order P=20, there is significant discrepancy of the AR-based estimated spectrum in Figure 4.18, especially at the valley regions. In this example, due to the model mismatch, the PSD estimated via the (non-parametric) periodogram achieves better accuracy than the one estimated with the AR model.    □\n\n\n\nA key component of autoregressive spectral estimation is the choice of the model order and there are many methods and criteria for that, such as the minimum description length. A simple alternative is to plot the energy of the prediction residue as the model order increases and choose the best. It helps choosing the model order if one knows aspects of the signal. For example, in speech analysis, each pair of complex-conjugate poles corresponds to a vocal tract resonance known as formant frequency. If one is looking for approximately five formants, a model of order 10 or 12 (considering that real poles can occur) is a reasonable choice. In general, if the order is too low, resolution suffers. If one increases the order too much, spurious peaks may appear.\n\n\n \n\n8 See, e. g., [?] (pages 25 and 32).\n\n                                                                                &lt;/div&gt;"
  },
  {
    "objectID": "ak_dsp_bookse58.html",
    "href": "ak_dsp_bookse58.html",
    "title": "58  Time-frequency Analysis using the Spectrogram",
    "section": "",
    "text": "4.9.1  Definitions of STFT and spectrogram\n\n\n\nThe PSD is a powerful tool to analyze signals in the frequency-domain. However, a single PSD fails when the signal “changes” over time. More strictly, if the signal cannot be assumed (WSS) stationary, alternative tools are potentially needed to describe how information varies in frequency and time domains. One relatively simple technique is the short-time Fourier transform (STFT).\n\n\n\nThe concept behind STFT is to extract segments of the signal under analysis using windowing and calculate several Fourier transforms, one for each segment. Mathematically, the STFT of a continuous-time signal x(t) is\n\n\n\n\n\n\n\n\n\n X(τ,f) = ∫ −∞∞x(t)w(t − τ)e−j2πftdt, \n\n\n(4.64)\n\n\n\n\n\n\nwhere τ is used to shift the window w(t) originally centered at t = 0. Eq. (4.64) can be interpreted by fixing τ = τ0 and observing that X(τ0,f) is the Fourier transform of the windowed signal x(t)w(t − τ0). The STFT is invertible and allows for recovering x(t). However, in the sequel it is assumed that the phase can be discarded given that the main interest is to observe the distribution of power along frequency and time.\n\n\n\nThe spectrogram (for continuous-time)\n\n\n\n\n\n\n\n\n\n S(τ,f) = |X(τ,f)|2 \n\n\n(4.65)\n\n\n\n\n\n\nis defined as the squared magnitude of the STFT and is widely used to analyze nonstationary signals.\n\n\n\n\n\n\n\n\nFigure 4.19: PSD (top) and spectrogram (bottom) of a cosine that has its frequency increased from Ω = 2π∕30 to 2π∕7 and its power decreased by 20 dB at half of its duration.\n\n\n\n\n\nThe specgram function in Matlab/Octave can be used to estimate spectrograms S(τ,Ω) for discrete-time signals. Because S(τ,Ω) is restricted to real numbers, a color scale can be used instead of a 3-d graph. For example, Listing 4.26 was used to generate Figure 4.19.\n\n\n\nListing 4.26: MatlabOctaveCodeSnippets/snip_frequency_cosine_spectogram.m\n\n\nN=3000; %total number of samples \nn=0:N-1; %abscissa \nx1=100*cos(2*pi/30*n); %first cosine \nx2=1*cos(2*pi/7*n); %second cosine \n5x=[x1 x2]; %concatenation of 2 cosines \nsubplot(211), pwelch(x) %PSD \nsubplot(212), specgram(x), colorbar %spectrogram\n  \n\n\nThe code and Figure 4.19 illustrate that the PSD describes only the existence of two cosines but is not capable of informing their location in time. The spectrogram also indicates, by color, that the first half of the signal is composed of a cosine x1 with power (20 dB) greater than x2. The burst of power spread over the whole bandwidth at approximately n = 1500 occurs because the windowed signal at this specific FFT is composed by incomplete cycles of both cosines.\n\n\n\nMatlab (but not Octave) has the spectrogram function. The companion software has ak_specgram, which represent two alternative functions to specgram9 with distinct input parameters.\n\n\n\n\n\n\n\n\nFigure 4.20: All twelve DTMF symbols: 1-9,*,0,#, each one composed by a sum of a low [697,770,852,941] and a high [1209,1336,1477] (Hz) frequencies.\n\n\n\n\n\nAs another spectrogram example, Figure 4.20 shows a sequence of all twelve dual-tone multi-frequency (DTMF) tones generated by the script figs_spectral_dtmf.m. In this case, each symbol has a 100 ms duration. It is possible to visually decode the signal. For example, the first symbol (left-most) is composed by a sum of sines of frequencies 697 and 1,209 Hz (representing “1”) while the second is composed by frequencies 697 and 1,336 Hz (symbol “2”) and so on. Note again the bursts of power at the transitions between symbols.\n\n\n\nAfter creating dtmfSignal with Fs = 8 kHz, the spectrogram of Figure 4.20 was generated with the commands below, and for a better visualization, the dynamic range was restricted to 40 dB via the parameter thresholdIndB:\n\n\nfilterBWInHz=40; %equivalent FFT bandwidth in Hz \nsamplingFrequency=8000; %sampling frequenci in Hz \nwindowShiftInms=1; %window shift in miliseconds \nthresholdIndB=40; %discards low power values below it \n5ak_specgram(dtmfSignal,filterBWInHz,samplingFrequency,... \n   windowShiftInms,thresholdIndB) %calculate spectrogram\n\n4.9.2  Wide and narrowband spectrograms\n\n\n\nA fundamental restriction of the STFT and, consequently, spectrograms, is the tradeoff between time and frequency resolution. When the window is made longer (its duration is increased), the frequency resolution improves but the time resolution gets worse. A spectrogram is called narrowband when the window is long and the FFT invoked by the spectrogram routine is equivalent to a bank of filters (see Application 4.4) with relatively narrow bandwidth. In contrast, a wideband spectrogram uses a short window and, consequently, the FFT corresponds to filters with relatively large bandwidths. The two spectrograms are contrasted here via an example using a speech signal. Speech is highly non stationary given that the information regarding the phonemes is encoded in segments composed of distinct frequencies. The sentence “We were away” was recorded with Fs = 8000 Hz using the Audacity free software and stored as a (RIFF) wav file.\n\n\n\n\n\n\n\n\nFigure 4.21: Example of narrowband spectrogam of a speech signal.\n\n\n\n\n\n\n\n\n\n\nFigure 4.22: Example of wideband spectrogam of a speech signal.\n\n\n\n\n\nFigure 4.21 and Figure 4.22 were generated with Listing 4.27.\n\n\n\nListing 4.27: MatlabOctaveCodeSnippets/snip_frequency_narrow_wide_spec.m\n\n\n[s,Fs,numbits]=readwav('WeWereAway.wav'); %read wav file \nNfft = 1024; %number of FFT points \nM=64; %window length \nspecgram(s,Nfft,Fs,hann(M),round(3/4*M)); colorbar, pause \n5M=256;specgram(s,Nfft,Fs,hann(M),round(3/4*M));colorbar\n  \n\n\nFigure 4.22 shows a broadband spectrogram (good time resolution but poor frequency resolution) calculated with frames of 64 samples obtained by a Hann window. The frames had an overlap of 3/4 of the frame size, and the spectrum of each windowed signal is calculated through a 1024-points FFT. Zero-padding was used (1024 instead of 64) in order to sample more densely the DTFT of the windowed signal. The user is invited to try the command specgram(s,M,Fs,hann(M),0), which corresponds to not using zero-padding and overlapping to notice the improvements these two strategies bring.\n\n\n\nFigure 4.21 simply increases the window length from 64 to 256 to create a narrowband spectrogram (poor time resolution and good frequency resolution). The narrowband version allows to see the harmonic structure due to the pitch (see Application 1.12) as horizontal strips in the graph. This harmonic structure appears in Figure 4.22 as vertical strips.\n\n\n \n\n9 Specgram was discontinued in Matlab.\n\n                                                                        &lt;/div&gt;"
  },
  {
    "objectID": "ak_dsp_bookse59.html",
    "href": "ak_dsp_bookse59.html",
    "title": "59  Applications",
    "section": "",
    "text": "Application 4.1. FFT leakage and picket-fence effects. This application explores a script that illustrates the results discussed in Section 4.3.6 and can be found at folder Applications/FFTLeakagePicketFenceEffects. The version ak_window4_noGUI.m runs on both Matlab and Octave while ak_window4gui.m incorporates a Matlab GUI.10\n\n\n\nBasically the software varies the frequency Ωc of a cosine and compares the magnitudes of its DTFT and FFT. The goal is to show how the cosine is represented by two frequency components  ±Ωc, and the interaction of the corresponding “positive” and “negative” sinc functions for composing the DTFT by their sum. Also, the script indicates how the FFT discretizes the frequency axis, which creates the picket-fence effect, and the leakage that an FFT user observes when Ωc does not coincide with a frequency bin.\n\n\n\nFigure 4.23 is a screenshot when the cosine frequency is Ωc = 1.7279 rad and the corresponding DTFT magnitude. The cosine is represented by two spectral lines (tones), at normalized frequencies  ±Ωc∕π = ±0.55. The (dashed) reference line indicates the value N(A∕2) = 24 given the FFT-length N = 8 and cosine amplitude A = 6. Note that the DTFT, which is the sum of two sinc functions, surpasses this reference value for normalized frequencies around  ± 0.55.\n\n\n\n\n\n\n\n\nFigure 4.23: DTFT magnitude of a cosine of frequency Ωc = 1.7279 rad.\n\n\n\n\n\nFigure 4.24 corresponds to a situation where Ωc = 2.1206 rad and the FFT magnitude values are superimposed to the DTFT. Besides, the two sinc functions centered at normalized frequencies  ± 0.675 are also displayed such that one can see the DTFT being composed by their summation.\n\n\n\n\n\n\n\n\nFigure 4.24: DTFT and FFT magnitudes of a cosine of frequency Ωc = 2.1206 rad.\n\n\n\n\n\nIn Figure 4.24, Ωc does not coincide with an FFT bin and the leakage is observed by the FFT values, which are samples of the DTFT. The picket-fence is clearly seen because in this case the resolution is low, and only N = 8 DTFT values in the range [−π,π[ are obtained by the FFT. Increasing N alleviates this effect. The reader is invited to run the code with different settings.    □\n\n\n\n Application 4.2. Using Welch’s method to estimate the mean square (MS) spectrum. As explained, to decrease the variance of the estimated spectrum, it is useful to adopt Welch’s method. In this case, one should consider the effect of the window w[n] of L samples on the estimation.\n\n\n\nIn Matlab/Octave it is convenient to estimate a MS spectrum using pwelch, which takes care of segmenting the input signal. Matlab has support for MS spectrum estimation using pwelch while Octave does not.\n\n\n\nWhen used for PSD estimation, pwelch.m scales the periodogram dividing it by the energy of the window:\n\n\n\n\n\n\n E = ∑ n=0L−1w2[n]. \n\n\n\n\n\n\nIn contrast, when estimating a MS spectrum, the periodogram should be divided by the square of the window DC value\n\n\n\n\n\n\n WDC2 =  (∑ n=0L−1w[n])2. \n\n\n\n\n\n\nThe reason is that the window is convolved with each power spectrum peak and WDC should be 1 to avoid modifying the peak height.\n\n\n\nMatlab has the undocumented option of invoking pwelch.m with the argument ’ms’ for MS spectrum, such as in:\n\n\nH = pwelch(x,window,[],[],Fs,'twosided','ms');\n\n\nwhich uses the adequate scaling factor to estimate a MS spectrum.\n\n\n\nBecause ’ms’ is not supported in Octave, Listing 4.28 illustrates that a workaround is to multiply the estimated spectrum by E∕WDC2.\n\n\n\nListing 4.28: MatlabOctaveCodeSnippets/snip_frequency_msspectrum.m\n\n\nN=1024; x = transpose(10*cos(2*3/64*(0:N-1))); %generate a cosine \nXk = (abs(fft(x,N))/N).^2; %MS spectrum: |DTFS|^2 \nFs = 2*pi; myWindow = hamming(N); %specify Fs and a Hamming window \nH = Fs*pwelch(x,myWindow,[],N,Fs,'twosided');%Welch's estimate. Third \n5%parameter is [] because in Matlab is num. samples while Octave is % \nH = H * sum(myWindow.^2)/sum(myWindow)^2; %scale for MS \nplot(Xk,'x-'), hold on, plot(H,'or-') %compare\n  \n\n\nUsing either Matlab or Octave, this code provides a pair of peaks of approximately 25 W for H, estimated via Welch’s method. Note that the cosine is not bin-centered and there is leakage. Matlab users11 can try a flat top window in place of Hamming’s with the command myWindow=flattopwin(N). When estimating the MS spectrum, a flat top window helps because it widens any peak in the original spectrum, such that the wider range of values has more chances of coinciding with a FFT bin. In other words, the flat top is not accurate to locate the sinusoid in frequency (bad frequency resolution) but helps when the goal is to find the sinusoid amplitude.\n\n\n\nAnother example with a sinusoid that is not bin-centered is provided below. It allows to observe the better performance of averaging segments of the signal using pwelch.m, as indicated in Listing 4.29.\n\n\n\nListing 4.29: MatlabOctaveCodeSnippets/snip_frequency_not_bin_cent_pwelch.m\n\n\nN=1000; x = 10*cos(2*pi/64*(0:N-1)); %generate a cosine \nXk = (abs(fft(x,N))/N).^2; %MS spectrum |Xk|^2 \nFs = 2*pi; window = hamming(128); %specify Fs and window \nH=2*pi*pwelch(x,window,[],128,Fs,'twosided'); %Welch's estimate \n5H = H * sum(window.^2)/sum(window)^2; %scale for MS \ndisp(['Peak from pwelch = ' num2str(max(H)) ' Watts']) \ndisp(['Peak when using one FFT = ' num2str(max(Xk)) ' W'])\n  \n\n\nThis code informs that Xk has peaks of 15.66 W and H (estimated via pwelch.m with segments of M = 128 samples) has peaks of 25.02 W (recall that the correct value is 25 W). In this and many practical cases, averaging windowed segments with pwelch.m leads to better results than using a single FFT.   □\n\n\n\n Application 4.3. Relation between roots location and spectrum. A window w[n] is used to multiply a signal in time-domain, but for the sake of observing its spectrum, it can be interpreted as the impulse response of a FIR filter, which does not have finite poles, only zeros. Under this assumption, there is an interesting relation between the zeros of the windows, and its spectrum W(ejΩ).\n\n\n\n\n\n\n\n\nFigure 4.25: Relationship between roots location and spectrum for a rectangular window with N = 32 samples.\n\n\n\n\n\nFigure 4.25 shows the spectrum (left plot) of a rectangular window with N = 32 and the roots (right plot) of this window. This plot outlines that there are N − 1 zeros (the amount of roots of a polynomial of order N) and the width of the main-lobe is obtained from the angle Θ = 2π∕N (in this case, Θ = 2π∕32 ≈ 0.1963 rad) of the first root, given that there is no root at Ω = 0 and the N − 1 roots are uniformly spaced around the unit circle.\n\n\n\n\n\n\n\n\nFigure 4.26: Relationship between roots location and spectrum for a Kaiser window with N = 32 samples.\n\n\n\n\n\nFigure 4.26 shows that a greater concentration of zeros in the region of the main-lobe, allows the Kaiser window (with β = 7.85) achieve a very small first side-lobe. However, the width of the main-lobe increases considerably when compared to the rectangular window in Figure 4.25, due to the increase of Θ. In summary, the rectangular window has the smallest main-lobe, but its first sidelobe level is just  − 13 dB below the main-lobe level. On the other hand, the Kaiser window has a wider main-lobe, but this window has very small sidelobe levels.    □\n\n\n\n Application 4.4. FFT Interpreted as a Filter Bank. Filter banks are often used for performing spectrum analysis and signal synthesis. A DFT can be seen as a uniform filter bank due to the fact that all filters have the same bandwidth.\n\n\n\nThe interpretation of FFT and other transforms as filter banks lead to improved insight. For example, the good result obtained by the rectangular window in Figure 4.4 is due to a particular property of its side-lobe structure: all side-lobes are zero in the center of the bins. When the spectrum being analyzed has non-zero components only in bin centers, the leakage due to the side-lobes will not appear in the final FFT result. The leakage could be seen in the DTFT though.\n\n\n\n\n\n\n\n\nFigure 4.27: DFT filter bank with rectangular window of N = 8 samples. The circles mark the DFT bin centers. The filter for k = 3 is emphasized.\n\n\n\n\n\n\n\n\n\n\nFigure 4.28: DFT filter bank with Kaiser window of N = 8 samples. The filter for k = 6 is emphasized.\n\n\n\n\n\nFigure 4.28 and Figure 4.27 indicate the filter banks corresponding to an FFT with rectangular and Kaiser windows, respectively. The rectangular has larger side-lobe levels, but all of them achieve a zero in the bin centers. The Kaiser window does not have side-lobes with amplitude equal to zero in the bin centers, but these side-lobe levels are very small. Modifying the scripts that obtained these figures allows to show the scalloping loss of each window, as discussed in [?].    □\n\n\n\n Application 4.5. Smoothing an FFT result by segmenting the signal and averaging the individual FFTs. When the task is to calculate the FFT of a noisy signal with N samples, it helps to segment it in M blocks and average the result of M FFTs. This is a basic strategy in spectral estimation, which is adopted in Welch’s method, for example. Here, the noisy signal is assumed to be the one recorded in the file impulseResponses.wav, as suggested in Application 1.4.\n\n\n\nFor isolating the response to an impulse using the signal described in Application 1.4, after zooming Figure 1.63 it was decided to consider the range of samples from n = 12650 to 22050 (given the third impulse was generated at n = 22051). This way the selected signal does not have many samples with small amplitudes before it actually begins.\n\n\n\nIn case you do not have available the companion file impulseResponses.wav and cannot generate yours due to the lack of a loopback cable, then use the signal suggested in the script below by changing the if instruction in line 3:\n\n\n\nListing 4.30: MatlabOctaveCodeSnippets/snip_systems_smothing_FFT.m\n\n\nnstart=12650;%chosen after zooming the signal in impulseResponses.wav \nnend=22050;%this was the chosen segment. Adjust them for your data! \nif 1 %if you have impulseResponses.wav available \n    [h,Fs,b]=readwav('impulseResponses.wav'); \n5    h=double(h(nstart:nend)); %segment and cast h to double \nelse %use signal with few samples extracted from impulseResponses.wav \n    duration = floor(nend-nstart+1); %same duration as h above \n    h =[-1051, 4155, -32678, -11250, 5536, -4756, 2941, -3162]; \n    h = [h zeros(1,duration-length(h))]; %pad with zeros \n10    h = h + 10*randn(1,length(h)); %add some noise \n    Fs=44100; %chosen sampling frequency \nend \nN=1024; %number of FFT points \nM=floor(length(h)/N); %number of segments of N samples each \n15h=h(1:N*M); h=h(:); %make sure h is a column vector with N*M samples \nxsegments=reshape(h,N,M); %segment h into M blocks \nX=abs(fft(xsegments)); %obtain the magnitude for each segment \nX=mean(transpose(X)); %transpose: the mean has to be over the FFTs \nf=Fs/N*(0:N/2)/1000; %create abscissa in kHz. Fs/N is the bin width \n20plot(f,20*log10(X(1:N/2+1))),xlabel('f (kHz)'),ylabel('|H(f)| (dBW)')\n  \n\n\nInstead of the FFT, another option is to use the command pwelch(h,N,N/2,N,Fs) to observe the PSD.\n\n\n\n\n\n\n\n\nFigure 4.29: Sound system magnitude frequency response |H(f)| estimated from an impulse response.\n\n\n\n\n\nFigure 4.29 is the result obtained by running the previous script. It can be seen that the filters along the processing chain that includes the DAC and ADC present strong attenuation after 20 kHz and extra gain from DC to approximately 800 Hz. Choosing Fs different than 44.1 kHz would move the cutoff frequencies fc of the anti-aliasing and reconstruction filters. These filters are analog, but are programmable in the sense that fc can be modified by software (after the user chooses a new Fs in Audacity, for example). For example, switched-capacitor filters use a technology that allows this programmability feature. To check this feature in your sound board, repeat the experiment with Fs = 8,000 and 22,050 Hz.\n\n\n\nAnother alternative for obtaining an estimate of |H(f)| is to use a white noise as input to the system and use Eq. (4.36).\n\n\n\n\n\n\n\n\nFigure 4.30: Sound system magnitude frequency response |H(f)| estimated from a white noise input.\n\n\n\n\n\nAudacity can conveniently generate white noise via the menu “Generate - Noise”. Another option is to create the noise in Matlab/Octave, save as a WAVE file and read it with Audacity. Figure 4.30 was obtained using the former procedure to generate noise, play it back and record the system response using a loopback cable as for Figure 4.29. Then, the transient in the beginning of the recorded signal was discarded and approximately 32 thousand samples were saved as a WAVE file for processing in Matlab/Octave via the script:\n\n\n\nListing 4.31: MatlabOctaveCodeSnippets/snip_systems_recorded_noise.m\n\n\n[x,Fs,b]=readwav('filteredNoise.wav'); %recorded output \nN=1024; %number of FFT points \nM=floor(length(x)/N); %number of segments of N samples each \nx=x(1:N*M); x=x(:); %make sure x is a column vector with N*M samples \n5xsegments=reshape(x,N,M); %segment x into M blocks \nX=abs(fft(xsegments)); %obtain the magnitude for each segment \nX=mean(transpose(X)); %transpose: the mean has to be over the FFTs \nf=Fs/N*(0:N/2)/1000; %create abscissa in kHz. Fs/N is the bin width \nplot(f,20*log10(X(1:N/2+1))),xlabel('f (kHz)'),ylabel('|H(f)| (dBW)')\n  \n\n\nFigure 4.30 and Figure 4.29 are consistent but differ with respect to ordinate values and variance of the estimate. Eq. (4.41) states that the input noise level scales the output PSD. Take this in account and repeat the experiment with an input noise of controlled power, such that you can use Eq. (4.41) to properly scale the estimate and make the values closer to the ones in Figure 4.29. Investigate other factors that improve the relation between the two figures.   □\n\n\n\n Application 4.6. Speech formant frequencies via LPC analysis.\n\n\n\nAn interesting application of LPC analysis is to estimate the formant speech frequencies. The formants are related to the peaks of the spectrum of a vowel sound and are relatively well-defined in a sentence such as “We were away”, which does not have consonant sounds.\n\n\n\n\n\n\n\n\nFigure 4.31: Spectrogram and tracks of the first four formant frequencies estimated via LPC for a speech sentence “We were away”.\n\n\n\n\n\nListing 4.32 was used to obtain Figure 4.31.\n\n\n\nListing 4.32: MatlabOctaveCodeSnippets/snip_frequency_formant_frequencies.m\n\n\n[s,Fs,numbits]=readwav('WeWereAway.wav'); %read wav file \ns = s - mean(s(:)); %extract any eventual DC level \nNfft=1024; %number of FFT points \nN = length(s); %number of samples in signal \n5frame_duration = 160; %frame duration \nstep = 80; %number of samples the window is shifted \norder = 10 %LPC order \nnumFormants = 4 %desired number of formants \n%calculate the number of frames in signal \n10numFrames = floor((N-frame_duration) / step ) + 1 \nwindow = hamming(frame_duration); %window for LPC analysis \nformants = zeros(numFrames,numFormants); %pre-allocate \nfor i=1:numFrames %go over all frames \n    startSample=1+(i-1)*step; %first sample of frame \n15    endSample=startSample + frame_duration - 1; %frame end \n    x = s(startSample:endSample); %extract frame samples \n    x = x.* window; %windowing \n    a = aryule(x,order); %LPC analysis \n    poles = roots(a); %Roots of filter 1/A(z) \n20    freqsInRads = atan2(imag(poles),real(poles)); %angles \n    freqsHz = round(sort(freqsInRads*Fs/(2*pi)))'; %in Hz \n    frequencies=freqsHz(freqsHz&gt;5); %keeps only &gt; 5 Hz \n    formants(i,:) = frequencies(1:numFormants); %formants \nend \n25window = blackman(64); %window for the spectrogram \nspecgram(s,Nfft,Fs,window,round(3/4*length(window))); \nylabel('Frequency (Hz)'); xlabel('Time (sec)'); hold on; \nt = linspace(0, N / Fs, numFrames); %abscissa \nfor i=1:numFormants %plot the formants \n30    text(t,formants(:,i),num2str(i),'color','blue') \nend\n  \n\n\nThe code illustrates how a long signal can be segmented via windowing. It is interesting to notice that the Hamming window is typically adopted in LPC analysis, and the code uses another (Blackman) window for the spectrogram. An LPC of order 10 was used to estimate four formants. The code eliminates frequencies below 5 Hz because LPC sometimes returns real poles that correspond to a zero frequency.\n\n\n\nFigure 4.31 indicates that at the sentence endpoints and at the pauses between words, when the vowel sounds are not well-defined, the formant estimation is noisy. In the middle of the sentence, around 0.5 s, the four formants are clearly identified. In the region around 0.2 s and also the one around 0.8 s, the estimation is problematic because more than four formants are necessary to describe the signal.    □\n\n\n\n Application 4.7. Spectral distortion. When the task is to compare two PSDs, the spectral distortion (SD) can be used. It is given by\n\n\n\n\n\n\n\n\n\n SD =    1 2π∫ −ππ  [10log ⁡ 10  (Sx(ejΩ) Sy(ejΩ) )]2dΩ \n\n\n(4.66)\n\n\n\n\n\n\nwhere x[n] and y[n] are discrete-time signals and Sx(ejΩ) and Sy(ejΩ) their respective PSDs. SD is given in dB and corresponds to the root mean square value of the error between 10log ⁡ 10Sx(ejΩ) and 10log ⁡ 10Sy(ejΩ) over frequency Ω. It should be noted that the SD can also be used to compare two MS spectra.\n\n\n\nBecause both the PSD and MS spectrum are normalized versions of the squared magnitude of a DTFT and the SD divides two spectra, any normalization factor is canceled out in Eq. (4.66). Therefore, Eq. (4.66) can be written in terms of DTFTs as\n\n\n\n SD  =    1 2π∫ −ππ  [10log ⁡ 10  (|X(ejΩ)|2 |Y (ejΩ)|2 )] 2dΩ (4.67)   =    1 2π∫ −ππ  (20log ⁡ 10|X(ejΩ)|− 20log ⁡ 10|Y (ejΩ)|)2dΩ (4.68)  \n\nwhere X(ejΩ) and Y (ejΩ) are the respective DTFTs.\n\n\n\nEq. (4.68) can be approximated by\n\n\n\n\n\n\n\n\n\n SD ≈   1 N∑ k=0N−1  (20log ⁡ 10|X[k]|− 20log ⁡ 10|Y [k]|)2 \n\n\n(4.69)\n\n\n\n\n\n\nwhere X[k] and Y [k] are the respective N-points FFTs. Listing 4.33 illustrates a software routine to calculate SD.\n\n\n\nListing 4.33: MatlabOctaveFunctions/ak_spectralDistortion.m\n\n\nfunction distortion = ak_spectralDistortion(x,y,Nfft,thresholdIndB) \n% function distortion = ak_spectralDistortion(x,y,Nfft,thresholdIndB) \n%Returns the spectral distortion (SD) in dB as defined at \n% http://en.wikipedia.org/wiki/Log-spectral_distance \n5X=abs(fft(x,Nfft)); %Calculate magnitude values of DTFTs \nY=abs(fft(y,Nfft)); \nlogX=20*log10(X); %Convert to dB and use factor=20 to convert ... \nlogY=20*log10(Y); %DTFTs into power spectra using log(X^2)=2*log(X) \nmaximumValue = max([max(logX) max(logY)]); %Avoid numerical problems \n10minimumValue = maximumValue - thresholdIndB; %such as log of 0 \nlogX(logX&lt;minimumValue)=minimumValue; %Impose the floor value \nlogY(logY&lt;minimumValue)=minimumValue; \ndistortion = sqrt(mean((logX-logY).^2)); % SD as a mean-squared error\n  \n\n\nOne issue when using logarithms is to deal with argument values equal to zero. For example, in Matlab/Octave, log(0) gives -Inf and can lead to NaN after operations such as 0*log(0). To avoid numerical problems, Listing 4.33 uses a floor value based on a threshold to limit the minimum value of an argument for logarithm functions. Some lines of Listing 4.33 that are not being shown also deal with special situations and are an example of how to prevent problems via proper exception treatment when using software. For example, some lines of Listing 4.33 deal with the situation where both signals x[n] and y[n] have only zero values.   □\n\n\n\n Application 4.8. Spectral distortion of speech autoregressive models. In speech coding applications, the signal is segmented into frames. Given two speech signals, it is discussed here how to compare their spectra using AR models in a frame-by-frame basis. While Application 4.7 used one value of SD for the whole duration of the signals, here the SD is calculated for each frame.\n\n\n\nThe pair of signals can be found at folder Applications/SpeechAnalysis. One of the files correspond to the digit “eight” spoken by a male speaker. The other file was generated by a computer (more specifically, using the Klatt speech synthesizer) and aims at sounding indistinguishable from the first, or target speech.\n\n\n\nListing 4.34 shows a code snippet with the part that segments the signals into frames and calculates the SD as described in Listing 4.33 and also two other SD versions. These SD versions are comparisons between autoregressive models and are calculated with the function ak_ARSpectralDistortion.m. Listing 4.34 uses the array isNotSilence, which was obtained from function Applications/SpeechAnalysis/endpointsDetector.m, to avoid computing the SD values for frames that have a low power and can be considered as silence, not speech.\n\n\n\nListing 4.34: Applications/SpeechAnalysis/spectralDistortion.m\n\n\nfor i=1:M %do not use last frame, because it may use zero padding \n    if isNotSilence(i)==0 \n        continue; %skip because this is a silence frame \n    end \n30    firstSample = 1+(i-1)*S; \n    lastSample = firstSample + L - 1; \n    x=target(firstSample:lastSample); %frame from target signal \n    y=synthetic(firstSample:lastSample); %frame from the other signal \n    sd_all(i)=ak_spectralDistortion(x,y); \n35    sdAR_all(i)=ak_ARSpectralDistortion(x,y,10,Nfft); \n    sdARp_all(i)=ak_ARSpectralDistortion(x,y,10,Nfft,60,1); \n    disp(['#' num2str(i) ': SD=' num2str(sd_all(i)) ', AR SD=' ... \n        num2str(sdAR_all(i)) ' and AR SD (power)= ' ... \n        num2str(sdARp_all(i))]); \n40end\n  \n\n\nAutoregressive models are widely used in speech coding applications and their quantization is often evaluated according to the AR version of the SD. In this context, transparent quantization is obtained when the average SD is not larger than 1 dB, having no\noutliers with SD larger than 4 dB and at most 2% of frames with SD between 2 and 4 dB.    □\n\n\n \n\n10 The files ak_window4gui.m and ak_window4gui.fig should be at the same folder.\n\n \n\n11 Because the flat top window has negative values, it trigs a bug [urlobug] in Octave’s pwelch (signal package, version 1.2.2).\n\n                                                                                               &lt;/div&gt;"
  },
  {
    "objectID": "ak_dsp_bookse60.html",
    "href": "ak_dsp_bookse60.html",
    "title": "60  Comments and Further Reading",
    "section": "",
    "text": "Spectral analysis is the topic of several good books, such as [?]. As discussed in these books dedicated to spectral analysis, there are involved issues on whether or not random signals can be analyzed with Fourier transforms in the same way done for deterministic power signals. Because it turns out the answer is yes, and the result is elegant and even intuitive, the proof and corresponding discussion was omitted here. Another strategy adopted here to avoid going deeper in the theory of spectral analysis was to focus primarily on discrete-time signals.\n\n\n\nTwo classic works about windows for FFT-based analysis are [?,?]. Scalloping loss and its compensation are well described in [?]. The windows described here are the so-called “symmetric” windows in Matlab’s documentation, which also discusses “periodic” windows. Another variation is to center the window at the origin n = 0 as discussed in [?], while here a window with N samples starts at n = 0 and ends at n = N − 1. The issue of correcting the amplitude and power of a FFT-based spectral analysis is discussed in [?].\n\n\n\nThere are distinct definitions in the literature for the periodogram. For example, it can be defined as e. g. in [url4spe]:\n\n\n\n\n\n\n\n\n\n Ŝ(ejΩ)=def 1 N|DTFT{xN[n]}|2 \n\n\n(4.70)\n\n\n\n\n\n\nto be an approximation of the discrete-time PSD S(ejΩ). However, this definition is not directly compatible with the periodogram function in Matlab/Octave.\n\n\n\nMore information about linear prediction can be obtained in specialized textbooks such as The Theory of Linear Prediction, by P. P. Vaidyanathan, 2008, available on the Web [url4ppv].\n\n\n\nIn  [?], it is discussed the minimum description length (MDL) criteria applied to autoregressive models. In spite of the relative simplicity of MDL, its application to autoregressive modeling is involved and [?] addresses several important issues.\n\n\n\nThe PSD and, equivalently, the autocorrelation, cannot convey all the information of a signal. Similarly, spectrograms are just one of the many techniques for time-frequency analysis. For example, wavelets and Wigner-Ville distributions  [?,?] have found various applications.\n\n\n\nIn many applications, higher-order spectra analysis  [?] can bring additional insight when compared to the techniques discussed in this chapter. Another very powerful tool is cyclostationary analysis  [?,?], which is important in applications such as blind signal identification  [?] and spectrum sensing for cognitive radios  [?]."
  },
  {
    "objectID": "ak_dsp_bookse61.html",
    "href": "ak_dsp_bookse61.html",
    "title": "61  Exercises",
    "section": "",
    "text": "4.1. Using the DTFT W(ejΩ) of the N-samples rectangular window given in Eq. (4.7), prove that the DTFTs of the Hann and Hamming windows, given in Eq. (4.3) and Eq. (4.2), are 0.5W(ejΩ) − 0.25W(ej(Ω+2π∕(N−1))) − 0.25W(ej(Ω−2π∕(N−1))) and 0.54W(ejΩ) − 0.23W(ej(Ω+2π∕(N−1))) − 0.23W(ej(Ω−2π∕(N−1))), respectively. 4.2. A cosine x[n] = Acos ⁡ (Ω1n) is multiplied by a rectangular window of N = 8 samples, from n = 0,…,7 and the windowed signal xw[n] has its spectrum estimated by an FFT with N points. The FFT resolution is ΔΩ = (2π)∕N rad. Using the DTFT of xw[n], what are the FFT values for X[k]|k=3 assuming: a) Ω1 = 3ΔΩ is centered in the fourth bin and b) Ω1 = 2.5ΔΩ is half-way the third and fourth bins? 4.3. Given the signal x(t) = 12sinc(6t) in Volts, what are its: a) total signal energy and b) energy within the frequency band [−1,12] Hz? 4.4. What is the MS spectrum Ŝms[k] of x[n] = 3cos ⁡ ((π∕4)n) Volts? Using the values of Ŝms[k], how can one obtain the average power of x[n] in Watts? 4.5. Assume X[k] is the N-points FFT of a periodic signal x[n] of period N0 = 8\nsamples. When N = 8, X[k] is [0,3 + 4j,0,6,0,6,0,3 − 4j]. Inform: a) the estimated MS spectrum Ŝms[k] of x[n], b) the signal power P and c) the new Ŝms[k] in case N = 16 samples of x[n] were used, together with a 16-points FFT. 4.6. Assume X = [8,8,8,8] is the 4-points FFT of a discrete-time signal x[n]. What are the values of its: a) DTFS, b) MS spectrum Ŝms[k], c) the periodogram Ŝ[k] (using Matlab/Octave convention of BW = 2π), d) the estimated PSD Ŝ(ejΩ) and e) the signal power P. 4.7. a) What is the PSD S(ejΩ) of a complex exponential x[n] = ejΩ1n, with Ω1 = π∕4 rad after multiplication by a rectangular window of N = 10 samples? b) When the periodogram Ŝ[k] of this signal is estimated with a FFT of N = 10 points, what are the values of Ŝ[0] and Ŝ[1]? 4.8. The periodogram of a sinusoid immersed in AWGN was calculated with an 8-points FFT as [2,2,2,6,2,6,2,2] in Watts/Hz, assuming Fs = 500 Hz. The noise and the sinusoid are uncorrelated, such that, at the sinusoid bins, the power is the sum of the sinusoid power and the noise power at that bin. Inform: a) the sinusoid average power, b) the noise average power, c) the SNR in dB. 4.9. The bilateral PSD of a continuous-time white noise signal ν(t) is N0∕2 = 8 W/Hz. This signal was digitized using an ideal lowpass filter and Fs = 20 kHz, creating a discrete-time signal ν[n]. Inform: a) the average power of ν(t),\n\nthe average power of ν[n] and c) the power corresponding to a single periodogram bin of a windowed ν[n] estimated with an FFT of length N = 256 when Fs = 20 kHz and Fs = 2π are informed. 4.10. The bilateral PSD of a continuous-time white noise signal ν(t) is N0∕2 = 8 W/Hz. This signal was digitized using an ideal lowpass filter and Fs = 20 kHz, creating a discrete-time signal ν[n]. Inform: a) the average power of ν(t),\nthe average power of ν[n] and c) the power corresponding to a single periodogram bin of a windowed ν[n] estimated with an FFT of length N = 256 when Fs = 20 kHz and Fs = 2π are informed. 4.11. Assuming Fs = 100 Hz, the result of Welch’s method with 8-length FFTs for a real signal composed by a sum of two sinusoids was S = [0,100,0,20,0] in dBW/Hz. Inform: a) the sinusoid frequencies, b) the sinusoid powers in Watts and c) their power ratio in dB. Hint: one can use Matlab/Octave to investigate this setup with the commands:\n\n\n\nN=8; n=0:N-1; x1=4*cos(pi/4*n); x2=10*cos(3*pi/4*n); x=x1+x2; \nFs=100; [S,f]=pwelch(x,rectwin(N),0,N,Fs), S*f(2), SdB=10*log10(S)\n\n4.12. A signal\n\n\nx[n]\n\nhas its PSD estimated via AR modeling with the result: A=[1.0, -1.8151, 0.9025] and Perror=4 (for example, with the Matlab command [A,Perror]=lpc(x,2)). What is the frequency of the peak of this PSD and its amplitude? 4.13. An autoregressive model\n\n\nH(z)\n\nof order one was obtained with the command [A,Perror]=lpc(x,1)), where A=[1, -0.75] and Perror=0.04 Watts. a) What is the expression for\n\n\nH(z) = g∕A(z)\n\nassuming that\n\n\ng\n\nincorporates the information from Perror? b) What is the expression for the PSD\n\n\nŜ(f)\n\ncorresponding to this model assuming\n\n\nFs = 100\n\n Hz (the expression\nmust depend only on\n\n\nf\n\n)? c) What is the value of\n\n\nŜ(f)\n\nat frequency\n\n\nf = 2\n\n Hz? 4.14. The unilateral PSD of a continuous-time white noise signal\n\n\nν(t)\n\nis\n\n\nN0 = 4\n\n W/Hz. The signal\n\n\nν(t)\n\nis the input to a lowpass filter with real-valued coefficients, gain\n\n\ng = 3\n\nand zero phase within the passband from 0 to 200 Hz (this filter is not realizable). The filter output\n\n\nx(t)\n\nis converted to a discrete-time signal\n\n\nx[n]\n\nusing a C/D process with sampling frequency\n\n\nFs = 1\n\n kHz. The average power values of\n\n\nx(t)\n\nand\n\n\nx[n]\n\nare the same and denoted as\n\n\nP\n\n. Inform: a) the value\n\n\nP\n\nin Watts, b) the detailed graphs of the PSDs\n\n\nSν(f)\n\n,\n\n\nSx(f)\n\n, and\n\n\nSx(ejΩ)\n\ncorresponding to the signals\n\n\nν(t)\n\n,\n\n\nx(t)\n\nand\n\n\nx[n]\n\n, respectively. Note that\n\n\nx[n]\n\nis not a discrete-time white noise, because its PSD does not have a constant value over the range\n\n\n[−Fs∕2,Fs∕2[\n\n. 4.15. The periodogram of a signal\n\n\nx[n]\n\nwith\n\n\nN = 100\n\nsamples is estimated via Welch’s method using a rectangular window\n\n\nw[n]\n\nwith\n\n\nM = 25\n\nsamples. The sampling frequency is\n\n\nFs = 1∕Ts = 100\n\n Hz. The window shift is\n\n\nM\n\nsamples, such that there is no overlapping among windows, and the samples of\n\n\nx[n]\n\nare organized into four blocks of\n\n\nM\n\nsamples each. After zero-padding, each block of samples is converted to frequency domain using an FFT of\n\n\nNf = 128\n\npoints. a) What is the frequency spacing\n\n\nΔf\n\nin Hertz between neighboring periodogram bins? b) What is the frequency resolution\n\n\nΔm\n\nin Hertz imposed by\nthe DTFT\n\n\nW(ejΩ)\n\nof the windows\n\n\nw[n]\n\n? Assume that\n\n\nΔm\n\nis the range between the two zeros of\n\n\nW(ejTs2πf)\n\nthat define its main lobe, where\n\n\nW(ejTs2πf)\n\nis the DTFT converted to continuous-time using\n\n\nΩ = Tsω = Ts2πf\n\n. c) Is the overall frequency resolution limited by\n\n\nΔf\n\nor\n\n\nΔm\n\n? d) Why is it that someone cannot consistently improve the overall resolution in spectral analysis by simply using zero-padding and improving\n\n\nΔf\n\nby using a larger number\n\n\nNf\n\nof FFT points?"
  },
  {
    "objectID": "ak_dsp_bookse62.html",
    "href": "ak_dsp_bookse62.html",
    "title": "62  Extra Exercises",
    "section": "",
    "text": "4.1. Taking Figure 4.20 as a reference, decode the DTMF symbols of Figure 4.32, which correspond to a phone number with eight digits.\n\n\n \n\nFigure 4.32: Eight DTMF symbols, each one with a 100 ms duration.\n\n\n\n\n4.2. Can a discrete-time random process\n\n\nX[n]\n\nhave infinite power? If yes, under what conditions? Can these conditions be achieved in practice? 4.3. Modify Listing 4.35 to investigate how the sidelobes are decreased in the DTFT of the Hann window by combining three DTFTs of a rectangular window. Then, perform the same analysis for the Hamming window.\n\n\n\nListing 4.35: MatlabOctaveCodeSnippets/snip_frequency_testHannDTFT.m\n\n\nfunction snip_frequency_testHannDTFT \nN=9; %number of window samples \n[dtftHann,w]=freqz(hann(N)); %get values of the DTFT as reference \ndtftHann2=0.5*rectWinDTFT(w,N)- 0.25*rectWinDTFT(w+(2*pi/(N-1)),N)... \n5    -0.25*rectWinDTFT(w-(2*pi/(N-1)),N); %DTFT using expression \nplot(w,real(dtftHann-dtftHann2),w,imag(dtftHann-dtftHann2)) %error \nend \nfunction D=rectWinDTFT(w,N) %get DTFT of rectangular window \nD=sin(0.5*N*w)./sin(0.5*w).*exp(-j*0.5*w*(N-1)); \n10end\n\n\n4.4. Explore the computation and relationships among convolution, correlation and FFT. Assume a vector x=[0 1 4 8 0] with\n\n\nM = 5\n\nelements. a) Explain why in Matlab, conv(x,x), the convolution of x with x itself results into a vector with\n\n\n2M − 1\n\nelements. b) Compare the equation of a convolution x[n]\n\n\n∗\n\nh[n]\n\n\n\n y[n] = ∑ k=−∞∞h[k]x[k − n]  \n\nwith the equation of an autocorrelation \n\n\n R[ℓ] = ∑ k=−∞∞x[k]x[k + ℓ].  \n\nThen, prove that x[n]∗x[−n]=R[n] and confirm it in Matlab observing that conv(x,fliplr(x)) and xcorr(x) lead to (approximately) the same result in Matlab. Plot conv(x,x) and xcorr(x). c) The results differ in b) mainly because xcorr uses a FFT to compute the correlation. Try to explain how the following code is used to compute the autocorrelation\n\n    % Autocorrelation\n    % Compute correlation via FFT\n    X = fft(x,2^nextpow2(2*M-1));\n    c = ifft(abs(X).^2);\n\n\nYou may want to use type xcorr and take a look at the source code to see how the vector c above is modified to give the expected result (i.e., there is a postprocessing step after the two lines above). 4.5. Short-time energy. Use the same procedure as before to record the sentence “We were away” or similar. Calculate the speech energy e for each frame of t milliseconds (ms), without overlap among frames. Use t ∈{10,30,100,200} ms to get four energy trajectories. Estimate the first-derivative of e using Δen = en − en−1. Estimate the second-derivative using ΔΔen = Δen −Δen−1. Use subplot to show simultaneously the waveform (subplot(411)) , e (subplot(412)), Δe (subplot(413)) and ΔΔe (subplot(414)). Do that for each t and choose the value of t that leads to the best plots in terms of distinguishing when it is silence or speech. 4.6. Spectrograms. Use a function such as specgram to plot both the wide and narrowband spectrograms of the sentence recorded in the previous exercise. In both cases, a) what were the window lengths and equivalent bandwidths you used? b) What were the complete commands? 4.7. Linear regression. Assume there are N points (x,y), where x ∈ ℝ and y ∈ ℝ. Find the parameters a and\n\nb of a first order polynomial y~ = ax + b that best fits the data in terms of the minimum total squared error E = ∑ ⁡ n=1N(yn −y~n)2. However, try to write the two final equations using correlations (for example, the autocorrelation Rx(0) = ∑ ⁡ n=1Nxn2 at lag m = 0 is one possible term). Use your equations to fit a line to the following training data: x=rand(1,100); y=3x+4+0.5randn(1,100); plot(x,y,’x’) and plot the line superimposed to the training data. 4.8. LPC analysis using the autocorrelation method. Assume we want to calculate a LPC filter of order P = 3 for the signal x[n] = (10,8,3,−2,−4,−2,1,2) using the autocorrelation method. The 3-rd order predictor is given by the difference equation \n\n\n y~[n] = a1x[n − 1] + a2x[n − 2] + a3x[n − 3].  \n\nBy convention, the LPC filter is given (in the Z-domain) as \n\n\n A(z) = 1 − a1z−1 + a 2z−2 + a 3z−3  \n\nand you can get it using lpc(x,3) in Matlab. a) Try to prove that the coefficients ai can be\nfound by solving \n\n\n  [  R(0)  R(1)  R(2)    R(1)  R(0)  R(1)    R(2)  R(1)  R(0)   ]  [  a1    a2    a3   ] =  [  R(1)    R(2)    R(3)   ]  \n\nwhere R(m) is the correlation at lag m. b) One can note that the matrix in the equation above is symmetric and, besides, has equal elements along the diagonals. This is called a Toeplitz matrix. The matrix can be inverted with the Levinson-Durbin algorithm below, that is O(P2) instead of a generic inversion method that is O(P3).\n\n\n\nListing 4.36: MatlabOctaveCodeSnippets/snip_frequency_Levinson_Durbin.m\n\n\nfunction [a,k,E]=snip_frequency_Levinson_Durbin(R,p) \n% function [a,k,E]=snip_frequency_Levinson_Durbin(R,p) \n%Usage: x=randn(1,100)+linspace(1,5,100); p=10; R=xcorr(x,p); \n%       R=R(p+1:end); %keep only values of interest: R(0)...R(p) \n5%       [a,k,E]=snip_frequency_Levinson_Durbin(R,p) \n%Inputs: \n% p - order of LPC analysis \n% R - p+1 sample correlation function values, R(0)...R(p) \n%Outputs: \n10% a - p LPC coefficients, from a(1) to a(p) \n% k - p reflection coefficients, from k(1) to k(p) \n% E - E energies of error, from E(1) to E(p) \n%Initialization \nk(1)=R(1+1)/R(0+1); a(1)=k(1); E(1)=R(0+1)*(1-k(1)^2); \n15%Recursion \nfor i=2:p \n    k(i)=(R(i+1)-sum(a(1:i-1).*R(i:-1:2)))/E(i-1); \n    a(i)=k(i); \n    a(1:i-1)=a(1:i-1)-k(i)*a(i-1:-1:1); \n20    E(i)=E(i-1)*(1-k(i)^2); \nend\n\n\n\nThe code above has errors, but it should give the same result as the function levinson in Matlab. For example, levinson([202 114 -2 -68],3) gives the right result, but my_durbin([202 114 -2 -68],3) does not. Unfortunately, levinson is a Mex (compiled) file and one cannot see the code. Consult a reference about LPC and fix the code above."
  },
  {
    "objectID": "ak_dsp_bookap2.html",
    "href": "ak_dsp_bookap2.html",
    "title": "Appendix B — Useful Mathematics",
    "section": "",
    "text": "[next] [prev] [prev-tail] [tail] [up]\n\n\n\nB.  Useful Mathematics\n\n\nB.1  Euler’s equation  B.2  Trigonometry  B.3  Manipulating complex numbers and rational functions  B.4  Manipulating complex exponentials  B.5  Q function  B.6  Matched filter and Cauchy-Schwarz’s inequality  B.7  Geometric series  B.8  Sum of squares  B.9  Summations and integrals  B.10  Partial fraction decomposition  B.11  Calculus  B.12  Sinc Function  B.13  Rectangular Integration to Define Normalization Factors for Functions  B.13.1  Two normalizations for the histogram  B.13.2  Two normalizations for power distribution using FFT  B.14  Linear Algebra  B.14.1  Inner products and norms  B.14.2  Projection of a vector using inner product  B.14.3  Orthogonal basis allows inner products to transform signals  B.14.4  Moore-Penrose pseudoinverse  B.15  Gram-Schmidt orthonormalization procedure  B.16  Principal component analysis (PCA)  B.17  Fourier Analysis: Properties  B.18  Fourier Analysis: Pairs  B.19  Probability and Stochastic Processes  B.19.1  Joint and Conditional probability  B.19.2  Random variables  B.19.3  Expected value  B.19.4  Orthogonal versus uncorrelated  B.19.5  PDF of a sum of two independent random variables  B.20  Stochastic Processes  B.20.1  Cyclostationary random processes  B.20.2  Two cyclostationary signals: sampled and discrete-time upsampled  B.20.3  Converting a WSC into WSS by randomizing the phase  B.21  Estimation Theory  B.21.1  Probabilistic estimation theory  B.21.2  Minimum mean square error (MMSE) estimators  B.21.3  Orthogonality principle\n B.22  One-dimensional linear prediction over time  B.22.1  The innovations process  B.23  Vector prediction exploring spatial correlation  B.24  Decibel (dB) and Related Definitions  B.25  Insertion loss and insertion frequency response  B.26  Discrete and Continuous-Time Impulses  B.26.1  Discrete-time impulse function  B.26.2  Why defining the continuous-time impulse? Some motivation  B.26.3  Definition of the continuous-time impulse as a limit  B.26.4  Continuous-time impulse is a distribution, not a function  B.26.5  Mathematical properties of the continuous-time impulse  B.26.6  Convolution with an impulse  B.26.7  Applications of the impulse  B.27  System Properties  B.27.1  Linearity (additivity and homogeneity)  B.27.2  Time-invariance (or shift-invariance)  B.27.3  Memory  B.27.4  Causality  B.27.5  Invertibility  B.27.6  Stability  B.27.7  Properties of Linear and time-invariant (LTI) systems  B.28  Fixed and Floating-Point Number Representations  B.28.1  Representing numbers in fixed-point  B.28.2  IEEE 754 floating-point standard\n\n\nB.1  Euler’s equation\n\n\n\n\n\n\n\n\n\n ejx = cos ⁡ (x) + jsin ⁡ (x), \n\n\n(B.1)\n\n\n\n\n\n\nwhere x is given in radians. When x = π, it leads to the famous identity ejπ = −1. The value ejx\ncan be interpreted as a complex number with magnitude one and angle x rad. Hence, Eq. (B.1) represents the conversion of this complex number from the polar to the Cartesian form cos ⁡ (x) + jsin ⁡ (x).\n\n\n\nUsing the fact that cosine and sine are even and odd functions, respectively, one can write e−jx = cos ⁡ (x) − jsin ⁡ (x) and using Eq. (B.1) obtain\n\n\n\n\n\n\n\n\n\n cos ⁡ (x) = 1 2(ejx + e−jx) \n\n\n(B.2)\n\n\n\n\n\n\nand\n\n\n\n\n\n\n\n\n\n sin ⁡ (x) = 1 2j(ejx − e−jx). \n\n\n(B.3)\n\n\n\n\n\n\n\n\nB.2  Trigonometry\n\n\n\n\n\n\n\n\n\n sin ⁡ (a + b) = sin ⁡ acos ⁡ b + cos ⁡ asin ⁡ b. \n\n\n(B.4)\n\n\n\n\n\n\n\n\n\n\n\n sin ⁡ (2a) = 2sin ⁡ acos ⁡ a. \n\n\n(B.5)\n\n\n\n\n\n\n\n\n\n\n\n cos ⁡ (a + b) = cos ⁡ acos ⁡ b − sin ⁡ asin ⁡ b. \n\n\n(B.6)\n\n\n\n\n\n\n\n\n\n\n\n cos ⁡ (2a) = cos ⁡ 2a − sin ⁡ 2a. \n\n\n(B.7)\n\n\n\n\n\n\nFrom B.7 and cos ⁡ 2a + sin ⁡ 2a = 1:\n\n\n\n\n\n\n\n\n\n cos ⁡ 2a = 1 2(1 + cos ⁡ (2a)) \n\n\n(B.8)\n\n\n\n\n\n\nand\n\n\n\n\n\n\n\n\n\n sin ⁡ 2a = 1 2(1 − cos ⁡ (2a)). \n\n\n(B.9)\n\n\n\n\n\n\nThe following are some of the so-called product to sum identities:\n\n\n\n\n\n\n\n\n\n cos ⁡ (a)cos ⁡ (b) = 1 2[cos ⁡ (a − b) + cos ⁡ (a + b)]. \n\n\n(B.10)\n\n\n\n\n\n\n\n\n\n\n\n sin ⁡ (a)sin ⁡ (b) = 1 2[cos ⁡ (a − b) − cos ⁡ (a + b)]. \n\n\n(B.11)\n\n\n\n\n\n\n\n\n\n\n\n sin ⁡ (a)cos ⁡ (b) = 1 2[sin ⁡ (a + b) + sin ⁡ (a − b)], \n\n\n(B.12)\n\n\n\n\n\n\nwhere a is the argument of the sine in Eq. (B.12).\n\n\n\n\n\nB.3  Manipulating complex numbers and rational functions\n\n\n\nThe complex-conjugate of a sum of two complex numbers is the sum of the conjugate of these numbers. For instance:\n\n\n\n\n\n\n\n\n\n [(a + bj) + (c + dj)]∗ = (a + bj)∗ + (c + dj)∗ = (a + b) + j(c + d)∗. \n\n\n(B.13)\n\n\n\n\n\n\nThis is also valid for the conjugate of a difference, product, or quotient of two numbers, which is the difference, product, or quotient, respectively, of their individual conjugates.\n\n\n\nThis is useful when manipulating a rational system function H(z) to obtain H∗(z), as required in Eq. (4.55). For instance, suppose H(z) = (z − 2ej4)(z − 3 + j5)∕(z + 6e−j4), then H∗(z) = (z∗− 2e−j4)(z∗− 3 − j5)∕(z∗ + 6ej4).\n\n\n\n\n\nB.4  Manipulating complex exponentials\n\n\n\nIn Fourier and Z transforms it is common to encounter expressions such as 1 − e−j𝜃. In some cases it is convenient to rewrite them as\n\n\n\n\n\n\n\n\n\n 1 − e−j𝜃 = ej𝜃∕2 ej𝜃∕2(1 − e−j𝜃) = ej𝜃∕2 − e−j𝜃∕2 ej𝜃∕2 = 2je−j𝜃∕2 sin ⁡ (𝜃∕2). \n\n\n(B.14)\n\n\n\n\n\n\nSimilarly, one can write\n\n\n\n\n\n\n\n\n\n 1 + e−j𝜃 = ej𝜃∕2 ej𝜃∕2(1 + e−j𝜃) = ej𝜃∕2 + e−j𝜃∕2 ej𝜃∕2 = 2e−j𝜃∕2 cos ⁡ (𝜃∕2). \n\n\n(B.15)\n\n\n\n\n\n\n\n\nB.5  Q function\n\n\n\n\n\nOne just needs to know Q(x) for positive x because\n\n\n\n\n\n\n\n\n\n Q(−x) = 1 − Q(x) \n\n\n(B.16)\n\n\n\n\n\n\n\n\nWhen expressed in dB, it is used 20log ⁡ 10(x).\n\n\n\n\n\n\nQ(x) = 0.5 erfc(x∕2), where erfc is the complimentary error function.\n\n\n\n\n\nMatlab provides the qfunc in the comm toolbox. In case this toolbox is not available or using Octave, it is possible to use the erfc function as follows: y = 0.5*erfc(x/sqrt(2)). See ak_qfunc.m and ak_qfuncinv.m.\n\n\n\n\n\nThe Q values for three different ranges of its argument are shown in Figure B.1(a), Figure B.1(b) and Figure B.1(c).\n\n\n\n\n\n\n\n\n\n\n\n(a) Very low SNR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b) Low SNR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(c) High SNR\n\n\n\n\n\n\n\nFigure B.1: Q function for three different SNR ranges.\n\n\n\n\n\nA Q function approximation that is good for x &gt; 3:\n\n\n\n\n\n\n\n\n\n Q(x) ≈ 1 x2πexp ⁡   (−x2 2 ) \n\n\n(B.17)\n\n\n\n\n\n\nAn accurate approximation (less than 1.2% of error) is:\n\n\n\n\n\n\n\n\n\n Q(x) ≈  [ 1 π−1 π x + 1 πx2   + 2π ] 1 2πexp ⁡   (−x2 2 ). \n\n\n(B.18)\n\n\n\n\n\n\nThe expression is valid for x ≥ 0 and for x &lt; 0 one should use Q(−x) = 1 − Q(x). The function ak_qfuncApprox implements Eq. (B.18). In case you have Matlab, the code below compares it with Matlab’s qfunc.\n\n\nx=-3:0.01:3; %define a range for x \nqx1=ak_qfuncApprox(x); %use the approximation \nqx2=qfunc(x); %qfunc in Matlab's Communication Toolbox \nplot(x,100*(qx1-qx2)./qx2); %get error in % \n5grid, xlabel('x'); ylabel('Error in Q approximation (%)')\n\nB.6  Matched filter and Cauchy-Schwarz’s inequality\n\n\n\nProving the matched filter is analogous to the following problem: given a vector x, what is the vector y that maximizes the inner product ⟨x,y⟩?\nThe Cauchy-Schwarz’s inequality states that:\n\n\n\n\n\n\n\n\n\n |⟨x,y⟩|2 ≤⟨x,x⟩⟨y,y⟩. \n\n\n(B.19)\n\n\n\n\n\n\nBecause ⟨x,x⟩ = ∥x∥2, taking the square root of Eq. (B.19), leads to |⟨x,y⟩|≤∥x∥ ∥y∥. Hence, the inner product ⟨x,y⟩ = ∥x∥∥y∥cos ⁡ (𝜃) assumes its maximum value ∥x∥ ∥y∥ when 𝜃 = 0 or 180 degrees, which corresponds to the vector y and x being colinear.\n\n\n\n\n\nB.7  Geometric series\n\n\n\nA geometric series is the sum of numbers that form a geometric progression with common ration r and scale factor (or starting value) α:\n\n\n\n\n\n\n ∑ n=0N−1αrn = αr0 + αr1 + αr2 + … + αrN−1 = α(1 − rN) 1 − r . \n\n\n\n\n\n\nWhen the number N of terms goes to infinity, the series converges if and only if |r| &lt; 1. In this case\n\n\n\n\n\n\n\n\n\n ∑ n=0∞αrn = α 1 − r,|r| &lt; 1. \n\n\n(B.20)\n\n\n\n\n\n\n\n\nB.8  Sum of squares\n\n\n\nThe sum of the squares of the first N integers is\n\n\n\n\n\n\n\n\n\n ∑ n=1Nn2 = N(N + 1)(2N + 1) 6 , \n\n\n(B.21)\n\n\n\n\n\n\nwhich can be proved by induction [urlBMson].\n\n\n\n\n\nB.9  Summations and integrals\n\n\n\nNote that\n\n\n\n\n\n\n  (∑ n=1Nx[n])  (∑ n=1Ny[n]) = ∑ n=1N ∑ m=1N  (x[n]y[m]) \n\n\n\n\n\n\nbecause, e.g., (a + b + c)(d + e + f) = ad + ae + af + bd + be + bf + cd + ce + cf. Similarly, in the continuous-case\n\n\n\n\n\n\n  (∫ ⟨T0⟩x(t)dt)  (∫ ⟨T0⟩y(t)dt) = ∫ ⟨T0⟩∫ ⟨T0⟩  (x(t)y(s))dtds, \n\n\n\n\n\n\nwhere one should note the adoption of distinct integration variables t and s. This result allows to express\n\n\n\n\n\n\n\n\n\n  (∫ ⟨T0⟩x(t)dt)2 = ∫ ⟨T0⟩∫ ⟨T0⟩  (x(t)x(s))dtds, \n\n\n(B.22)\n\n\n\n\n\n\nwhich is an useful expression. Note that, in general,\n\n\n\n\n\n\n  (∫ ⟨T0⟩x(t)dt)2≠∫ ⟨T0⟩x2(t)dt. \n\n\n\n\n\n\n\n\nB.10  Partial fraction decomposition\n\n\n\nA partial fraction decomposition is used to convert a rational function P(x)∕Q(x) into a sum of simpler fractions, where P(x) and Q(x) are two polynomials with N and M being the degrees of P(x) and Q(x), respectively and x ∈ ℂ.\n\n\n\nTwo assumptions simplify the decomposition:\n\n\n\nf 1.\n\n\n\nM &gt; N, i. e., the denominator has larger degree than the numerator,\n\n\nf 1.\n\n\nall M roots pi of the denominator (called poles when dealing with transforms such as Laplace and Z) are distinct, which allows to write Q(x) = (x − p1)(x − p2)…(x − pM).\n\n\n\n\nIn this special case, it is possible to write\n\n\n\n\n\n\n\n\n\n P(x) Q(x) = r1 x − p1 + r2 x − p2 + … + r1 x − pM, \n\n\n(B.23)\n\n\n\n\n\n\nwhere\n\n\n\n\n\n\n\n\n\n ri =   (P(x) Q(x)(x − pi))|x=pi \n\n\n(B.24)\n\n\n\n\n\n\nis called the residue of the (pole) pi. To understand (and prove) Eq. (B.24), one can observe that multiplying both sides of Eq. (B.23) by (x − p1) leads to\n\n\n\n\n\n\n P(x) Q(x)(x − p1) = r1 + r2(x − p1) x − p2 + … + r1(x − p1) x − pM \n\n\n\n\n\n\nand substituting x by p1 makes all terms ri(x − p1)∕(x − pi),i≠1, equal to zero. The same can be done to the other poles and the general expression for this procedure is Eq. (B.24). For example, expanding 1∕(x2 − 5x + 6) leads to\n\n\n\n\n\n\n 1 x2 − 5x + 6 = r1 x − 2 + r2 x − 3, \n\n\n\n\n\n\nwhere\n\n\n\n\n\n\n r1 =   ( 1 (x − 2)(x − 3)(x − 2))|x=2 =   1 x − 3 |x=2 = −1 \n\n\n\n\n\n\nand\n\n\n\n\n\n\n r2 =   ( 1 (x − 2)(x − 3)(x − 3))|x=3 = 1. \n\n\n\n\n\n\nIf the roots are complex (typically they occur as complex conjugate pairs), the procedure is similar, but the parcels can be rearranged.\n\n\n\nWhen the first assumption is not valid, one needs to use polynomial division to first obtain\n\n\n\n\n\n\n P(x) Q(x) = R(x) + S(x) Q(x), \n\n\n\n\n\n\nwhere the degree L of S(x) is L &lt; M. This pre-processing stage is similar to writing an improper fraction as a mixed fraction, e. g., 7 4 = 13 4 = 1 + 3 4. For example, when P(x)∕Q(x) = (3x3 − 13x2 + 8x + 13)∕(x2 − 5x + 6), some algebra shows that it is not possible to find two residues r1 and r2 such that\n\n\n\n\n\n\n P(x) Q(x) = r1 x − 2 + r2 x − 3. \n\n\n\n\n\n\nHence, first one obtains\n\n\n\n\n\n\n P(x) Q(x) = 3x3 − 13x2 + 8x + 13 x2 − 5x + 6 = 3x + 2 + 1 x2 − 5x + 6, \n\n\n\n\n\n\nwith R(x) = 3x + 2 and S(x) = 1 having a degree L = 0 smaller than M = 2, and then uses the standard partial fraction expansion on S(x) Q(x) to obtain\n\n\n\n\n\n\n 3x3 − 13x2 + 8x + 13 x2 − 5x + 6 = 3x + 2 + 1 x − 3 − 1 x − 2. \n\n\n\n\n\n\nWhen one or more roots pi of Q(x) (poles) have multiplicity λi larger than one, the second simplifying assumption does not hold and the expansion is trickier as discussed in the sequel.\n\n\n\nNote that, in general, Q(x) can be written as Q(x) = (x − p1)λ1(x − p2)λ2…(x − pM)λM, while the previous results were restricted to λi = 1,∀ ⁡i. A pole pi with λi &gt; 1 requires not only a parcel ri∕(x − pi) but λi parcels with residues rij,j = 1,…,λi for the following powers of (x − pi):\n\n\n\n\n\n\n ri1 (x − pi) + ri2 (x − pi)2 + ri3 (x − pi)3 + … + riλi (x − pi)λi. \n\n\n\n\n\n\nThe residues can be obtained using factorial and derivatives via the Theorem of residuals:\n\n\n\n\n\n\n\n\n\n rij = 1 (λi − j)!   dλi−j dxλi−j  (P(x) Q(x)(x − pi)λi )| x=pi \n\n\n(B.25)\n\n\n\n\n\n\nfor j = 1,…,λi. When λi = 1, this equation simplifies to Eq. (B.24). For example, the expansion of (x3 + 5)∕(x4 − 9x3 + 30x2 − 44x + 24) can use Eq. (B.25) because the denominator can be written as (x − 2)3(x − 3), having a single pole p1 = 3 and a pole p2 = 2 with multiplicity 3. Hence, the following residues need to be found\n\n\n\n\n\n\n x3 + 5 x4 − 9x3 + 30x2 − 44x + 24 = r1 (x − p1) + r21 (x − p2) + r22 (x − p2)2 + r23 (x − p2)3. \n\n\n\n\n\n\nThe residue r1 can be found with Eq. (B.24):\n\n\n\n\n\n\n r1 =   ( x3 + 5 (x − 2)3(x − 3)(x − 3))|x=3 = 33 + 5 (3 − 2)3 = 32, \n\n\n\n\n\n\nwhile the other residues are given by Eq. (B.25) and require using Eq. (B.26) to obtain the following derivatives:\n\n\n\n\n\n\n d dx  (x3 + 5 x − 3 ) = 3x2(x − 3) − (x3 + 5) (x − 3)2 = 2x3 − 9x2 − 5 (x − 3)2 \n\n\n\n\n\n\nand\n\n\n\n\n\n\n d dx  (2x3 − 9x2 − 5 (x − 3)2 ) = (6x2 − 18x)(x − 3)2 − (2x3 − 9x2 − 5)2(x − 3)) (x − 3)4 , \n\n\n\n\n\n\nwhich will be used for calculating r22 and r21, respectively. Therefore,\n\n\n\n\n\n\n r21 = 1 (3 − 1)!   d2 dx2  (P(x) Q(x)(x − 2)3)|  x=2 = 1 2(−62) = −31, \n\n\n\n\n\n\n\n\n r22 = 1 (3 − 2)!   d dx  (P(x) Q(x)(x − 2)3)|  x=2 = −25 \n\n\n\n\n\n\nand\n\n\n\n\n\n\n r23 = 1 (3 − 3)!   (P(x) Q(x)(x − 2)3)|  x=2 = −13. \n\n\n\n\n\n\nIt is useful to use algebra and double check the obtained expansion:\n\n\n\n\n\n\n x3 + 5 x4 − 9x3 + 30x2 − 44x + 24 = 32 (x − 3) +  − 31 (x − 2) +  − 25 (x − 2)2 +  − 13 (x − 2)3. \n\n\n\n\n\n\nAlternatively, one can use Matlab/Octave to obtain the residues with the commands b=[1 0 0 5],a=[1 -9 30 -44 24],[r,p,k]=residue(b,a). It should be noted that Octave has the option of a more complete output with [r,p,k,e]=residue(b,a), where the vector e relates each residue to the corresponding parcel in the expansion. When using Matlab, one needs to know that the residues are given in the order ri1,ri2,…,riλi.\n\n\n\n\n\nB.11  Calculus\n\n\n\n\n\n\nf 1.\n\n\nDerivative product rule: (f(x)g(x)) ′ = f(x)g ′(x) + f ′(x)g(x)\n\n\nf 2.\n\n\nDerivative of a rational function\n\n\n\n\n\n\n\n\n  (f(x) g(x) ) ′ = f ′(x)g(x) − f(x)g ′(x) g2(x) \n\n\n(B.26)\n\n\n\n\n &lt;/dd&gt;&lt;dt class=\"enumerate-enumitem\"&gt;&lt;span \nclass=“ec-lmbx-10x-x-109”&gt;f 3.\n\n\nIntegration by parts:\n\n\n\n\n\n\n\n\n ∫ f(x)g ′(x)dx = f(x)g(x) −∫ f ′(x)g(x)dx \n\n\n(B.27)\n\n\n\n\n &lt;/dd&gt;&lt;dt class=\"enumerate-enumitem\"&gt;&lt;span \nclass=“ec-lmbx-10x-x-109”&gt;f 4.\n\n\nDerivative of an exponential: (ef(t)) ′ = ef(t)f ′(t)\n\n\nf 5.\n\n\nIntegral of an exponential: ∫ (ef(t))dt = ef(t) f ′(t)\n\n\n\n\n\n\nB.12  Sinc Function\n\n\n\nOur definition of sinc is:\n\n\n\n\n\n\n sinc(x) = sin ⁡ (πx) πx . \n\n\n\n\n\n\nSome authors call it Sa (sample function) and others do not include π in the definition. Its first zero occurs when x = 1. Its value sinc(0) = 1 at origin can be determined using L’Hospital rule. The sinc is an energy signal with unitary energy E = 1, which can be determined by its Fourier transform and Parseval’s relation. Its scaled version sinc(t∕Ts) is widely used in sampling theory and has energy E = Ts. As discussed in Example 1.5, sinc((t − 5)∕3) corresponds to expanding sinc(t) by a factor of 3 and then delaying this intermediate result by 5.\n\n\n\nThe sincs are orthogonal when shifted by integers m,n ∈ ℤ (e. g., sinc(t − 3) and sinc(t+1) are orthogonal) and, consequently, the scaled sincs sinc(t∕Ts) are orthogonal when shifted by multiples of Ts, i. e.\n\n\n\n\n\n\n\n\n\n ∫ −∞∞sinc  (t − mTs Ts ) sinc  (t − nTs Ts ) dt = Tsδ[m − n]. \n\n\n(B.28)\n\n\n\n\n\n\n\n\nB.13  Rectangular Integration to Define Normalization Factors for Functions\n\n\n\nIn several situations a computer is used to obtain points that should represent a continuous function f(x), x ∈ ℝ. Two examples of this situation are the estimation of probability density functions (PDF) via histograms and power spectral density (PSD) estimation via an FFT routine.\n\n\n\nInstead of aiming at an analytical expression f^(x) to represent f(x), the task consists in obtaining a set of points {f^(x[n])} calculated at the values x[n], n = 0,…,N − 1, which are a uniformly-sampled version of the abscissa x.\n\n\n\nOften it is possible to first obtain a set of values {ĝ(x[n])} in which the value ĝ(x[n]) is proportional to f(x[n]), i. e., f(x[n]) ∝ĝ(x[n]). In this case, it is required to later determine a scaling factor κ ∈ ℝ such that the final set of values to represent f(x) is obtained via\n\n\n\n\n\n\n\n\n\n f^(x[n]) = κĝ(x[n]). \n\n\n(B.29)\n\n\n\n\n\n\nNote that the goal is not necessarily to have f^(x) ≈ f(x). There are situations in which the set of points {f^(x[n])} must obey a property. For example, when histograms are used to estimate probability mass functions, one desired property is that ∑ ⁡ x[n]f^(x[n]) = 1.\nAlternatively, the goal may be to scale the histogram such that the two resulting curves (normalized histogram and probability density function) coincide. The values of κ are different for these two possible cases of histogram normalization as discussed after recalling the rectangle method.\n\n\n\nThe rectangle method [urlBMrec] is used for approximating a definite integral:\n\n\n\n\n\n\n\n\n\n ∫ abf(x)dx ≈ h∑ n=0N−1f(x[n]), \n\n\n(B.30)\n\n\n\n\n\n\nwhere h = (b − a)∕N is the rectangle width and x[n] = a + nh.\n\n\n\nThe rectangle method can be used, for instance, to relate the continuous-time convolution in Eq. (3.4) with its discrete-time counterpart in Eq. (3.3). Assuming Ts is the sampling interval used to obtain the discrete-time signals x[n] and h[n] from x(t) and h(t), respectively, the factor Ts is required to better approximate the samples of y(t) = x(t)∗h(t) when using a discrete-time convolution:\n\n\n\n\n\n\n\n\n\n y(nTs) ≈ Ts(x[n]∗h[n]). \n\n\n(B.31)\n\n\n\n\n\n\nBesides, rectangle integration is useful to calculate the scaling factor κ in the two cases discussed in next section.\n\n\n\n\n\nB.13.1  Two normalizations for the histogram\n\n\n\nWhen the task is to estimate the PDF f(x) of a continuous random variable, one can try using a discrete histogram ĝ(x[n]), which is obtained by drawing M values from f(x) and counting the number of values occurring at each of B bins. Intuitively, for large M and B, the curve (or the “envelope”) of the histogram resembles f(x) but it is off by a normalization factor κ.\n\n\n\nIf κ = 1∕M is chosen, which is the most adopted option, one has f^(x[n]) = (1∕M)ĝ(x[n]) and, consequently ∑ ⁡ nf^(x[n]) = 1. However, in this case, f^(x[n]) may be far from f(x) by a large scaling factor. This can be observed in the curves generated by the following code:\n\n\nM=1000; x=3*rand(1,M); %M random numbers from 0 to 3 \nB=100; [hatgx,N_x]=hist(x,B); %histogram with B bins \nhatfx = hatgx/M; %normalize the histogram to sum up to 1 \nplot(N_x,hatfx,[-1,0,0,3,3,4],[0,0,1/3,1/3,0,0],'o-') \n5xlabel('random variable x'), ylabel('PDF f(x)') \nlegend('estimated','theoretical'); sum(hatfx)\n\n\nThe result of sum(hatfx) is equal to one, as specified, but the PDF of the simulated distribution U(0,3) is 1/3 over its support and the superimposed estimated and theoretical graphs do not match. This discrepancy between the curves should be expected given that the normalized histogram hatfx was in fact an estimate of a probability mass function (PMF) of a discrete random variable, obtained by quantizing the original x. Another normalization factor κ≠1∕M must be used if the goal is to have f(x) ≈f^(x[n]).\n\n\n\nTo obtain κ such that f(x) ≈f^(x[n]), one can use the property that the integral of a PDF is one. Based on the rectangle method one can write\n\n\n\n\n\n\n ∫ abf(x)dx ≈ h∑ n=0N−1f^(x[n]) = hκ∑ n=0N−1ĝ(x[n]) = 1, \n\n\n\n\n\n\nwhere h = (b − a)∕B. Because ∑ ⁡ n=0N−1ĝ(x[n]) = M, one obtains κ = 1∕(hM), which is the original factor 1∕M divided by the bin width h. The function ak_normalize_histogram.m uses this approach. Using the same example of the previous code, the following commands for obtaining hatfx would lead to consistent theoretical and estimated curves:\n\n\nM=1000; x=3*rand(1,M); %M random numbers from 0 to 3 \nB=100; [hatgx,N_x]=hist(x,B); %histogram with B bins \nh=3/B; %h is the bin width assuming the support is 3 \nhatfx = hatgx/(M*h); %PDF values via normalized histogram \n5plot(N_x,hatfx,[-1,0,0,3,3,4],[0,0,1/3,1/3,0,0],'o-') \nxlabel('random variable x'), ylabel('PDF f(x)') \nlegend('estimated','theoretical'); sum(hatfx)\n\n\nAs expected, in contrast to the sum equal to one in the first code, in this case sum(hatfx)=1/h=33.3. Both histogram normalization factors, κ = 1∕M and κ = 1∕(hM), are useful and the choice depends whether the application requires values from a PMF or PDF, respectively.\n\n\n\n\n\nB.13.2  Two normalizations for power distribution using FFT\n\n\n\nAnother application that can be related to Eq. (B.21) is the use of FFT for estimating how the signal power is distributed over frequency. It is assumed here a finite-duration discrete-time signal x[n] with N non-zero samples.\n\n\n\nThe squared FFT magnitude |FFT{x[n]}|2 plays the role of the function ĝ in Eq. (B.29). The choice κ = 1∕N2 leads to an estimate f^(⋅) of the mean-square spectrum (MSS) Sms[k] of Eq. (4.32), while κ = 1∕(N2Δf) corresponds to PSD S(f) in Eq. (4.20), where Δf = BW∕N\nand BW is given in Hz. As indicated in Table B.1, the two options for κ have similarities with the ones for histogram normalization.\n\n\n\n\n\n\n\nTable B.1: Analogy between using the histogram and DFT for estimation, where ĝ(x[n]) is the estimated function and f^(x[n]) = κĝ(x[n]) its normalized version. The unit of f^(x[n]) is indicated within parentheses.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nĝ(⋅) is histogram \n\n\n\nĝ(⋅) is |FFT{x[n]}|2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate a discrete function\n\n\n\n\nκ = 1∕M\n\n\n\nκ = 1∕N2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nf^(⋅) is PMF (probability) \n\n\n\nf^(⋅) is MSS Ŝms[k] (Watts) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEstimate a continuous function\n\n\n\n\nκ = 1 hM\n\n\n\nκ = 1 N2Δf\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nf^(⋅) is PDF (likelihood) \n\n\n\nf^(⋅) is PSD Ŝ(f) (Watts/Hz) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                                                                                                                                                                                                                  &lt;/div&gt;\n                                                                          \n                                                                          \n\n\n\nIn both cases in Table B.1, when going from a discrete to a continuous function, the bin width (h for histogram and Δf for the FFT), is used as normalization factor.\n\n\nB.14  Linear Algebra\n\n\n\n\n\nB.14.1  Inner products and norms\n\n\n\nThe inner product in a K-dimensional space with complex-valued vectors is:\n\n\n\n\n\n\n\n\n\n ⟨a,b⟩ = ∑ i=1Ka ibi∗ = ∥a∥ ∥b∥cos ⁡ (𝜃). \n\n\n(B.32)\n\n\n\n\n\n\nSee Table 2.2 for alternative definitions of inner products.\n\n\n\nWhen a = b, Eq. (B.32) can be written as\n\n\n\n\n\n\n\n\n\n ∥a∥ = ⟨a , a ⟩. \n\n\n(B.33)\n\n\n\n\n\n\nAn inner product ⟨a,b⟩ can also be written as a multiplication of two vectors\n\n\n\n\n\n\n\n\n\n ⟨a,b⟩ = bHa, \n\n\n(B.34)\n\n\n\n\n\n\nwhere in this case both are assumed to be column vectors (row vectors would suggest abH).\n\n\n\nIn case a vector b = Aa is obtained via multiplication by a unitary matrix A, Eq. (B.33) and Eq. (B.34) lead to\n\n\n\n\n\n\n\n\n\n ∥b∥ = ⟨A a , A a ⟩ = (A a )H   A a = a H   A H   A a = a H   I a = ∥a∥ \n\n\n(B.35)\n\n\n\n\n\n\nbecause AHA = I, which indicates that the unitary A does not alter the norm of the input vector.\n\n\n\n\n\nB.14.2  Projection of a vector using inner product\n\n\n\nTo explore the properties and advantages of linear transforms, it is useful to study the vector projection (or simply projection) of a vector onto another one.\n\n\n\n\n\n\n\n\nFigure B.2: The perpendicular line for obtaining the projection pxy of a vector x onto y in ℝ2. Note that 𝜃 = cos ⁡ −1(⟨x,y⟩∕(∥x∥∥y∥) is the angle between x and y and the inner product ⟨x,y⟩ = ∥pxy∥∥y∥ = ∥pyx∥∥x∥.\n\n\n\n\n\nUsing ℝ2 for simplicity, note that the projection pxy of a vector x in another vector y is obtained by choosing the point along the direction of y that has the minimum distance to x. This line is perpendicular to y, as indicated in Figure B.2. Using the Pythagorean theorem and assuming that 0 ≤ 𝜃 ≤ π∕2, the norm ∥pxy∥ of the projection can be written as ∥pxy∥ = ∥x∥cos ⁡ (𝜃). If π∕2 &lt; 𝜃 ≤ π, ∥pxy∥ = ∥x∥cos ⁡ (π − 𝜃) = −∥x∥cos ⁡ (𝜃). Hence, in general,\n\n\n\n\n\n\n\n\n\n ∥pxy∥ = ∥x∥ |cos ⁡ (𝜃)| = |⟨x,y⟩| ∥y∥ . \n\n\n(B.36)\n\n\n\n\n\n\nFor a given norm ∥y∥, the larger the inner product, the larger the norm of the projection. The same is valid for pyx as depicted in Figure B.3:\n\n\n\n\n\n\n ∥pyx∥ = ∥y∥ |cos ⁡ (𝜃)| = |⟨x,y⟩| ∥x∥ . \n\n\n\n\n\n\n\n\n\n\n\nFigure B.3: Projections of a vector x and y onto each other. Note the errors are orthogonal to the directions of the respective projections.\n\n\n\n\n\nAny vector z can be written as its norm ∥z∥ multiplied by a unity norm vector  z∥z ∥ that indicates its direction. Note that the vector pyx is in the same or the opposite direction of x, which can be specified by the unity norm vector sign(cos ⁡ (𝜃)) x∥x ∥. Hence, one has\n\n\n\n\n\n\n pyx = ∥pyx∥sign(cos ⁡ (𝜃)) x ∥x∥ =  (⟨x,y⟩ ∥x∥2 ) x, \n\n\n\n\n\n\nwhere ⟨x,y⟩ ||x||2  is a scaling factor that can be negative but does not change the direction of x. Similarly, the projection pxy of x onto y is given by\n\n\n\n\n\n\n pxy =  (⟨x,y⟩ ∥y∥2 ) y. \n\n\n\n\n\n\nNote that if the vector y has unitary norm, the absolute value of the inner product ⟨x,y⟩ coincides with\nthe norm ∥pxy∥. These expressions are valid for other vector spaces, such as ℝn, n &gt; 2.\n\n\n\nUsing geometry to interpret a projection vector is very useful. When one projects x in y, the result pxy is the “best” representation (in the minimum distance sense, or least-square sense) of x that y alone can provide. The error vector exy = x −pxy is orthogonal to y (and, consequently, to pxy), i. e., ⟨exy,y⟩ = 0. The vector exy represents what should be added to pxy in order to completely represent x, and the orthogonality ⟨exy,y⟩ = 0 indicates that y cannot contribute any further.\n\n\n\nFigure B.3 completes the example. It was obtained using the following Matlab/Octave script.1 The example assumes x = [2,2] and y = [5,1], having an angle 𝜃 of approximately 33.7 degrees between them.\n\n\n\nListing B.1: MatlabOctaveCodeSnippets/snip_transforms_projection.m\n\n\nx=[2,2]; %define a vector x \ny=[5,1]; %define a vector y \nmagx=sqrt(sum(x.*x))%magnitude of x \nmagy=sqrt(sum(y.*y))%magnitude of y \n5innerprod=sum(x.*y) %&lt;x,y&gt;=||x|| ||y|| cos(theta) \ntheta=acos(innerprod / (magx*magy)) %angle between x and y \n%obs: could use acosd to have angle in degrees \ndisp(['Angle is ' num2str(180*theta/pi) ' degrees']); \n%check if inverting direction is needed: \n10invertDirection=1; if theta&gt;pi/2 invertDirection=-1; end \n%find the projection of x over y (called p_xy) and p_yx \nmag_p_xy = magx*abs(cos(theta)) %magnitude of p_xy \n%directions: obtained as y normalized by magy, x by magx \ny_unitary=y/magy; %normalize y by magy to get unitary vec. \n15p_xy = mag_p_xy * y_unitary* invertDirection %p_xy \nmag_p_yx = magy*abs(cos(theta)); %magnitude of p_yx \nx_unitary=x/magx; %normalize x by magx to get unitary vec. \np_yx = mag_p_yx * x_unitary* invertDirection %p_yx \n%test orthogonality of error vectors: \n20error_xy = x - p_xy; %we know: p_xy + error_xy = x \nsum(error_xy.*y) %this inner product should be zero \nerror_yx = y - p_yx; %we know: p_yx + error_yx = y \nsum(error_yx.*x) %this inner product should be zero\n  \n\n\nThe concept of projections via inner products will be extensively used in our discussion about transforms. For example, the coefficients of a Fourier series of a signal x(t) correspond to the normalized projections of x(t) on the corresponding basis functions. A large value for the norm of a projection indicates that the given basis function provides a good contribution in the task of representing x(t).\n\n\n\nChapter 2 discusses block transforms and relies on orthogonal functions. Hence, it is useful to discuss why orthogonality is important.\n\n\nB.14.3  Orthogonal basis allows inner products to transform signals\n\n\n\nAssume the existence of a set of orthogonal vectors composing the basis of a vector space. For example, in ℝ2, a convenient basis is the standard, composed by i¯ = [1,0] and j¯ = [0,1]. The inner product ⟨i¯,j¯⟩ = 0 indicates that these two vectors are orthogonal. The orthogonality property simplifies the following analysis task: given any vector x, the coefficients α and β of the linear combination x = αi¯ + βj¯, can be easily found by using the dot products α = ⟨x,i¯⟩ and β = ⟨x,j¯⟩. The following theorem proves this important result.\n\n\n\n\n  Theorem 4.  Analysis via inner products. If the basis set {b1,…,bN} of an inner product space (e. g., Euclidean) is orthonormal, the coefficients of a linear combination x = ∑ ⁡ i=1Nαibi that generates a vector x can be calculated by the inner product αi = ⟨x,bi⟩ between x and the respective vector bi in the basis set.\n\n\n\nProof: Recall the following properties of a dot product: ⟨a¯ + b¯,c¯⟩ = ⟨a¯,c¯⟩ + ⟨b¯,c¯⟩ and ⟨αa¯,b¯⟩ = α⟨a¯,b¯⟩ and write\n\n\n\n\n\n\n ⟨x,bj⟩ = ⟨∑ i=1Nα ibi,bj⟩ = ∑ i=1Nα i⟨bi,bj⟩ \n\n\n\n\n\n\nbecause the basis vectors are orthonormal, ⟨bi,bj⟩ = 1 if i = j and ⟨bi,bj⟩ = 0 if i≠j. Therefore,\n\n\n\n\n\n\n ⟨x,bj⟩ = αj. \n\n\n\n\n\n\nbecause all the terms in the above summation are zero but the one for i = j.   □\n\n\n\n\n\n\n\n Example B.1. Obtaining the coefficients of a linear combination of basis functions. A simple example can illustrate the analysis procedure: the coefficients of x = 4i¯ + 8j¯ are α = 4 and β = 8 by inspection, but they could be calculated as α = ⟨x,i¯⟩ = ⟨[4,8],[1,0]⟩ = 4 and β = ⟨x,j¯⟩ = ⟨[4,8],[0,1]⟩ = 8. Note that the zeros in these basis vectors make the calculation overly simple. Another example may be more useful to highlight orthogonality and the following alternative basis set is assumed: i¯ = [0.5,0.866] and j¯ = [0.866,−0.5]. Let x = 3i¯ + 2j¯ = [3.232,1.5980]. Given\n\nx, the task is again to find the coefficients such that x = αi¯ + βj¯. Due to the orthonormality of i¯ and j¯, one can for example obtain α = ⟨x,i¯⟩ = ⟨[3.232,1.5980],[0.5,0.866]⟩ = 3. These computations can be done in Matlab/Octave as follows.\n\n\ni=[0.5,0.866], j=[0.866 -0.5] %two orthonormal vectors \nx=3*i+2*j %create an arbitrary vector x to be analyzed \nalpha=sum(x.*i), beta=sum(x.*j) %find inner products\n\n\nIn contrast, let us modify the previous example, adopting a non orthogonal basis. Assume that i¯ = [1,1] and j¯ = [0,1] (note that ⟨i¯,j¯⟩ = 1, hence the vectors are not orthogonal). Let x = 3i¯ + 2j¯ = [3,5]. In this case, ⟨x,i¯⟩ = 8 and ⟨x,j¯⟩ = 5, which do not coincide with the coefficients α = 3 and β = 2. How could the coefficients be properly recovered in this case? An alternative is to write the problem as a set of linear equations, organize it in matrix notation and find the coefficients by inverting the matrix. In Matlab/Octave:\n\n\n\nListing B.2: MatlabOctaveCodeSnippets/snip_transforms_non_orthogonal_basis.m\n\n\ni=transpose([1,1]), j=transpose([0,1]) %non-orthogonal \nx=3*i+2*j %create an arbitrary vector x to be analyzed \nA=[i j]; %organize basis vectors as a matrix \ntemp=inv(A)*x; alpha=temp(1), beta=temp(2) %coefficients\n  \n\n\nIn summary, the analysis procedure for many linear transforms (such as Fourier, Z, etc.) obtain the coefficients via an inner product, and the procedure can be interpreted as calculating the projection of x onto basis i¯ (eventually scaled by the norm of i¯).   □\n\n\n\nThis discussion leads to the conclusion that a basis with orthogonal vectors significantly simplifies the task: in this case, the analysis procedure can be done via inner products. This applies when the basis vectors do not have unitary-norm, but in this case a normalization by their norms is needed. Orthogonal basis vectors are a property of all block transforms discussed in this text.\n\n\n\n\n\nB.14.4  Moore-Penrose pseudoinverse\n\n\n\nPseudoinverses2\nare generalizations of the inverse matrix and are useful when the given matrix does not have an inverse (for example, when the matrix is not square or full rank).\n\n\n\nThe Moore-Penrose pseudoinverse has several interesting properties and is adequate to least square problems. It provides the minimum-norm least squares solution z = Xb to the problem of finding a vector z that minimizes the error vector norm ||Xz −b||. Assuming X is an m × n matrix, the pseudoinverse provides the solution for a set of overdetermined or underdetermined equations if m &gt; n or m &lt; n, respectively.\n\n\n\nTwo properties of a Moore-Penrose pseudoinverse X+ are\n\n\n\n\n\n\n\n\n\n XH = XHXX+, \n\n\n(B.37)\n\n\n\n\n\n\nand\n\n\n\n\n\n\n\n\n\n XH = X+XXH. \n\n\n(B.38)\n\n\n\n\n\n\nWith r being the rank of X, then:\n\n &lt;ul class=\"itemize1\"&gt;\n &lt;li class=\"itemize\"&gt;\n &lt;!--l. 221--&gt;&lt;p class=\"noindent\" &gt;if &lt;!--l. 221--&gt;&lt;math \nxmlns=“http://www.w3.org/1998/Math/MathML”\ndisplay=“inline” &gt;m = n and r = m = n, the pseudoinverse X+ = X−1 is equivalent to the usual inverse;\n\n\n &lt;li class=\"itemize\"&gt;\n &lt;!--l. 222--&gt;&lt;p class=\"noindent\" &gt;if &lt;!--l. 222--&gt;&lt;math \nxmlns=“http://www.w3.org/1998/Math/MathML”\ndisplay=“inline” &gt;m &gt; n (overdetermined) and, besides, r = n (the columns of X are linearly independent), XHX is invertible and using Eq. (B.37) the pseudoinverse is given by\n\n\n\n\n\n\n\n\n\n X+ =  (XHX)−1XH; \n\n\n(B.39)\n\n\n\n\n &lt;/li&gt;\n &lt;li class=\"itemize\"&gt;\n &lt;!--l. 227--&gt;&lt;p class=\"noindent\" &gt;if &lt;!--l. 227--&gt;&lt;math \nxmlns=“http://www.w3.org/1998/Math/MathML”\ndisplay=“inline” &gt;n &gt; m (underdetermined) and r = m (the rows of X are linearly independent), XXH is invertible and using Eq. (B.38) leads to\n\n\n\n\n\n\n\n\n\n X+ = XH  (XXH) −1. \n\n\n(B.40)\n\n\n\n\n &lt;/li&gt;&lt;/ul&gt;\n\n\nWhenever available, instead of Eq. (B.39) or Eq. (B.40) that requires linear independence, one should use a robust method to obtain X+ such as the pinv function in Matlab/Octave, which adopts a SVD decomposition to calculate X+. Listing B.3 illustrates such calculations and the convenience of relying on pinv when the independence of rows or columns is not guaranteed.\n\n\n\nListing B.3: MatlabOctaveCodeSnippets/snip_systems_pseudo_inverse.m\n\n\ntest=3; %choose the case below \nswitch test \n    case 1 %m&gt;n (overdetermined/tall) and linearly indepen. columns \n        X=[1 2 3; -4+1j -5+1j -6+1j;1 0 0;0 1 0]; %4 x 3 \n5    case 2 %n&gt;m (underdetermined/fat) and linearly independent rows \n        X=[1 2 3; -4+1j -5+1j -6+1j]; %2 x 3 \n    case 3 %neither rows nor columns of X are linearly independent \n        %rows X(2,:)=2*X(1,:) and columns X(:,4)=3*X(:,1) \n        X=[1 2 3 3; 2 4 6 6; -4+1j -5+1j -6+1j -12+3*1j]; %3 x 4 \n10end \nXp_svd = pinv(X) %pseudoinverse via SVD decomposition \nXp_over = inv(X'*X)*X' %valid when columns are linearly independent \nXp_under = X'*inv(X*X') %valid when rows are linearly independent \nrank(X'*X) %X'*X is square but may not be full rank \n15rank(X*X') %X*X' is square but may not be full rank \nXhermitian=X'*X*pinv(X) %equal to X' (this property is always valid) \nXhermitian2=pinv(X)*X*X' %equal to X' (the property itself is valid) \nmaxError_over=max(abs(Xp_svd(:)-Xp_over(:))) %error for overdetermin. \nmaxError_under=max(abs(Xp_svd(:)-Xp_under(:))) %for underdetermined\n  \n\n\n\n\nB.15  Gram-Schmidt orthonormalization procedure\n\n\n\nThe Gram-Schmidt is an automatic procedure to obtain a set of N orthonormal vectors y from an input set {x1,x2,…,xM} composed by M vectors. If the M vectors are linearly independent, then N = M. If there is linear dependency among the vectors, then N &lt; M. Listing B.4 illustrates the procedure.\n\n\n\nIn summary, the first basis function is y1 = x1∕∥x1∥. Because y1 is the normalized first input vector, it can properly represent x1. The next step is to iteratively (in a loop) add new basis functions to represent the remaining vectors. For example, if x2 were colinear to y1, it would not be necessary to enlarge the basis set (y1 could represent both x1 and x2). If that is not the case, y2 cannot be simply y2 = x2∕∥x2∥ because,\neventually, y1 and y2 can be linearly dependent. Therefore, first x2 is projected in y1 and y2 is the (normalized) error vector of this projection. This process is repeated. For example, when obtaining yi to represent a vector xj, one first takes in account the projection of xj into all previously selected basis 1,2,…,i − 1. If the error is not numerically negligible (given a tolerance), this suggests that xj does not reside in the space spanned by the current set of basis functions and the normalized projection error is added to this set.\n\n\n\nListing B.4: MatlabOctaveFunctions/ak_gram_schmidt.m\n\n\nfunction [Ah,A]=ak_gram_schmidt(x,tol) \n% function [Ah,A]=ak_gram_schmidt(x,tol) \n%Gram-Schmidt orthonormalization procedure (x is real) \n%Inputs:  x - real matrix: each row is an input vector \n5%         tol - tolerance for stopping procedure (and SVD) \n%Outputs: Ah  - direct matrix, which is the Hermitian (transpose in \n%               this case) of A \n%         A   - inverse matrix with basis functions as columns \n%Example of usage: \n10% x=[1 1; 0 2; 1 0; 1 4] \n% [Ah,A]=ak_gram_schmidt(x,1e-12) %basis functions are columns of A \n% testVector=[10; 10]; %test: vector in same direction as first basis \n% testCoefficients=Ah*testVector %result is [14.1421; 0] \n \n15%% Notes about the code: \n%1) The inner product sum(y(m,:).*x(k,:)) is alternatively \n%   calculated as y(m,:)*x(k,:)' (row vectors) \n%2) In general, a projection is projectionOverBasis=p_xy= \n%             ( &lt;y,x&gt; / ||y||^2 ) * y = \n20%             ((y(m,:)*x(k,:)')/(y(m,:)*y(m,:)'))*y(m,:); \n%but in the code y(m,:)*y(m,:)'=1 and the denominator is 1 \nif nargin&lt;2 \n    tol=min(max(size(x)')*eps(max(x))); %find tolerance \nend \n25[m,dimension]=size(x); \nif dimension&gt;m \n    warning(['Input dimension larger than number of vectors, ' ... \n        'which is ok if you are aware about']); \nend \n30N=rank(x,tol); %note: rank is slow because it uses SVD \ny=zeros(N,dimension); %pre-allocate space \n%pick first vector and normalize it \ny(1,:)=x(1,:)/sqrt(sum(x(1,:).^2)); \nnumBasis = 1; \n35for k=2:m \n    errorVector=x(k,:); %error (or target) vector \n    for m=1:numBasis%go over all previously selected basis \n        %p_xy = &lt;x,y&gt; * y; %recall, in this case: ||y||=1 \n        projectionOverBasis = ((y(m,:)*x(k,:)'))*y(m,:); \n40        %update target: \n        errorVector = errorVector - projectionOverBasis; \n    end \n    magErrorVector = sqrt(sum(errorVector.^2)); \n    if ( magErrorVector &gt; tol ) \n45        %keep the new vector in basis set \n        numBasis = numBasis + 1; \n        y(numBasis,:)= errorVector / magErrorVector; \n        if (numBasis &gt;= N) \n            break; %Abort. Reached final number of vectors \n50        end \n    end \nend \nA=transpose(y);  %basis functions are the columns of inverse matrix A \nAh=A'; %the direct matrix transform is the Hermitian of A\n  \n\n\nThe result of the Gram-Schmidt procedure depends on the order of the input vectors. Rearranging these vectors may produce a different (still valid) solution. An example of Gram-Schmidt execution can be found in Application 2.1.\n\n\n\nThe following section presents another procedure, which has similarities to the Gram-Schmidt and very interesting properties.\n\n\n\n\n\nB.16  Principal component analysis (PCA)\n\n\n\nPrincipal component analysis (PCA) or Karhunen-Loève transform (KLT) is an orthogonal linear transformation typically used to transform the input data to a new coordinate system such that the greatest variance by any projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.\n\n\n\nThe following development does not try to be mathematically rigorous, but provide intuition. As in the Gram-Schmidt procedure, assume the goal is to obtain a set of N orthonormal vectors y from an input set {x1,x2,…,xM} composed by M vectors of\ndimension K. An important point is that all elements of an input vector x = [X1,…,XK] are assumed to have zero mean (or the mean is subtracted before PCA), i. e., 𝔼[Xi] = 0,∀ ⁡i. Instead of arbitrarily picking the first vector x1 as in Gram-Schmidt, PCA seeks the vector (first principal component) that maximizes the variance of the projected data. Restricting the basis function to have unity norm ∥y∥ = 1, the absolute value of the inner product ⟨y,x⟩ corresponds to the norm of the projection of x over y. PCA aims at minimizing the variance of this norm or, equivalently, the variance of ⟨y,x⟩. Because y is a fixed vector and 𝔼[x] = 0, it can be deduced that and 𝔼[⟨y,x⟩] = 0 and, consequently, the variance coincides with 𝔼{⟨y,x⟩2}. Hence, the first PCA basis function is given by\n\n\n\n\n\n\n y1 = arg ⁡ max ⁡  ∥y∥=1𝔼{⟨y,xi⟩2}. \n\n\n\n\n\n\nAfter obtaining y1, similarly to Gram-Schmidt, one finds new targets by projecting all M input vectors in y1 and keeping the errors as targets x^. For example, the targets for finding y2 are\n\n\n\n\n\n\n x^i = xi −⟨xi,y1⟩y1,i = 1,…,M, \n\n\n\n\n\n\nand the new basis function is obtained by\n\n\n\n\n\n\n y2 = arg ⁡ max ⁡  ∥y∥=1𝔼{⟨y,x^i⟩2}. \n\n\n\n\n\n\nSimilarly,\n\n\n\n\n\n\n y3 = arg ⁡ max ⁡  ∥y∥=1𝔼{⟨y,xi − (⟨xi,y1⟩y1 + ⟨xi,y2⟩y2)⟩} \n\n\n\n\n\n\nand so on.\n\n\n\nIt is out of the scope of this text3 to examine how to solve the maximization problems and find the optimal y vectors, but it is noted that the solution can be obtained via eigenvectors of the covariance matrix or singular value decomposition (SVD). Listing B.5 illustrates the former procedure, which uses eigen analysis. Note that the order of importance of the principal components y is given by the magnitude of the respective eigenvalues.\n\n\n\nListing B.5: MatlabOctaveFunctions/ak_pcamtx.m\n\n\nfunction [Ah,A,lambdas] = ak_pcamtx(data) \n% function [Ah,A,lambdas] = ak_pcamtx(data) \n%Principal component analysis (PCA). \n%Input: \n5%  data - M x D input matrix (M vectors of dimension D each) \n%Outputs: \n%  Ah - direct matrix, Hermitian (transpose in this case) of A \n%  A  - inverse matrix with principal components (or eigenvectors) in \n%       each column \n10%  lambdas - Mx1 matrix of variances (eigenvalues) \n%Example of usage: \n%x=[1 1; 0 2; 1 0; 1 4] \n%[Ah,A]=ak_pcamtx(x) %basis functions are columns of A \n%testVector=[1; 1]; %test: vector in same direction as first basis \n15%testCoefficients=Ah*testVector %result is [0.9683; -1.0307] \ncovariance = cov(data); % calculate the covariance matrix \n[pc,lambdas]=eig(covariance);%get eigenvectors/eigenvalues \nlambdas = diag(lambdas); % extract the matrix diagonal \n[temp,indices]= sort(-1*lambdas);%sort in decreasing order \n20lambdas = lambdas(indices); %rearrange \nA = pc(:,indices); %obtain sorted principal components: columns of A \nAh=A'; %provide also the direct transform matrix (Hermitian of A)\n  \n\n\nPCA is typically used for dimensionality reduction in a data set by retaining those characteristics of the data that contribute most to its variance, by keeping lower-order principal components and ignoring higher-order ones. Therefore, it is also useful for coding. One important question is why maximizing the variance is a good idea. In fact, this is not always the case and depends on the application.\n\n\n\nThe utility of PCA for representing signals will be illustrated by an example with data draw from a bidimensional Gaussian fX 1,X 2(x1,x2) with mean μ^ = (1,3)T  and covariance matrix C =  [  1  0.9       0.9   4    ]. Figure B.4 provides a scatter plot of the data and the basis functions obtained via PCA and, for comparison purposes, Gram-Schmidt orthonormalization. It can be seen that PCA aligns the basis with the directions where the variance is larger. The solution obtained with Gram-Schmidt depends on the first vector and the only guarantee is that the basis functions are orthonormal.\n\n\n\n\n\n\n\n\nFigure B.4: Scatter plot of the input data and the basis functions obtained via PCA and Gram-Schmidt orthonormalization.\n\n\n\n\n\nAs already indicated, the basis functions of a orthonormal basis can be organized as the columns of a matrix A. Examples will be provided in the sequel where A stores the basis obtained with PCA or the Gram-Schmidt procedure. At this point it is interesting to adopt the same convention used in most textbooks: instead of Eq. (2.1), the input and output vectors are assumed to be related by x = Ay. In other words, it is the inverse of A that transforms x into y: y = A−1x. This is convenient because y will be interpreted as storing the transform coefficients of the linear combination of basis functions (columns of A) that leads to x.\n\n\n\nIt is possible now to observe the interesting effect of transforming input vectors using PCA and Gram-Schmidt. Using the same input data that generated Figure B.4, Figure B.5 and Figure B.6 show the scatter plots of y obtained by y = A−1x, where A represents the basis from PCA and Gram-Schmidt, respectively.\n\n\n\n\n\n\n\n\nFigure B.5: Scatter plots of two-dimensional Gaussian vector x (represented by x) and PCA transformed vectors y (represented by +). Note that the first dimension y1 contains most of the variance in the data.\n\n\n\n\n\n\n\n\n\n\nFigure B.6: Scatter plots of two-dimensional Gaussian vector x (x) and Gram-Schmidt transformed vectors y (+).\n\n\n\n\n\nComparing Figure B.5 and Figure B.6 clearly shows that PCA does a better job in extracting the correlation between the two dimensions of the input data. In the Gram-Schmidt case, the figure shows a negative correlation between the coefficients y1 and y2. This is an example of a feature that may be useful. In summary, some linear transforms will be designed such that the basis functions have specified properties while others will focus on properties of the coefficients.\n\n\nB.17  Fourier Analysis: Properties\n\n\n\nIn the sequel, it is assumed that X(f), Y (f) and Z(f) are the Fourier transforms of x(t), y(t) and z(t), respectively. A pair (time / frequency) is denoted by  ⇔. The following discussion assumes the Fourier transform, but the properties are valid for all four Fourier tools with subtle distinctions.\n\n\n\nLinearity: if a signal x(t) = αy(t) + z(t) is obtained by multiplying y(t) by a constant α and summing the result to z(t), its transform is X(f) = αY (f) + Z(f). Linearity can be stated as:\n\n\n\n\n\n\n\n\n\n αx(t) + βy(t) ⇔ αX(f) + βY (f). \n\n\n(B.41)\n\n\n\n\n\n\nLinearity can be decomposed into two properties: a) homogeneity and b) additivity, which correspond to the properties αx(t) ⇔ αX(f) and x(t) + y(t) ⇔ X(f) + Y (f), respectively.\n\n\n\nTime-shift:\n\n\n\n\n\n\n\n\n\n x(t − t0) ⇔ X(f)e−j2πft0 . \n\n\n(B.42)\n\n\n\n\n\n\nScaling:\n\n\n\n\n\n\n\n\n\n x(at) ⇔ 1 |a|X(f∕a). \n\n\n(B.43)\n\n\n\n\n\n\nTime-reversal (scaling with a = −1):\n\n\n\n\n\n\n\n\n\n x(−t) ⇔ X(−f). \n\n\n(B.44)\n\n\n\n\n\n\nComplex-conjugate:\n\n\n\n\n\n\n\n\n\n x∗(t) ⇔ X∗(−f). \n\n\n(B.45)\n\n\n\n\n\n\nCombined time-reversal and complex-conjugate:\n\n\n\n\n\n\n\n\n\n x∗(−t) ⇔ X∗(f). \n\n\n(B.46)\n\n\n\n\n\n\nMultiplication:\n\n\n\n\n\n\n\n\n\n x(t)y(t) ⇔ X(f)∗Y (f). \n\n\n(B.47)\n\n\n\n\n\n\nFrequency-shift:\n\n\n\n\n\n\n\n\n\n x(t)ej2πf0t ⇔ X(f − f 0). \n\n\n(B.48)\n\n\n\n\n\n\nConvolution:\n\n\n\n\n\n\n\n\n\n x(t)∗y(t) ⇔ X(f)Y (f). \n\n\n(B.49)\n\n\n\n\n\n\nDuality:\n\n\n\n\n\n\n\n\n\n X(t) ⇔ x(−f). \n\n\n(B.50)\n\n\n\n\n\n\nExample: rect(t) ⇔sinc(f), then by duality sinc(t) ⇔rect(−f) = rect(f) (because rect(⋅) is an even function).\n\n\n\nEnergy and power conservation (Plancherel / Parseval theorem).For energy signals:\n\n\n\n\n\n\n\n\n\n E = ∫ −∞∞|x(t)|2dt = ∫ −∞∞|X(f)|2df. \n\n\n(B.51)\n\n\n\n\n\n\nFor periodic (power) signals with fundamental period T0:\n\n\n\n\n\n\n\n\n\n P = 1 T0 ∫ &lt;T0&gt;|x(t)|2dt = ∑ k=−∞∞|c k|2, \n\n\n(B.52)\n\n\n\n\n\n\nwhere ck are the coefficients of the Fourier series of x(t).\n\n\n\nAutocorrelation (Wiener-Khinchin theorem):\n\n\n\n\n\n\n\n\n\n Rx(τ) = ∫ −∞∞x(t + τ)x∗(t)dt ⇔ X∗(f)X(f) = |X(f)|2. \n\n\n(B.53)\n\n\n\n\n\n\n\n\nB.18  Fourier Analysis: Pairs\n\n\n\nThis section lists few pairs, which are among the most important ones. Both continuous and discrete-time signals are exemplified.\n\n\n\n\n\n\nf 1.\n\n\nimpulse  ⇔ DC level\n\n\n\n\n\n x(t) = δ(t) ⇔ X(ω) = 2π \n\n\n\n\n &lt;table class=\"equation-star\"&gt;&lt;tr&gt;&lt;td&gt;\n &lt;!--l. 131--&gt;&lt;math \nxmlns=“http://www.w3.org/1998/Math/MathML”\ndisplay=“block” class=“equation”&gt; x[n] = δ[n] ⇔ X(ejΩ) = 1 \n\n\n\n\n &lt;table class=\"equation-star\"&gt;&lt;tr&gt;&lt;td&gt;\n                                                                          \n                                                                          \n &lt;!--l. 135--&gt;&lt;math \nxmlns=“http://www.w3.org/1998/Math/MathML”\ndisplay=“block” class=“equation”&gt; x[n] = 1 2π ⇔ X(ejΩ) = ∑ k=−∞∞δ(Ω − k2π) \n\n\n\n\n &lt;/dd&gt;&lt;dt class=\"enumerate-enumitem\"&gt;&lt;span \nclass=“ec-lmbx-10x-x-109”&gt;f 2.\n\n\npulse  ⇔ sinc \n\nSeveral pairs of pulses and sincs are described below.4\n\n &lt;table class=\"equation\"&gt;&lt;tr&gt;&lt;td&gt;\n &lt;!--l. 143--&gt;&lt;p class=\"noindent\" &gt;\n &lt;/p&gt;&lt;!--l. 143--&gt;&lt;math \nxmlns=“http://www.w3.org/1998/Math/MathML”\ndisplay=“block” class=“equation”&gt; x(t) =  {   A, − T∕2 ≤ t ≤ T∕2    0,otherwise    ⇔X(f) = ATsinc(fT) \n\n\n(B.54)\n\n\n\n\n &lt;!--l. 147--&gt;&lt;p class=\"noindent\" &gt; &lt;br \nclass=“newline” /&gt;\n\n\n\n\n\n\n x[n] =  {   1,0 ≤ n ≤ M − 1    0,otherwise     ⇔X(ejΩ) = sin ⁡ (ΩM∕2) sin ⁡ (Ω∕2) e−jΩ(M−1)∕2 \n\n\n\n\n &lt;!--l. 154--&gt;&lt;p class=\"noindent\" &gt;Due to the duality property, sincs in time-domain lead to pulses in frequency-domain.\n In continous-time, one has: &lt;/p&gt;&lt;table class=\"equation\"&gt;&lt;tr&gt;&lt;td&gt;\n &lt;!--l. 155--&gt;&lt;p class=\"noindent\" &gt;\n &lt;/p&gt;&lt;!--l. 155--&gt;&lt;math \nxmlns=“http://www.w3.org/1998/Math/MathML”\ndisplay=“block” class=“equation”&gt; x(t) = 1 πtsin ⁡ (2πFt) = 2Fsinc(2Ft) ⇔ X(f) =  {   1,  |f|≤ F    0,  otherwise   \n\n\n(B.55)\n\n\n\n\n &lt;!--l. 159--&gt;&lt;p class=\"noindent\" &gt;where &lt;!--l. 159--&gt;&lt;math \nxmlns=“http://www.w3.org/1998/Math/MathML”\ndisplay=“inline” &gt;F is given in Hertz, and\n\n\n\n\n\n\n\n\n\n x(t) = 1 πtsin(Wt) ⇔ X(ω) =  {   1,  |ω|≤ W    0,  otherwise   \n\n\n(B.56)\n\n\n\n\n &lt;!--l. 164--&gt;&lt;p class=\"noindent\" &gt;where &lt;!--l. 164--&gt;&lt;math \nxmlns=“http://www.w3.org/1998/Math/MathML”\ndisplay=“inline” &gt;W is given in rad/s. Note that in Eq. (B.56) one has a sine, not a sinc.\n\n\n\nConsidering discrete-time signals and assuming α ≥ 1 determines the spectrum bandwidth (recall that it suffices to specify X(ejΩ) for  − π ≤Ω ≤ π ) one has:\n\n\n\n\n\n\n\n\n\n x[n] = 2π α sinc  (n α ) ⇔X(ejΩ) =  {   1, − π∕α ≤Ω ≤ π∕α   0, otherwise     \n\n\n(B.57)\n\n\n\n\n &lt;!--l. 172--&gt;&lt;p class=\"noindent\" &gt;Now it is assumed a discrete-time pulse train\n &lt;!--l. 172--&gt;&lt;math \nxmlns=“http://www.w3.org/1998/Math/MathML”\ndisplay=“inline” &gt;x[n] with period N and, for the pulse centered in 0, x[n] = 1 from n = −M to N = M and 0 otherwise. This pulse is replicated: the next is centered in n = N and has non-zero values in the range [N − M,N + M] and so on. The spectrum is\n\n\n\n\n\n\n X[k] = 1 N sin ⁡   (k(2M + 1) π N )   sin ⁡   (k π N )  , \n\n\n\n\n &lt;!--l. 178--&gt;&lt;p class=\"noindent\" &gt;where &lt;!--l. 178--&gt;&lt;math \nxmlns=“http://www.w3.org/1998/Math/MathML”\ndisplay=“inline” &gt;X[k] = 2M+1 N ,k = 0,±N,±2N,…, via L’Hopital’s rule.\n\n\n\nFor a continuous-time pulse train with each pulse of duration 2Tp and period T0 (one pulse is centered at t = 0, with duration from  − Tp to Tp) one has:\n\n\n\n\n\n\n ck = sin ⁡ (2πkf0Tp) kπ = sin ⁡ (kω0Tp) kπ = 2f0Tpsinc(2kf0Tp) = 2Tp T0 sinc  (k2Tp T0 ) , \n\n\n\n\n &lt;!--l. 206--&gt;&lt;p class=\"noindent\" &gt;where &lt;!--l. 206--&gt;&lt;math \nxmlns=“http://www.w3.org/1998/Math/MathML”\ndisplay=“inline” &gt;f0 = 1∕T0 and ω0 = 2πf0.\n\n\n\nf 3.\n\n\nexponential  ⇔ rational function\n\n\n\n\n\n e−atu(t) ⇔ 1 (jω + a),a &gt; 0 \n\n\n\n\n &lt;!--l. 212--&gt;&lt;p class=\"noindent\" &gt; &lt;br \nclass=“newline” /&gt;\n\n\n\n\n\n\n anu[n] ⇔ 1 (1 − ae−jΩ),  |a| &lt; 1 \n\n\n\n\n &lt;/dd&gt;&lt;dt class=\"enumerate-enumitem\"&gt;&lt;span \nclass=“ec-lmbx-10x-x-109”&gt;f 4.\n\n\ncomplex exponential  ⇔ impulse\n\n\n\n\n\n\n\n\n ej2πf0t ⇔ δ(f − f 0) \n\n\n(B.58)\n\n\n\n\n &lt;/dd&gt;&lt;dt class=\"enumerate-enumitem\"&gt;&lt;span \nclass=“ec-lmbx-10x-x-109”&gt;f 5.\n\n\ntrain of impulses  ⇔ train of impulses \n\nThe Fourier series coefficient of an impulse train of period T0, with one of the impulses δ(t) at the origin t = 0 is ck = 1∕T0. In case the impulses are shifted in time, a linear phase appears in ck. If the series should be represented in the transform domain, the coefficient values ck are represented by impulses with area ck and separated in frequency by multiples of f0 = 1∕T0, creating another impulse train:\n\n\n\n\n\n\n\n\n\n ∑ l=−∞∞δ(t − lT 0) ⇔ 1 T0 ∑ k=−∞∞δ  (f − k T0 ) . \n\n\n(B.59)\n\n\n\n\n &lt;!--l. 232--&gt;&lt;p class=\"noindent\" &gt;In rad/s instead of Hertz, the pair is: &lt;/p&gt;&lt;table class=\"equation\"&gt;&lt;tr&gt;&lt;td&gt;\n &lt;!--l. 233--&gt;&lt;p class=\"noindent\" &gt;\n &lt;/p&gt;&lt;!--l. 233--&gt;&lt;math \nxmlns=“http://www.w3.org/1998/Math/MathML”\ndisplay=“block” class=“equation”&gt; ∑ l=−∞∞δ(t − lT 0) ⇔ 2π T0 ∑ k=−∞∞δ  (ω − k2π T0 ) \n\n\n(B.60)\n\n\n\n\n &lt;!--l. 237--&gt;&lt;p class=\"noindent\" &gt;For discrete-time, a train of impulses with amplitude one and period\n &lt;!--l. 237--&gt;&lt;math \nxmlns=“http://www.w3.org/1998/Math/MathML”\ndisplay=“inline” &gt;N0 leads to series coefficients X[k] = 1∕N0,∀ ⁡k. Representing these coefficients in the transform domain leads to another train of impulses in Ω spaced by 2π∕N0 with areas 2π∕N0:\n\n\n\n\n\n\n ∑ l=−∞∞δ[n − lN 0] ⇔ 2π N0 ∑ k=−∞∞δ  (Ω − k 2π N0 ) . \n\n\n\n\n &lt;!--l. 242--&gt;&lt;p class=\"noindent\" &gt;Due to the corresponding Fourier series, note that any periodic impulse train can be\n written as a sum of of complex exponentials. For instance, in discrete-time:\n &lt;/p&gt;&lt;table class=\"equation\"&gt;&lt;tr&gt;&lt;td&gt;\n &lt;!--l. 243--&gt;&lt;p class=\"noindent\" &gt;\n                                                                          \n                                                                          \n &lt;/p&gt;&lt;!--l. 243--&gt;&lt;math \nxmlns=“http://www.w3.org/1998/Math/MathML”\ndisplay=“block” class=“equation”&gt; ∑ l=−∞∞δ[n − lN 0] = 1 N0 ∑ k=0N0 −1ej2πkn∕N0 . \n\n\n(B.61)\n\n\n\n\n &lt;!--l. 247--&gt;&lt;p class=\"noindent\" &gt;In continuous-time, one can represent the transform\n &lt;!--l. 247--&gt;&lt;math \nxmlns=“http://www.w3.org/1998/Math/MathML”\ndisplay=“inline” &gt;X(f) of the impulse train in two distinct ways. One is by calculating the series coefficients ck = 1∕T0 and then representing them in the transform domain. The second alternative is via the definition of the Fourier transform of the impulse train, and then using the impulse sifting property. These two alternatives can be written as:\n\n\n\n\n\n\n\n\n\n X(f) = 1 T0 ∑ k=−∞∞δ  (f − k T0 ) = ∑ k=−∞∞e−j2πfkT0 . \n\n\n(B.62)\n\n\n\n\n &lt;!--l. 252--&gt;&lt;p class=\"noindent\" &gt;The equality in Eq. (&lt;a \nhref=“#x12-278016r62”&gt;B.62) is not trivial because the continuous-time impulse is a generalized function. In order to get insight on how the sum of these complex exponentials converge to an impulse train, the interested reader can use the code MatlabOctaveCodeSnippets/snip_appfourier_impulse_train.m.\n\n &lt;/dd&gt;&lt;/dl&gt;\n\n\n\n\nB.19  Probability and Stochastic Processes\n\n\n\nThis appendix will provide a very brief overview about key topics. If the reader finds extra time, he/she can find many good textbooks about probability and stochastic (or random) processes.\n\n\n\n\n\nB.19.1  Joint and Conditional probability\n\n\n\nIf two events A and B are statistically independent, their joint probability P(A,B) is the multiplication of their individual probabilities: P(A,B) = P(A)P(B). For instance, obtaining two heads when tossing a fair coin twice is P(H,H) = 0.5 × 0.5 = 0.25, given that P(H) = 0.5. Given that the event B is tossing the coin for the second time, we can say that the conditional probability P(B∕A) of B given that the first tossing event A occurred is P(B∕A) = P(B) because A does not influence B.\n\n\n\nIn general, the conditional probability is obtained by its definition: P(B∕A) = P(A,B)∕P(A). Writing this equation as P(A,B) = P(A)P(B∕A), one can imagine a causal relation between A and B, with A occurring with P(A), before B, and then B occurring with P(B∕A) given that A happened.\n\n\n\nStarting from the joint probability P(A,B), one can pick B as the first event that happens and write P(A,B) = P(B)P(A∕B). Combining the two alternatives of writing P(A,B), i. e. P(A)P(B∕A) = P(B)P(A∕B) leads to\n\n\n\n\n\n\n\n\n\n P(A∕B) = P(B∕A)P(A) P(B) , \n\n\n(B.63)\n\n\n\n\n\n\nwhich is called the Bayes rule (see, e. g., [urlBMpro]).\n\n\n\n\n\nB.19.2  Random variables\n\n\n\nFirst note that the outcome of a probabilistic experiment need not be a number but any element of a set Ω of possible outcomes. For example, the outcome when a coin is tossed can be ω =“heads” or ω =“tails”. Basically, random variables allow us to map any probabilistic event into numbers, which are then conveniently manipulated using mathematical operations such as integral and derivatives. A source of confusion is that, strictly, a random variable (e.g., X or Y) is a function. More specifically, a random variable (r.v.) is a function X : Ω → ℝ that associates a unique numerical value with every outcome of an experiment (r.v. can be complex numbers, vectors, etc., but here it will be assumed as a real number). In math, a function output is often represented as y = f(x). When dealing with a r.v., instead of adopting something like X(ω), both the random variable (equivalent to the function f) and its output value (equivalent to y) is represented by a single letter (e.g., X or Y).\n\n\n\nThere are two types of r.v.: discrete and continuous. Hence, a r.v. has either an associated probability distribution (discrete r.v.) or probability density function (continuous r.v.).\n\n\n\nAssume a discrete r.v. X and a continuous r.v. Y. While the former is typically described by a probability mass function (pmf), the latter can be described by a probability density function (pdf).\n\n\n\nSay that X represents the outcome of rolling a dice. Its PMF is shown in Figure B.7\nand indicates that each face of a fair dice has a probability of 1∕6.\n\n\n\n\n\n\n\n\nFigure B.7: PMF for a dice result.\n\n\n\n\n\nNow consider that Y represents the amplitude of a Gaussian noise source with mean 2 and variance equal to 1. Its pdf is shown in Figure B.8. A common mistake is to assign a non-zero value to a specific value of a density function. For example, it is wrong to say that the probability of Y = 2 is 0.4, in spite of this being the value of the function. The function represents a density, and the correct answer is that the probability of Y = 2, or any other point, is 0. One can extract probability from a pdf only integrating it over a non-zero range of its abscissa. For example, over the range [2, 3] the probability is approximately 0.34 as indicated by the shaded area in Figure B.8.\n\n\n\n\n\n\n\n\nFigure B.8: Obtaining probability from a pdf (density function) requires integrating over a range. Example shows that the probability of this Gaussian variable be within [2, 3] is approximately 0.34.\n\n\n\n\n\nWhen dealing with ratios of pdfs it is possible to have the abscissa range Δx canceling out. For example, if a discrete binary r.v. is used to represent two classes (A and B, for example), and each class has a pdf associated to it (f(x|A) and f(x|B), respectively), the Bayes’ rule states\n\n\n\n\n\n\n\n\n\n P(A|x) = f(x|B) f(x|A)P(B|x). \n\n\n(B.64)\n\n\n\n\n\n\nIn this case, Δx cancels because it appears on both numerator and denominator.\n\n\nB.19.3  Expected value\n\n\n\nThe expected value 𝔼[⋅] operator is the most common mathematical formalism for calculating an average (or mean).\n\n\n\nThe expected value is a linear operator (see Appendix B.27.1 for more information about linearity), such that\n\n\n\n\n\n\n\n\n\n 𝔼[αX + βY] = α𝔼[X] + β𝔼[Y]. \n\n\n(B.65)\n\n\n\n\n\n\nThe expected value 𝔼[X] of a random variable X can be estimated as a typical average when X can be represented by a finite-dimension vector. For example, if x = [3,5,4,4,5,3] is a vector with random samples from X, the expected value 𝔼[X] can be estimated as 𝔼[X] ≈x¯ = (3 + 5 + 4 + 4 + 5 + 3)∕6 = 4, where x¯ is the conventional mean value.\n\n\n\nIf one has realizations of a random variable X, for instance organized as a vector x, and is looking for the expected value 𝔼[g(X)] of a function g(X), it is possible to apply the function g(⋅) to each realization (value of x) and then take their average. For example, assume g(X) = X2 and one is interested on estimating 𝔼[X2] based on realizations x = [−1,3,−3,4,−2] of X. In this case, applying g(X) = X2 to elements of x leads to the vector y = [(−1)2,32,(−3)2,42,(−2)2], which has the average y¯ = (1 + 9 + 9 + 16 + 4)∕5 = 7.8. The estimate is 𝔼[X2] ≈y¯ = 7.8.\n\n\n\nAs another example, consider μx = 𝔼[X] is known (or has been previously estimated), and one is interested on estimating the variance 𝔼[(X − μx)2]. In this case, g(X) = (X − μx)2 and if the realizations are still x = [−1,3,−3,4,−2], which has an average μx = 0.2, applying f(X) leads to a vector z = [(−1 − 0.2)2,(3 − 0.2)2,(−3 − 0.2)2,(4 − 0.2)2,(−2 − 0.2)2] with mean value z¯ = 7.76. In this case the estimated variance is 𝔼[(X − μx)2] ≈z¯ = 7.76.\n\n\n\nThe variance is often denoted as σx2, and can be written as\n\n\n\n σx2  = 𝔼[(X − μ x)2]    (definition of variance)       = 𝔼[X2 − 2Xμ x + μx2]    (by expanding the square)       = 𝔼[X2] − 𝔼[2Xμ x] + 𝔼[μx2]    (the expected value is a linear operation)       = 𝔼[X2] − 2μ x𝔼[X] + μx2     (μ x is a constant)       = 𝔼[X2] − 2μ x2 + μ x2     (𝔼[X] = μ x)       = 𝔼[X2] − μ x2,   (B.66)  \n\nwhich is often interpreted as 𝔼[X2] = σx2 + μx2.\n\n\n\nWhen a discrete random variable X has V  distinct values, its mean 𝔼[X] can be estimated with\n\n\n\n\n\n\n\n\n\n 𝔼[X] ≈x¯ = ∑ i=1V p ixi, \n\n\n(B.67)\n\n\n\n\n\n\nwhere pi is the probability of the i-th possible value xi. For instance, the realizations x = [3,5,4,4,5,3] have only V = 3 distinct values: x1 = 3,x2 = 4 and x3 = 5, each one with estimated probability pi = 1∕3,∀ ⁡i. Hence,\n\n\n\n\n\n\n 𝔼[X] ≈x¯ = ∑ i=1V p ixi = (1∕3)(3 + 4 + 5) = 4. \n\n\n\n\n\n\nThe same result could be obtained by directly taking the mean value of the N = 6 elements with\n\n\n\n\n\n\n 𝔼[X] ≈x¯ = 1 N∑ n=1Nx[n] = (3 + 5 + 4 + 4 + 5 + 3)∕6 = 4. \n\n\n\n\n\n\nNote that x[n] corresponds to the n-th element in x, while xi is the i-th distinct value of x.\n\n\n\nWhen X ∈ ℝ is a continuous random variable, instead of Eq. (B.67) one has its continuous version:\n\n\n\n\n\n\n 𝔼[X] = ∫ −∞∞xf X(x)dx, \n\n\n\n\n\n\nwhere fX(x) is the probability density function of X. In this case, for each value of x, the role of the probability pi in the discrete r.v. case, is played by the value fX(x) that corresponds to a “weight” that depends on the likelihood of the specific value x.\n\n\n\nIn order to find 𝔼[g(X)] for a given function g(⋅) one can use:\n\n\n\n\n\n\n\n\n\n 𝔼[g(X)] = ∫ −∞∞g(x)f X(x)dx. \n\n\n(B.68)\n\n\n\n\n\n\n\n\nB.19.4  Orthogonal versus uncorrelated\n\n\n\nTwo random variables X and Y are said to be orthogonal to each other if\n\n\n\n\n\n\n 𝔼[XY∗] = 0. \n\n\n\n\n\n\nThey are said to be uncorrelated with each other if\n\n\n\n\n\n\n 𝔼[(X − 𝔼[X])(Y − 𝔼[Y])∗] = 0. \n\n\n\n\n\n\nThe above condition is equivalent to\n\n\n\n\n\n\n 𝔼[XY∗] = 𝔼[X]𝔼[Y]∗. \n\n\n\n\n\n\nNote that if one or both of X and Y have zero mean, then the orthogonal and uncorrelated conditions are equivalent.\n\n\n\n\n\nB.19.5  PDF of a sum of two independent random variables\n\n\n\nIf X and Y are independent, Z = X + Y implies f(z) = f(x)∗f(y). For example, the sum of a bipolar signal (-5 and +5V with 0.5 of probability each) with pdf fx(x) = 0.5[δ(x − 5) + δ(x + 5)] and additive white Gausssian noise (AWGN) with pdf fy(y) is a good example because the result is the Gaussian scaled by 0.5 and shifted to the position of each of the original impulses, at  − 5 and 5, i. e., fz(z) = 0.5[fy(z − 5) + fy(z + 5)].\n\n\n\n\n\nB.20  Stochastic Processes\n\n\n\nStochastic or random processes are a powerful formalism for studying signals that incorporate some randomness. While the outcome of a random variable is a number, the outcome of a stochastic process is a time-series, each one called a realization. Hence, when using stochastic processes, it is possible to calculate “time” statistics of a single realization. Besides, one can choose a given time instant and calculate “ensemble” statistics over all realizations at that specific time. These two options may be confusing at first, but are essential. To get intuition on them, the reader is invited to observe the following simple example.\n\n\n\n Example B.2. Simple example to explain time and ensemble averages in a stochastic process. Assume a vector [2, 0, 2, 3, 3, 2, 3] where each element represents the outcome of a random variable X indicating the number of victories of a football team in four matches (one per week), along a seven-months season (for example, the team won 2 games in the first month and none in the second). One can calculate the average number of victories as 𝔼[X] ≈ 2.14, the standard deviation σ ≈ 1.07 and other moments of a random variable. This vector could be modeled as a finite-duration random signal x[n], with Figure B.9 depicting its graph.\n\n\n\n\n\n\n\n\nFigure B.9: Example of a finite-duration random signal.\n\n\n\n\n\nAssume now there are 5 vectors, one for each year. A discrete-time random process can be used to model the data, with each vector (a time series) corresponding to a realization of the random process. For example, following the Matlab convention of organizing different time series in columns (instead of rows), the tabular data below represents the five random signals:\n\nvictories = [\n     2     3     0     0     1\n     0     1     3     2     1\n     2     1     4     0     2\n     3     4     3     1     2\n     3     3     4     3     4\n     2     1     1     2     3\n     3     4     4     2     4]\n\n\n\nEach random signal (column) can be modeled as a realization of a random process. Figure B.10 shows a graphical representation of the data with the five realizations of the example. One should keep in mind that a random process is a model for generating an infinite number of realizations but only 5 are used here for simplicity.\n\n\n\n\n\n\n\n\nFigure B.10: Example of five realizations of a discrete-time random process.\n\n\n\n\n\nA discrete-time random process will be denoted by X[n] and one of its realization as x[n]. For the sake of explaining this example, let us define the notation xi[n] to indicate the i-th realization of X[n]. Because xi[n] is a random signal, one can calculate for the given example, 𝔼[x1[n]] ≈ 2.14 and other statistics taken over time. But a random process is a richer model than a random signal and other statistics can be calculated or, equivalently, more questions can be asked.\n\n\n\nBesides asking for statistics of one realization taken over “time”, it is important to understand that when fixing a given time instant n0, the values that the realizations assume are also outcomes of a random variable that can be denoted by X[n0]. In the example, assuming n0 = 4 (i.e., assume the fourth month), one can estimate the average 𝔼[X[4]] = 2.6. Similarly, 𝔼[X[6]] = 1.8. Figure B.11 graphically illustrates the operation of evaluating a random process at these two time instants. Differently than the previous time averages, these “ensemble” statistics are obtained across realizations with fixed time instants.\n\n\n\n\n\n\n\n\nFigure B.11: Example of evaluating a random process at time instants n = 4 and n = 6, which correspond to the values of two random variables X[4] and X[6]. There are only five pairs and each occurs once, hence a 0.2 estimated probability for all points.\n\n\n\n\n\nThe continuous-time random process X(t) is similar to its discrete-time counterpart. Taking two different time values t and s leads to a pair of random variables, which can then be fully (statistically) characterized by a joint pdf function f(X(t),X(s)). This can be extended to three or more time instants (random variables). Back to the discrete-time example, one can use a normalized two-dimensional histogram to estimate the joint PMF of X[4] and X[6] as illustrated in Figure B.12.\n\n\n\n\n\n\n\n\nFigure B.12: Example of a joint pdf of the continuous random variables X[4] and X[6].\n\n\n\n\n\nThis example aimed at distinguishing time and ensemble averages, which is fundamental to understand stochastic processes.   □\n\n\nCorrelation Function and Matrix\n\n \n\nSecond-order moments of a random process involve statistics taken at two distinct time instants. Two of them are the correlation and covariance functions, which are very useful in practice.\n\n\n\nAs discussed in Section 1.12, the correlation function is an extension of the correlation concept to random signals generated by stochastic processes. The autocorrelation function (ACF)\n\n\n\n\n\n\n\n\n\n RX(s,t) = 𝔼[X(s)X(t)] \n\n\n(B.69)\n\n\n\n\n\n\nis the correlation between random variables X(s) and X(t) at two different points s and t in time of the same random process. Its purpose is to determine the strength of relationship between the values of the signal occurring at two different time instants. For simplicity, it is called RX(s,t) instead of RXX(s,t).\n\n\n\nA discrete-time version of Eq. (B.69) is\n\n\n\n\n\n\n\n\n\n RX[n1,n2] = 𝔼  [X[n1]X[n2]], \n\n\n(B.70)\n\n\n\n\n\n\nwhere n1 and n2 are two “time” instants.\n\n\n\nSpecially for discrete-time processes, it is useful to organize autocorrelation values in the form of an autocorrelation matrix.\n\n\n\n Example B.3. Calculating and visualizing the autocorrelation matrix (not assuming stationarity). Stationarity is a concept that will be discussed in the sequel (Appendix B.20.0.0). Calculating and visualizing autocorrelation is simpler when the process is stationary. But in this example, this property is not assumed and the autocorrelation is obtained for a general process.\n\n\n\nListing B.6 illustrates the estimation of RX[n1,n2] for a general discrete-time random process.5\n\n\n\nListing B.6: MatlabOctaveFunctions/ak_correlationEnsemble.m\n\n\nfunction [Rxx,n1,n2]=ak_correlationEnsemble(X, maxTime) \n% function [Rxx,n1,n2]=ak_correlationEnsemble(X, maxTime) \n%Estimate correlation for non-stationary random processes using \n%ensemble averages (does not assume ergodicity). \n5%Inputs:  - X, matrix representing discrete-time process realizations \n%           Each column is a realization of X. \n%         - maxTime, maximum time instant starting from n=1, which \n%           corresponds to first row X(1,:). Default: number of rows. \n%Outputs: - Rxx is maxTime x maxtime and Rxx[n1,n2]=E[X[n1] X*[n2]] \n10%         - n1,n2, lags to properly plot the axes. The values of \n%           n1,n2 are varied from 1 to maxTime. \n[numSamples,numRealizations] = size(X); %get number of rows and col. \nif nargin==1 \n    maxTime = numSamples; %assume maximum possible value \n15end \nRxx = zeros(maxTime,maxTime); %pre-allocate space \nfor n1=1:maxTime \n    for n2=1:maxTime %calculate ensemble averages \n        Rxx(n1,n2)=mean(X(n1,:).*conj(X(n2,:))); %ensemble statistics \n20        %that depends on numRealizations stored in X \n    end \nend\n  \n\n\nFigure B.13 was created with the command ak_correlationEnsemble(victories) and shows the estimated correlation for the data in Figure B.10 of Example B.2. The data cursor indicates that RX[1,1] = 2.8 because for n = 1 the random variables of the five realizations are [2,3,0,0,1]. In this case, RX[1,1] = 𝔼[X[1]2] ≈ (4 + 9 + 1)∕5 = 2.8. Similar calculation leads to RX[1,7] = 𝔼[X[1]X[7]] ≈ 4.4 because in this case the products victories(1,:).*victories(7,:) are [6, 12, 0, 0, 4], which have an average of 4.4.\n\n\n\n\n\n\n\n\nFigure B.13: Correlation for data in matrix victories.\n\n\n\n\n\nFigure B.14 presents an alternative view of Figure B.13, where the values of the z-axis are indicated in color. The two datatips indicate that Rx[1,2] = 0.8 and Rx[3,5] = 6.6 (these are the values identified by Index for the adopted colormap, and the associated color is also shown via its RGB triple).\n\n\n\n\n\n\n\n\nFigure B.14: Version of Figure B.13 using an image.\n\n\n\n\n\nFor real-valued processes, the autocorrelation is symmetric, as indicated in Figure B.13 and Figure B.14, while it would be Hermitian for complex-valued processes.    □\n\n\nTwo time instants: both absolute or one absolute and a relative (lag)\n\n\n\nWhen characterizing second-order moments of a random process, instead of using two absolute time instants, it is sometimes useful to make one instant as a relative “lag” with respect to the other. For example, take two absolute instants t = 4 and s = 7 in Eq. (B.69). Alternatively, the same pair of instants can be denoted by considering t = 4 as the absolute time that provides the reference instant and a lag τ = s − t = 3 indicating that the second instant is separated by 3 from the reference.\n\n\n\nUsing this scheme, Eq. (B.69) can be written as\n\n\n\n\n\n\n\n\n\n RX(t,τ) = 𝔼[X(t + τ)X(t)]. \n\n\n(B.71)\n\n\n\n\n\n\nSimilarly, Eq. (B.70) can be denoted as\n\n\n\n\n\n\n\n\n\n RX[n,l] = 𝔼  [X[n]X[n + l]], \n\n\n(B.72)\n\n\n\n\n\n\nwith n1 = n and n2 = n + l, where l is the lag in discrete-time.\n\n\n\n Example B.4. Contrasting two different representations of autocorrelations. Figure B.13 is a graph that corresponds to Eq. (B.70), with two absolute time instants. When using the alternative representation of Eq. (B.72), with a lag, the resulting matrix is obtained by rearranging the elements of the autocorrelation matrix. Figure B.15 compares the two cases. Note that the name autocorrelation matrix is reserved for the one derived from Eq. (B.70) (and Eq. (B.69)) that is, for example, symmetric.\n\n\n\n\n\n\n\n\nFigure B.15: Comparison between the 3-d representation of the autocorrelation matrix in Figure B.13 and the one using lags as in Eq. (B.72).\n\n\n\n\n\nFigure B.16 is an alternative view of Figure B.15, which makes visualization easier. It better shows that Eq. (B.72) leads to shifting the column of the autocorrelation matrix (Figure B.13) corresponding to each value n = n1 of interest.\n\n\n\n\n\n\n\n\nFigure B.16: Comparison between autocorrelation representations using images instead of 3-d graphs as in Figure B.15.\n\n\n\n\n\nProperties such as the symmetry of the autocorrelation matrix may not be so evident from the right-side representation of Figure B.16 as in the left-side representation, but both are useful in distinct situations.    □\n\n\nCovariance matrix\n\n\n\nSimilar to autocorrelation functions, it is possible to define covariance functions that expand the definition for random variables such as Eq. (1.53) and take as arguments two time instants of a random process. For example, for a complex-valued discrete-time process the covariance is\n\n\n\n\n\n\n\n\n\n cX[n,l] = 𝔼  [(X[n] − μ[n])(X[n + l] − μ[n + l])∗]. \n\n\n(B.73)\n\n\n\n\n\n\n\n\nStationarity, cyclostationarity and ergodicity\n\n \n\nA random process is a powerful model. Hence, an important question is what information is necessary to fully characterize it. Take the 7 × 5 matrix in Example B.2: this data does not fully characterize the random process because it consists of only five realizations (tossing a coin five times does not allow to estimate its probability of heads robustly). We would need an infinite number of realizations if we insist in characterizing it in a tabular form. The most convenient alternative is to describe it through pdfs. We should be able to indicate, for all time t, the pdf of X(t). Besides, for each time pair (t,s), the joint pdf f(X(t),X(s)) should be known.\nThe same for the pdf f(X(t),X(s),X(r)) of each triple (t,s,r) and so on. It can be noted that a general random process requires a considerable amount of statistics to be fully described. So, it is natural to define particular and simpler cases of random processes as discussed in the sequel.\n\n\n\nAn important property of a random process is the stationarity of a statistics, which means this statistics is time-invariant even though the process is random. More specifically, a process is called n-th order stationary if the joint distribution of any set of n of its random variables is independent of absolute time values, depending only on relative time.\n\n\n\nA first-order stationary process has the same pdf f(X(t)),∀ ⁡t (similar is valid for discrete-time). Consequently, it has a constant mean\n\n\n\n\n\n\n\n\n\n 𝔼[X(t)] = μ,∀ ⁡t, \n\n\n(B.74)\n\n\n\n\n\n\nconstant variance and other moments such as 𝔼[X4(t)].\n\n\n\nA second-order stationary process has the same joint pdf f(X(t),X(s)) = f(X(t + τ),X(s + τ)),∀ ⁡t,s. Consequently, its autocorrelation is\n\n\n\n\n\n\n\n\n\n RX(τ) = 𝔼[X(t + τ)X(t)], \n\n\n(B.75)\n\n\n\n\n\n\nwhere the time difference τ = s − t is called the lag and has the same unit as t in X(t). Note that, in general, RX(t,τ) of Eq. (B.71) depends on two parameters: t and τ. However, for a stationary process, Eq. (B.75) depends only on the lag τ because, given τ, RX(t,τ) = RX(τ) is the same for all values of t.\n\n\n\nSimilarly, if a discrete-time process is second-order stationary, RX[n,l] Eq. (B.72) can be simplified to\n\n\n\n\n\n\n\n\n\n RX[l] = 𝔼[X[n]X[n + l]], \n\n\n(B.76)\n\n\n\n\n\n\nwhere l = n2 − n1 is the lag.\n\n\n\nA third-order stationary process has joint pdfs f(X(t1),X(t2),X(t3)) that do not depend on absolute time values (t1,t2,t3) and so on. A random process X(t) is said to be strict-sense stationary (SSS) or simply stationary if any joint pdf f(X(t1),…,X(tn)) is invariant to a translation by a delay τ. A random process with realizations that consist of i. i. d. random variables (called an i. i. d. process) is SSS. A random process that is not SSS is referred to as a non-stationary process even if some of its statistics have the stationarity property.\n\n\n\nThe SSS process has very stringent requirements, which are hard to verify. Hence, many real-world signals are modeled as having a weaker form of stationarity called wide-sense stationarity.\n\n\n\nBasically, a wide-sense stationary (WSS) process has a mean that does not vary over time and an autocorrelation that does not depend on absolute time, as indicated in Eq. (B.74) and Eq. (B.75), respectively. Broadly speaking, wide-sense theory deals with moments\n(mean, variance, etc.) while strict-sense deals with probability distributions. Note that SSS implies WSS but the converse is not true in general, with Gaussian processes being a famous exception.\n\n\n\nFor example, the process corresponding to Figure B.13 is not WSS: its autocorrelation does not depend only on the time difference τ.\n\n\n\n Example B.5. The autocorrelation matrix of a WSS process is Toeplitz. For a WSS process, elements RX(i,j) of its autocorrelation matrix depend only on the absolute difference |i − j| and, therefore, the matrix is Toeplitz. In this example, the function toeplitz.m is used to visualize the corresponding autocorrelation matrix as an image. For example, the command Rx=toeplitz(1:4) generates the following real-valued matrix:\n\n Rx=[1     2     3     4\n     2     1     2     3\n     3     2     1     2\n     4     3     2     1].\n\n\n\nUsually, the elements of the main diagonal correspond to RX(0) = 𝔼[X2[n]].\n\n\n\nTo visualize a larger matrix, Figure B.17 was generated with Rxx=toeplitz(14:-1:4).\n\n\n\n\n\n\n\n\nFigure B.17: Representation of a WSS autocorrelation matrix that depends only on the lag l.\n\n\n\n\n\nGiven the redundancy in Figure B.17, one can observe that for a WSS, it suffices to describe RX(l) for each value of the lag l.   □\n\n\n\nA cyclostationary process is a non-stationary process having statistical properties that vary periodically (cyclically) with time. In some sense it is a weaker manifestation of a stationarity property. Cyclostationary processes are further discussed in Appendix B.20.1.\n\n\n\nA given statistics (variance, autocorrelation, etc.) of a random process is ergodic if its time average is equal to the ensemble average. For example, an i. i. d. random process is ergodic in the mean.6 And, loosely speaking, a WSS is ergodic in both mean and autocorrelation if its autocovariance decays to zero.7\n\n\n\nA process could be called “ergodic” if all ensemble and time averages are interchangeable. However, it is more pedagogical to consider ergodicity as a property of specific statistics, and always inform them.8 Even non-stationary or cyclostationary processes can be ergodic in specific statistics such as the mean. Figure B.18 depicts the suggested taxonomy of random processes using a Venn diagram.\n\n\n\n\n\n\n\n\nFigure B.18: Suggested taxonomy of random processes. SSS are sometimes called stationary and all processes that are not SSS are called non-stationary.\n\n\n\n\n\n Example B.6. Polar and unipolar bitstreams modeled as WSS processes. This example is based on processes used in digital communications to represent sequences of bits. Note that, after these bit streams are upsampled in order to create line codes, they become cyclostationary processes. This issue is discussed in Appendix B.20.1, while here the sequences are not upsampled.\n\n\n\nFigure B.19 illustrates two processes for which the main diagonal RX[1,1] = RX[2,2] = RX[3,3]… has the same values. The process (a) (left-most) is called a unipolar line code and consists of sequences with equiprobable values 0 and 1, while for the second process (called polar) the values are  − 1 and 1.\n\n\n\nFor the unipolar and polar cases, the theoretical correlations RX[n1,n2] for n1 = n2 are 0.5 and 1, respectively. The result is obtained by observing that RX[n1,n2] = RX[n1,n1] = 𝔼[X[n1]2] and for the unipolar case, X[n1]2 is 0 or 1. Because the two values are equiprobable, 𝔼[X[n1]2] = 0.5. For the polar case, X[n1]2 = 1 always and 𝔼[X[n1]2] = 1. The values of RX[n1,n2] for n1≠n2 are obtained by observing the values of all possible products X[n1]X[n2]. For the unipolar case, the possibilities are 0 × 0 = 0, 0 × 1 = 0, 1 × 0 = 0 and 1 × 1 = 1, all with probability 1/4 each. Hence, for this unipolar example, 𝔼[X[n1]X[n2]] = (3 × 0 + 1 × 1)∕4 = 0.25,n1≠n2. For the polar case, the possibilities are  − 1 ×−1 = 1,  − 1 × 1 = 1, 1 ×−1 = −1 and 1 × 1 = 1, all with probability 1/4 each. Hence, for this polar example, 𝔼[X[n1]X[n2]] = (2 ×−1 + 2 × 1)∕4 = 0,n1≠n2.\n\n\n\nIn summary, for the polar code RX[n1,n2] = 1 for n1 = n2 and 0 otherwise. For the unipolar code, RX[n1,n2] = 0.5 for n1 = n2 and 0.25 otherwise.\n\n\n\n\n\n\n\n\nFigure B.19: Correlation for random sequences with two equiprobable values: a) values are 0 and 1 (unipolar), b) values are  − 1 and 1 (polar).\n\n\n\n\n\nA careful observation of Figure B.19 indicates that two dimensions are not necessary and it suffices to describe RX[l] as in Eq. (B.72). As discussed previously, in the case of WSS processes, the correlation depends only on the difference between the “time” instants n1 and n2. Figure B.20 emphasizes this aspect using another representation for the polar case in Figure B.19.\n\n\n\n\n\n\n\n\nFigure B.20: Alternative representation of the correlation values for the polar case of Figure B.19.\n\n\n\n\n\nBecause the polar and unipolar processes that were used to derive Figure B.19 are WSS, Listing B.7 can be used to take advantage of having RX[n1,n2] depending only on the lag and convert this matrix to an array RX[l].\n\n\n\nListing B.7: MatlabOctaveFunctions/ak_convertToWSSCorrelation.m\n\n\nfunction [Rxx_tau, lags] = ak_convertToWSSCorrelation(Rxx) \n% function [Rxx_tau,lags]=ak_convertToWSSCorrelation(Rxx) \n%Convert Rxx[n1,n2] into Rxx[k], assuming process is \n%wide-sense stationary (WSS). \n5%Input: Rxx -&gt; matrix with Rxx[n1,n2] \n%Outputs: Rxx_tau[lag] and lag=n2-n1. \n[M,N]=size(Rxx); %M is assumed to be equal to N \nlags=-N:N; %lags \nnumLags = length(lags); %number of lags \n10Rxx_tau = zeros(1,numLags); %pre-allocate space \nfor k=1:numLags; \n    lag = lags(k); counter = 0; %initialize \n    for n1=1:N \n        n2=n1+lag; %from: lag = n2 - n1 \n15        if n2&lt;1 || n2&gt;N %check \n            continue; %skip if out of range \n        end \n        Rxx_tau(k) = Rxx_tau(k) + Rxx(n1,n2); %update \n        counter = counter + 1; %update \n20    end \n    if counter ~= 0 \n        Rxx_tau(k) = Rxx_tau(k)/counter; %normalize \n    end \nend\n  \n\n\nFigure B.21 was generated using Listing B.7 for the data in Figure B.19.\n\n\n\n\n\n\n\n\nFigure B.21: One-dimensional ACF RX[l] for the data corresponding to Figure B.19 (unipolar and polar codes).\n\n\n\n\n\nIf ergodicity can be assumed, a single realization of the process can be used to estimate the ACF. Figure B.22 was generated using the code below, which generates a single realization of each process:\n\n\nnumSamples=1000; \nXunipolar=floor(2*rand(1,numSamples)); %r.v. 0 or 1 \n[Rxx,lags]=xcorr(Xunipolar,9,'unbiased');%Rxx for unipolar \nsubplot(121); stem(lags,Rxx) title('Unipolar'); \n5Xpolar=2*[floor(2*rand(1,numSamples))-0.5]; %r.v. -1 or 1 \n[Rxx,lags]=xcorr(Xpolar,9,'unbiased'); %Rxx for polar code \nsubplot(122); stem(lags,Rxx) title('Polar');\n\n\nIncreasing the number of samples to numSamples=1e6 achieves estimation errors smaller than in Figure B.22.\n\n\n\n\n\n\n\n\nFigure B.22: ACF estimated using ergodicity and waveforms with 1,000 samples for each process. The theoretical values are 0.25 and 0.5 (at l = 0) for unipolar, while for polar these values are 0 and 1.\n\n\n\n  □\n\nB.20.1  Cyclostationary random processes\n\n\n\nThis section aims at providing some intuition about cyclostationarity. More information can be found e. g. in [?,?,?].\n\n\n\nA discrete-time process X[n] is wide-sense cyclostationary (WSC) if, and only if, exists an integer period P &gt; 0 such that the mean and correlation are periodic (consequently, the covariance is periodic too), as summarized by:\n\n\n\n μX[n]  = μX[n + P]  and   RX[n,l]  = RX[n + P,l]. (B.77)  \n\nNote that the correlation is periodic but a cyclostationary process generates realizations that are random and not periodic. To avoid confusion, the period P in Eq. (B.77) of a WSC process is referred to as its cycle.\n\n\n\nBecause WSC are more common than strict-sense cyclostationary processes, the term cyclostationary is assumed here to correspond to a WSC (wide, not strict sense), as adopted in  [?], which is the main reference for the discussion presented in the sequel. In order to better understand WSC processes, it is useful to review the concepts of non-stationary processes in Appendix B.20 and get familiar with representations such as the ones in Figure B.15 and Figure B.16. While WSS processes have autocorrelations depending only on one parameter, the lag l, WSC correlations depend on two parameters (RX[n,l]) and this is a key factor for the following discussion.\n\n\n\nNote that Eq. (B.77) implies the correlation RX[n,l] in Eq. (B.72) is periodic in n for each “lag” l. In other words, given a lag value l0, the time-series RX[n,l0] over the “absolute time instant” n is periodic. Hence, it can be represented using a DTFS in which the Fourier coefficients CX[k,l] are called cyclic correlations. Therefore, when RX[n,l] has a period of P samples over n, the following Fourier pair can be used:\n\n\n\n\n\n\n\n\n\n RX[n,l] = ∑ k=0P−1C X[k,l]ej2π P nk ⇔ C X[k,l] = 1 P∑ n=0P−1R X[k,l]e−j2π P nk. \n\n\n(B.78)\n\n\n\n\n\n\n\nRX[n,l] is also called time-varying correlation and instantaneous correlation.\n\n\n\nAs explained in [?], sampling a continuous-time periodic signal may lead to an almost periodic signal for which Eq. (1.39) does not hold. This occurs in the so-called wide-sense almost cyclostationary processes, in which RX[n,l] has the following generalized Fourier series:\n\n\n\n\n\n\n\n\n\n RX[n,l] = ∑ αk∈CCX[αk,l]ejαkn ⇔ C X[αk,l] = lim ⁡  N→∞ 1 N∑ n=0P−1R X(k,l)e−jαkn, \n\n\n(B.79)\n\n\n\n\n\n\nwhere C is the set C = {αk : CX[αk,l]≠0,−π ≤ αk &lt; π} of cycles.\n\n\n\nThe estimation of both CX[k,l] and CX[αk,l] is often done with FFTs and, consequently, both periodic and almost periodic realizations are treated similarly. Hereafter, the notation CX[αk,l] is also adopted for CX[k,l].\n\n\n\nFor a given cycle αk, the evolution of CX[αk,l] over the lags l can be interpreted over frequency using its DTFT to define the cyclic spectrum:\n\n\n\n\n\n\n\n\n\n SX(αk,Ω) = ∑ l=−∞∞C X[αk,l]e−jΩl. \n\n\n(B.80)\n\n\n\n\n\n\nIn summary, while stationary discrete-time signals are analyzed using PSDs in frequency-domain (Ω) and time-invariant correlations in lag-domain (l), non-stationary signals include also the time (n) and cycle (α) domains. Figure B.23 illustrates the relations.\n\n\n\n\n\n\n\n\nFigure B.23: Functions used to characterize cyclostationary processes. FS stands for “generalized Fourier series”, useful for representing “almost periodic” discrete-time signals (Adapted from Fig. 17.1 of [?] to include the notation adopted in this text and the one widely used by  [?]).\n\n\n\n\n\nAs further discussed in Section B.20.3, a cyclostationary process can be converted into a WSS through the “uniform shift” or “uniformly randomizing the phase”  [?].\n\n\n\nThe following example aims at making these concepts more concrete.\n\n\n\n Example B.7. Example: WGN modulated by sinusoid. Assume ν[n] represents a realization of an i. i. d. white Gaussian noise process with variance σ2 . Each realization is modulated by a carrier of frequency Ωc and phase ϕ (both in radians) leading to9\n\n\n\n\n\n\n\n\n\n x[n] = ν[n]cos ⁡ (Ωcn + ϕ). \n\n\n(B.81)\n\n\n\n\n\n\nThe top plot in Figure B.24 illustrates a single realization of x[n], as generated by Listing B.8. It can be seen that the sinusoid is “buried” in noise. All M=5000 realizations are stored in matrix X. The ensemble variance for each “time” instant n is shown in the bottom plot of Figure B.24, indicating that the variance has a period P∕2 (in this case, 15 samples, given that P=30) because it depends on the absolute value of the sinusoid amplitude at instant n. This can be also seen from the correlation of x[n] over time, as discussed in the sequel.\n\n\n\nListing B.8: MatlabOctaveCodeSnippets/snip_appprobability_modulatednoise.m\n\n\nM=5000; %number of realizations for ensemble statistics \nN=1000; %number of samples of each realization \nP=30; %sinusoid period (in samples) \nX=zeros(N,M); %pre-allocate space \n5Wc=2*pi/P; Phc=pi/5; %carrier frequency and phase, both in radians \nn=0:N-1; carrier=transpose(cos(Wc*n+Phc)); %same for all realizations \n%carrier(carrier&gt;0)=1; carrier(carrier&lt;0)=-1; %if wants square wave \nfor m=1:M %for each realization \n    X(:,m)=randn(N,1).*carrier; %WGN modulated by sinusoidal carrier \n10end\n  \n\n\n\n\n\n\n\nFigure B.24: Single realization x[n] of Eq. (B.81) (top) and the ensemble variance over time (bottom plot), which has a period of P∕2 = 15 samples.\n\n\n\n\n\nFrom Eq. (B.72), the correlation of x[n] is\n\n\n\n RX[n,l]  = 𝔼  [ν[n]cos ⁡ (Ωcn + ϕ)ν[n + l]cos ⁡ (Ωc(n + l) + ϕ)]     = cos ⁡ (Ωcn + ϕ)cos ⁡ (Ωc(n + l) + ϕ)𝔼  [ν[n]ν[n + l]] = cos ⁡ 2(Ω cn + ϕ)σ2δ[l]     = σ2 2  (1 + cos ⁡ (2Ωcn + 2ϕ))δ[l], (B.82)  \n\nwhich is depicted in Figure B.25(b). The value of δ[l] is 1 for all n when l = 0 and zero otherwise. Note that Figure B.25(a) is the alternative representation with a pair of absolute time instants [n1,n2].\n\n\n\n                                   &lt;div class=\"center\" \n\n\n\n\n\n\n\n\n\n\n\n\n(a)  &lt;mrow &lt;mi R&lt;mrow &lt;mi X[&lt;mrow &lt;mi n&lt;mrow 1,&lt;mrow &lt;mi n&lt;mrow 2]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(b)  &lt;mrow &lt;mi R&lt;mrow &lt;mi X[&lt;mi n,&lt;mi l]\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n(c)  |&lt;mrow &lt;mi C&lt;mrow &lt;mi X[&lt;mrow &lt;mi α&lt;mrow &lt;mi k,&lt;mi l]|\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(d)  |&lt;mrow &lt;mi S&lt;mrow &lt;mi X(&lt;mrow &lt;mi α&lt;mrow &lt;mi k,Ω)|"
  },
  {
    "objectID": "ak_dsp_bookap3.html",
    "href": "ak_dsp_bookap3.html",
    "title": "Appendix C — Useful Softwares and Programming Tricks",
    "section": "",
    "text": "[next] [prev] [prev-tail] [tail] [up]\n\n\n\nC.  Useful Softwares and Programming Tricks\n\n\n\n\n\nC.1  Matlab and Octave\n\n\n\nFor using the companion code, add the respective directories to the Matlab/Octave PATH with commands such as addpath(genpath(’C:/mydir/Code/’),’-end’) and savepath.\n\n\n\nIn spite of many similarities, Matlab and Octave are of course two distinct softwares. In fact, compatibility with Matlab is not the top priority in Octave’s development. Hence, even if a function has the same name and syntax, the algorithms are typically implemented differently and may lead to discrepant results. Besides, Mathworks is strategically substituting functions by classes in new versions, such that functions such as butter (for designing a filter) are invoked after an object is created. This has been making Matlab and Octave increasingly different.\n\n\n\nAs another example of distinct behavior, fir1 in Matlab normalizes the filter using:\n\n\nif First_Band \n    b = b / sum(b);  % unity gain at DC \nelse \n    ...\n\n\nwhile fir1 in Octave uses:\n\n\n  ## normalize filter magnitude \n  if scale == 1 \n    ## find the middle of the first band edge \n    if m(1) == 1, w_o = (f(2)-f(1))/2; \n5    else w_o = f(3) + (f(4)-f(3))/2; \n    endif \n    ## compute |h(w_o)|^-1 \n    renorm = 1/abs(polyval(b, exp(-1i*pi*w_o))); \n    ## normalize the filter \n10    b = renorm*b; \n  endif\n\n\nTherefore, the command B=fir1(10,0.4) in Matlab generates a filter that has a gain of 1 at DC, while this command in Octave generates a filter with gain of 0.7557 dB at DC.\n\n\n\nThe bilinear function is a representative example of different syntaxes, which is a major problem when dealing simultaneously with Matlab and Octave. While Matlab uses the sampling frequency Fs, Octave uses the sampling period Ts = 1∕Fs. Hence, for obtaining approximately the same results, the call [NUMd,DENd] = bilinear(NUM,DEN,Fs) in Matlab must be mapped to [NUMd,DENd] = bilinear(NUM,DEN,1/Fs) in Octave. Another example of discrepancy is the third argument of pwelch.m). Octave assumes it is a percentage while Matlab assumes it is the number of samples. The Web has good sites comparing the two softwares, such as [urlomoc].\n\n\nC.1.1  Octave Installation\n\n\n\nThe primary source for Octave is [urlooct]. But, for better organization, most of Octave’s packages (called toolboxes in Matlab’s jargon) are kept separated, at Octave Forge: [urloofo].\nTherefore, after installing Octave itself, it may be necessary to download and install its packages (toolboxes).\n\n\n\nAfter installing Octave, from its prompt, the command ver can be used to verify not only the Octave version but also its toolboxes. For example, the toolbox signal is required to execute several of the companion scripts. When using Linux, it is possible to use a command such as [urloins]:\n\n\nsudo apt-get install $( apt-cache search octave-forge | awk '{printf $1; printf \" \"}' )\n\n\nto install the packages.\n\n\n\nAn alternative to this two-steps installation procedure is to obtain a complete “distribution” of Octave, with all (or most) packages of interest already incorporated. For example, installing Octave on Windows can be simplified by following the guidelines at [urlowin], which indicate how to install the Octave executable from a single file and then most of its packages from a second compressed file.\n\n\n\nIt is suggested to add the folder with the companion functions in your Octave PATH using addpath. For example, to add the folder\n\nC:\\Code\\MatlabOctaveFunctions\n\n\n\nin a Windows machine, avoid the backslash using\n\naddpath(\"C:/Code/MatlabOctaveFunctions\",\"-end\")\n\n\n\nor\n\naddpath([\"C:\" filesep \"Code\" filesep \"MatlabOctaveFunctions\"],\"-end\")\n\n\n\nand use savepath to save the modifications.\n\n\n\nMake sure the command sound (or soundsc) is working. You may need to install a sound player and inform Octave of its PATH using an Octave’s global environment variable called sound_play_utility. A good strategy is to setup this variable at the .octaverc file with something like global sound_play_utility = \"c:/Program Files/VideoLan/VLC/vlc\", but changing the PATH to your installed audio player [url1rec].\n\n\n\nThen keep your Octave installation up to date. For installing a package, for example, from the Octave prompt it is possible to run pkg install -forge signal to install the signal package.\n\n\n\nAnother point worth mentioning is that some packages, such as the Fixed Point, are not part of the ones supported by Octave Forge and need to be installed individually (taking in account their compatibility with the installed version of Octave).\n\n\n\n\n\nC.2  Manipulating signals stored in files\n\n\n\n\n\nC.2.1  Hex / Binary File Editors\n\n\n\nIt is important to be able to visualize the contents of files. For this task one can use hex (hexadecimal) editors such as [urlBMhex]. This is especially useful for binary files, but it is interesting for text files too.\n\n\n\nThe companion software FileViewer can also be used. It is a Java program that can be invoked, for example, from command line withjava -jar FileViewer.jar\n\n\n\nThis software was used to analyze a Windows (or DOS) text file called fileviewer_test.txt, which has 52 bytes and the following contents:\n\nA simple text\nwith 52 bytes:\nTo be or not to be?\n\n\n\nFigure C.1 is a screenshot of the file, showing each byte in hexadecimal in the main panel. One can see that the first letter, a capital ‘A’ is represented by the byte 0x41 in hexadecimal (the prefix 0x indicates an hexadecimal number). Clicking on the button “Decimal” (at the left of “Hexa”), one can note that 0x41 is 65 in decimal.\n\n\n\n\n\n\n\n\nFigure C.1: Screenshot of the FileViewer software.\n\n\n\n\n\nThese plain text files are known as ASCII files, where ASCII stands for American Standard Code for Information Interchange. These files do not have the many extra bytes that a modern text editor (e.g, Microsoft Word and Open Office Writer) inserts to represent extra information, such as font size, color, etc.\n\n\nC.2.2  ASCII Text Files: Unix/Linux versus Windows\n\n\n\nThere is an important distinction between how Windows and Unix’es (e.g., Linux) interpret text files. Details are provided in [urlBMlfc] and other sites. In summary, Windows uses two ASCII characters (CR and LF) to indicate a new line, while Linux uses LF alone. In hexadecimal these ASCII characters are represented by 0x0A (LF or line feed) and 0x0D (CR or carriage return). Note in Figure C.1 that each line is terminated by a pair of bytes 0x0D 0x0A because fileviewer_test.txt is a Windows text file. In fact, when using a regular text editor, the number of visible letters in this file is 46, but each of the three new lines demand 2 extra (invisible) bytes, so the total file size is 46 + 3 × 2 = 52 bytes.\n\n\n\nThe FileViewer software can be used to convert between Linux and Windows text files. In menu Tools / Converter one can find the dialog window depicted in Figure C.2.\n\n\n\n\n\n\n\n\nFigure C.2: Screenshot of the FileViewer dialog window that allows to convert between Linux and Windows text files.\n\n\n\n\n\nThe result of executing the instruction indicated in Figure C.2 is indicated in Figure C.3, which shows in the Description Panel that the file size was reduced to 49 bytes. The Main Panel also indicates that the pair 0x0D 0x0A was substituted by the byte 0x0A, which is enough to represent a new line on Linux.\n\n\n\n\n\n\n\n\nFigure C.3: Screenshot of the FileViewer software showing the result of converting the Windows file of Figure C.1 to the Linux format. Note that the new lines are signaled by the byte 0x0A.\n\n\n\n\nC.2.3  Binary Files: Big versus Little-endian\n\n\n\nAnother important distinction is the little-endian versus the big-endian (endian, not indian) representation of variables composed of more than one byte. In the big-endian representation, the most significant byte (MSB) is the first. In little-endian, the least significant byte (LSB) is the first. For example, suppose one is dealing with a binary file that stores values represented by short variables. A short is represented by two bytes in most languages, such as C and Java. The first two bytes of this file are 0x02 and 0x01. Assuming big-endian, they would be interpreted as the number 513 in decimal (0x0201 in hexa), while in little-endian this pair of bytes corresponds to 258 in decimal (0x0102 in hexa). Note that in this example both numbers are positive. When dealing with negative numbers, one needs to pay attention to the adopted representation. Negative integers are typically represented in two’s-complement arithmetic [urlBMwtc].\n\n\n\nFigure C.4 illustrates the result of interpreting the bytes in the file of Figure C.3 as short. From Figure C.3 one can see that the first two bytes are 0x41 0x20, which (in this order 0x4120) corresponds to 16,672 in decimal. Using the option in FileViewer to interpret these bytes as little-endian would lead to 8,256 (i.e., 0x2041).\n\n\n\n\n\n\n\n\nFigure C.4: Interpretation of the file in Figure C.3 as short (2-bytes) big-endian elements.\n\n\n\n\n\nThe companion C code laps_swap.c can swap bytes and create little-endian files from big-endian files and vice-versa. Note that the code simply reverses the order of the bytes. Its usage requires knowing the number of bytes of each element. For example, short corresponds to 2 bytes, float to 4, etc.\n\n\n\nIt is worth considering the following situation: Java always uses big-endian, even in computers that adopt a little-endian architecture such as the PC. After compiled, laps_swap.c can be used to convert binary big-endian files generated by a Java program into little-endian files to be interpreted by C programs executed in a PC. Assuming double (8 bytes per element), the commandlaps_swap.exe inputFile.bin outputFile.bin 8will swap the bytes of inputFile.bin (assumed to be big-endian) and create a file called outputFile.bin. FileViewer can be used to check the conversion.\n\n\nC.2.4  Some Useful Code to Manipulate Files\n\n\n\n\n\nMatlab/Octave language\n\n \n\nAssume we have a vector z in Matlab/Octave. The command save can be used to create a file somefile.txt and store z as followssave -ascii ’somefile.txt’ z\n\n\n\nHowever, save does not allow much control on formatting the output numbers. For improved formatting, one can use the fprintf function:\n\n\nz=rand(10,1); %generate a random vector \nfp = fopen('somefile.txt','wt'); %open the file for writing \nfprintf(fp,'%12.8f\\n',z); %save according to the specified format \nfclose(fp);  %close the file\n\n\nThe command load can be used to read in data. For example:x = load(’anotherfile.txt’);would create a variable x with the contents of anotherfile.txt.\n\n\n\nText (ASCII) files have a relatively large size when compared to binary files and require parsing. Hence, the interface between other programs and Matlab/Octave is often more efficient when done via binary files. The following code illustrates how to read a binary file with samples stored as 16 bits.\n\n\nfp=fopen('somefile.bin','rb'); %open for reading in binary \nx=fread(fp,Inf,'int16'); %read all samples as signed 16-bits \nfclose(fp);  %close the file\n\n\nWhen dealing with binary files one should inform the endianness when opening the file. When Matlab/Octave is running on PCs, its default is little-endian. For example, fid = fopen(’littleendianfile.bin’,’rb’); opens a file and prepares to interpret its data in little-endian. In contrast, fid = fopen(’bigendianfile.bin’,’rb’,’b’); assumes the file is big-endian. In any case, the command [filename, permission, machineformat, encoding]=fopen(fid) retrieves the information associated to the opened fid variable.\n\n\n\nIn case of dealing with matrices representing images, one may need some extra commands such as, for example:\n\n\n% Read image of size rows x cols stored in unsigned chars (no header) \n rows = 256; cols = 256; %number of pixels in rows / columns \n fid = fopen('myimage.bin','r'); %open the file \n out = fread(fid,[rows,cols],'uchar'); %read the file \n5 out = flipud(rot90(out)); %reorganize the data \n fclose(fid); %close the file\n\n\n\n\nC language\n\n\n\nThe goal of this section is to illustrate how to read into Matlab/Octave data stored in files that were written by C programs and vice-versa. The example below shows how to generate a cosine signal in C, write it into a file and load in Matlab/Octave.\n\n\n#include &lt;stdio.h&gt; \n#include &lt;stdlib.h&gt; \n#include &lt;math.h&gt; \n/* Program to generate a cosine of 200 Hz and amplitude 10 */ \n5#define D 5 /* duration in seconds */ \nint main() { \n  FILE *fp; \n  char name[2048]=\"cosine.txt\"; \n  double sample; \n10  int i; /*counter*/ \n  double freq_cosine = 200; \n  double Fs = 8000; /*sampling frequency*/ \n  int n; /* total number of samples */ \n  double Ts; /*  sampling period */ \n15  double t; /* current time */ \n \n  n = (int) (Fs * D); \n  Ts = 1/Fs;   \n  fp = fopen(name, \"w\"); /* open the output file */ \n20  if (fp == NULL) {   \n    printf(\"Error opening file %s\\n\", name); \n    exit(-1); /* exit if error */ \n  } \n  /* write all samples */ \n25  for (i=0; i&lt;n; i++) { \n    t = i * Ts; /* calculate current time */ \n    sample = 10.0*cos(2.0*M_PI*freq_cosine*t); \n    fprintf(fp, \"%f\\n\", sample); \n  } \n30  fclose(fp); /* close file */ \n  printf(\"Wrote %d samples in file %s\\n\",n,name); \n}\n\n\nAfter compiling, executing the program and generating the file cosine.txt, it is possible to load it in Matlab/Octave, listen to the signal and observe its spectrogram with the commands:\n\n\nx = load('cosine.txt'); \nsoundsc(x) \nspecgram(x,[],8000); \ncolorbar\n\n\nNote that the sampling frequency 8,000 Hz is informed to the specgram function for proper labeling of the axes.\n\n\n\nConsidering that the text file somefile.txt (see Section C.2.4.0) should be processed in a C program, the following code illustrates how to read it. Note that the value of N should be changed according to your needs.\n\n\n#include &lt;stdio.h&gt; \n#include &lt;stdlib.h&gt; \n#define N 10 /*number of samples in file*/ \nint main() { \n5  FILE *fp; /*file pointer*/ \n  char name[2048]=\"somefile.txt\"; \n  double signal[N]; \n  int i; /*counter*/   \n  fp = fopen(name, \"r\"); /* open file for reading */ \n10  if (fp == NULL) {   \n    printf(\"Error opening file %s\\n\", name); \n    exit(-1); /* exit if error */ \n  } \n  for (i=0; i&lt;N; i++) { \n15    /* note the use of \"%lf\" because the elements are double: */ \n    fscanf(fp, \"%lf\\n\", &signal[i]); /*read N samples*/ \n  } \n  /*from here you can manipulate signal*/ \n  fclose(fp); /* close file */ \n20  printf(\"Finished reading %d samples from file %s\",N,name); \n}\n\n\n\n\nJava language\n\n\n\nJava is a modern language and has many features to deal with files. Listing C.1 illustrates writing and reading a file with float samples using a small header, which informs the number of samples. Recall that Java always uses big-endian. The output file has a size of 22 bytes, corresponding to 5 float (4-bytes) samples plus the 2-bytes header.\n\n\n\nListing C.1: Java_Language/FileManipulation.java\n\n\n/** Example of reading and writing binary (float) files.*/ \nimport java.io.*; \n \npublic class FileManipulation { \n5 \n  public static void main(String[] args) { \n  String fileName = \"floatsamples.bin\"; \n  float[] x = {-2,-1,0,1,2}; \n  writeVectorToBinaryFile(fileName, x); \n10  float[] y = readVectorFromBinaryFile(fileName); \n  System.out.println(\"Samples obtained from file:\"); \n  for (int i=0;i&lt;y.length;i++) { \n    System.out.print(y[i] + \" \"); \n  } \n15  System.out.println(\"\\nFinished writing and reading \"+ fileName); \n  } \n \n  /** Method to write a binary file*/ \n  public static void writeVectorToBinaryFile(String fileName, \n20                                             float[] vector) { \n    try { \n      File file = new File(fileName); \n      FileOutputStream fileOutputStream = new FileOutputStream(file); \n      DataOutputStream dataOutputStream = new DataOutputStream(fileOutputStream); \n25      //create a small header with the number of samples \n      dataOutputStream.writeShort( (short) vector.length); \n      for (int i = 0; i &lt; vector.length; i++) { \n        dataOutputStream.writeFloat(vector[i]); \n      } \n30      dataOutputStream.close(); \n      fileOutputStream.close(); \n    } \n    catch (IOException e) { \n      e.printStackTrace(); \n35      System.err.println(\"Problem writing file \" + fileName); \n    } \n  } \n \n  /** Method to read a binary file*/ \n40  public static float[] readVectorFromBinaryFile(String fileName) { \n    float[] x = null; \n    try { \n      File file = new File(fileName); \n      FileInputStream fileInputStream = new FileInputStream(file); \n45      DataInputStream dataInputStream = new DataInputStream(fileInputStream); \n      //read the number of samples (in header) \n      int numSamples = (int) dataInputStream.readShort(); \n      x = new float[numSamples]; \n      for (int i = 0; i &lt; numSamples; i++) { \n50          x[i] = dataInputStream.readFloat(); \n      } \n      fileInputStream.close(); \n    } \n    catch (IOException e) { \n55      e.printStackTrace(); \n      System.err.println(\"Problem reading file \" + fileName); \n    } \n    return x; \n  } \n60 \n}\n  \n\n\n\n\nC.2.5  Interpreting binary files with complex headers\n\n\n\nSome binary files are headerless (also called raw files), i.e., they do not contain any extra information in a header, just the data (for example, the samples of a signal). However, most binary files start with a header. If the header information is encoded in the same format as the data itself, it is relatively easy to inspect the file contents using a software such as FileViewer. However, if the header is composed by a mix of float, short, char variables, etc., then the interpretation is trickier.\n\n\n\nAs an example, assume the file floatsamples.bin generated with Listing C.1. Figure C.5 shows the floatsamples.bin file contents interpreted as hexadecimal bytes.\n\n\n\n\n\n\n\n\nFigure C.5: Contents in hexadecimal of file floatsamples.bin generated with Listing C.1.\n\n\n\n\n\nThe data is in float format but the header is a 2-bytes short. Hence, reading the file as float will get all numbers wrong as shown in Figure C.6. The correct would be to skip the header and then read the data.\n\n\n\n\n\n\n\n\nFigure C.6: Interpretation as big-endian floats of file floatsamples.bin generated with Listing C.1. The numbers are wrong (the data after the header is  − 2,−1,0,1,2) because the reading is not aligned with the end of the 2-bytes header.\n\n\n\n\n\nThe companion code laps_dump.c can be helpful in situations where the header is heterogeneous. One can study its source code to understand how to simultaneously visualize all interpretations (short, float, etc.). For example, the interpretation of the big-endian file floatsamples.bin generated with Listing C.1, which is not trivial on a little-endian PC, can be performed as follows.\n\n\n\nRecall that the useful information is the sequence 5,−2.0,−1.0,0.0,1.0,2.0. The commandlaps_dump.exe floatsamples.bin swapprovides the output below. Each line indicates distinct interpretations (char, short, etc.) assuming the element starts at the corresponding byte. For example, the second line below indicates that the corresponding byte is 5, its ASCII code (fourth column) is a special character, and the fifth column interprets this byte as the beginning of a short variable composed by the pair of bytes 0x05C0, which corresponds to 1472 in decimal. Note that the numbers of interest in this case are inside frame boxes. The file size is 22 bytes, but after reading 16 bytes, the code tries to read the next double (8 bytes) and aborts the execution because there are only 6 bytes remaining. This is the reason for not observing the last element 2.0. Typically the last bytes will be missed.\n\n\n\nEach columns represents:\n\n&lt;byte counter&gt; &lt;decimal char&gt; &lt;hexa char&gt; &lt;(ASCII)&gt; ...\n  &lt;short&gt; &lt;int&gt; &lt;long int&gt; &lt;float&gt; &lt;double&gt;\n\n\n\n1 0 0 ( ) 5 376832 376832 5.280541e-40 7.996359e-3092 5 5 (?) 1472 96468992 96468992 1.805559e-35 5.509016e-2813 192 c0 ( ) -16384 -1073741824 -1073741824 -2.000000e+00 -2.000001e+004 0 0 ( ) 0 191 191 2.676480e-43 4.063622e-3125 0 0 ( ) 0 49024 49024 6.869726e-41 1.040287e-3096 0 0 ( ) 191 12550144 12550144 1.758650e-38 4.485749e-3057 191 bf ( ) -16512 -1082130432 -1082130432 -1.000000e+00 -7.812500e-038 128 80 (?) -32768 -2147483648 -2147483648 -0.000000e+00 -3.112614e-3229 0 0 ( ) 0 0 0 0.000000e+00 8.031531e-32010 0 0 ( ) 0 0 0 0.000000e+00 2.056072e-31711 0 0 ( ) 0 0 0 0.000000e+00 5.263544e-31512 0 0 ( ) 0 63 63 8.828180e-44 1.347467e-31213 0 0 ( ) 0 16256 16256 2.277951e-41 3.449516e-31014 0 0 ( ) 63 4161536 4161536 5.831554e-39 1.752246e-30715 63 3f (?) 16256 1065353216 1065353216 1.000000e+00 7.812502e-0316 128 80 (?) -32768 -2147483584 -2147483584 -8.968310e-4416 bytes read from file floatsamples.bin\n\n\n\nAnother example is the interpretation of a binary file generated by HCopy, which is part of the HTK package [urlBMhtk], widely used in speech recognition. Using the HList tool to interpret a file htk_file.bin with the command HList -t htk_file.binleads to the output:\n\n--------------------- Target -----------------------\n  Sample Bytes:  96       Sample Kind:   MFCC\n  Num Comps:     24       Sample Period: 6439.9 us\n  Num Samples:   855      File Format:   HTK\n\n\n\n\n\n\nHTK assumes the file content is a multi-variate time series with 855 samples of dimension 24 each. Because each of these 24 elements is represented by a 4-bytes float, the total size of a sample is 96 bytes. The sample period of 6439.9 microseconds indicates that the sampling frequency was Fs = 106∕6439.9 = 155.28 Hz. MFCC is an identifier that is internally represented by a 2-bytes short of value 6 in this case (MFCC is typically represented by the short 838 in HTK) and indicates the kind of parameters in this time series. Figure C.7 shows the first bytes of the file htk_file.bin. The HTK manual informs that this header is composed by 12 bytes and the file is big-endian (by default). The first element is a four-bytes integer (int) that indicates the number of samples (855 in this case). The second header element is again an int that indicates the sample period in the unit of 10−7 seconds. The third element is a 2-bytes short that informs the sample size in bytes (96 in this case) and the last element is another short that specifies the kind of parameters (6 in this case).\n\n\n\n\n\n\n\n\nFigure C.7: Contents interpreted as floats of big-endian file htk_file.bin generated with HTK. Note the first 12 bytes (3 floats) correspond to the header.\n\n\n\n\n\nUsing the commandlaps_dump.exe htk_file.bin swapallows to investigate the contents of the file. The elements of interest are surrounded by frame boxes in the output below and can be compared to the previous output of the HList command.\n\n\n\nEach columns represents:\n\n&lt;byte counter&gt; &lt;decimal char&gt; &lt;hexa char&gt; &lt;(ASCII)&gt; ...\n  &lt;short&gt; &lt;int&gt; &lt;long int&gt; &lt;float&gt; &lt;double&gt;\n\n\n\n1 0 0 ( ) 0 855 855 1.198110e-42 1.814306e-3112 0 0 ( ) 3 218880 218880 3.067162e-40 4.644624e-3093 3 3 (?) 855 56033280 56033280 6.318282e-37 1.440497e-2924 87 57 (W) 22272 1459618043 1459618043 1.407417e+14 1.202742e+1115 0 0 ( ) 0 64399 64399 9.024222e-41 1.366544e-3096 0 0 ( ) 251 16486144 16486144 2.310201e-38 6.279160e-3047 251 fb ( ) -1137 -74514336 -74514336 -1.485012e+36 -1.475190e+2878 143 8f ( ) -28928 -1895800832 -1895800832 -6.329376e-30 -2.011753e-2369 0 0 ( ) 96 6291462 6291462 8.816216e-39 7.120277e-30710 96 60 (’) 24576 1610614272 1610614272 3.690024e+19 2.685490e+15411 0 0 ( ) 6 393216 393216 5.510130e-40 8.344027e-30912 6 6 (?) 1536 100663296 100663296 2.407412e-35 8.814426e-28013 0 0 ( ) 0 0 0 0.000000e+00 0.000000e+0014 0 0 ( ) 0 0 0 0.000000e+00 3.260833e-32215 0 0 ( ) 0 0 0 0.000000e+00 8.363543e-32016 0 0 ( ) 0 0 0 0.000000e+00 2.141067e-31717 0 0 ( ) 0 0 0 0.000000e+00 5.481132e-31518 0 0 ( ) 0 66 66 9.248570e-44 1.403170e-31219 0 0 ( ) 0 16928 16928 2.372118e-41 3.592114e-31020 0 0 ( ) 66 4333568 4333568 6.072622e-39 2.016473e-30721 66 42 (B) 16928 1109393408 1109393408 4.000000e+01 3.435975e+1022 32 20 ( ) 8192 536870975 536870975 1.084210e-19 1.491758e-15423 0 0 ( ) 0 16256 16256 2.277951e-41 3.449516e-31024 0 0 ( ) 63 4161536 4161536 5.831554e-39 1.752246e-30725 63 3f (?) 16256 1065353216 1065353216 1.000000e+00 7.812502e-03\n\n\n\nReading and writing HTK files. By default, HTK writes all files as big-endian. Matlab/Octave reads files according to the native architecture of the computer. Hence, on PCs Matlab reads/writes little-endian. For example, if you open a binary file with fid = fopen(’testhtk.mfc’,’rb’);, Matlab on a PC will read it as little-endian. On the other hand, fid = fopen(’testhtk.mfc’,’rb’,’b’);, reads as big-endian. A file in HTK format has a header with 12 bytes. You can write Matlab functions to read and write a file with MFCC parameters according to HTK’s format. To test, use a file generated by HCopy and compare the results of your functions with HList. To help you with parsing the header, the code below shows how to read a HTK file with Java, which always uses big-endian (even on PCs).\n\n\n    public static float[][] getParametersFromFile(String hTKFileName) { \n        float[][] fparameters = null; \n        try { \n            FileInputStream fileInputStream = \n5                new FileInputStream(new File(hTKFileName)); \n            DataInputStream dataInputStream = \n                new DataInputStream(fileInputStream); \n \n            int nSamples = dataInputStream.readInt(); //4 bytes \n10            int sampPeriod = dataInputStream.readInt(); //4 bytes \n            short sampSize = dataInputStream.readShort(); //2 bytes \n            short parmKind = dataInputStream.readShort(); //2 bytes \n            //parmKind = 838 corresponds to MFCC_E_D_A \n \n15            //allocate space (assume float, 4 bytes per number) \n            int nspaceDimension = sampSize / 4; \n            fparameters = new float[nSamples][nspaceDimension]; \n            for (int i = 0; i &lt; fparameters.length; i++) { \n                for (int j = 0; j &lt; nspaceDimension; j++) { \n20                    fparameters[i][j] = dataInputStream.readFloat(); \n                } \n            } \n            fileInputStream.close(); \n            dataInputStream.close(); \n25        } \n        catch (IOException e) { \n            e.printStackTrace(); \n            return null; \n        } \n30        return fparameters; \n    }\n\n\n\n[next] [prev] [prev-tail] [front] [up]"
  },
  {
    "objectID": "ak_dsp_bookap4.html",
    "href": "ak_dsp_bookap4.html",
    "title": "Appendix D — Glossary",
    "section": "",
    "text": "[prev] [prev-tail] [tail] [up]\n\n\n\nC.  Glossary\n\n\n\n\n\n1  Text Conventions\n\n\n\n\n\n\nf 1.\n\n\nDefinitions are indicated by  ≜.\n\n\nf 2.\n\n\nWhen discussing digital modulation, N is the number of dimensions (e.g., N = 2 for QAM), M is the number of constellation symbols, S is the number of samples per symbol (also called oversampling factor) and b = log ⁡ 2M is the number of bits per symbol.\n\n\nf 3.\n\n\nRandom variables are upper-case mathsf in Latex, e.g. X,Y and Z.\n\n\nf 4.\n\n\nRandom signals are lower-case mathsf in Latex, e.g. x[n]\n\n\nf 5.\n\n\nRandom processes use cal in Latex, e.g., X[n] and X(t)\n\n\nf 6.\n\n\nSets also use cal in Latex, e.g., C = A ∪B. The context should be enough to disambiguate random processes and sets.\n\n\nf 7.\n\n\nWe call D/C (discrete/continuous) and C/D (continuous/discrete) the theoretical models used as stages of the practical D/A and A/D conversions, respectively.\n\n\nf 8.\n\n\nAnalog (unquantized continuous-time) signals x(t). A sampled signal is indicated with a subscript s, e.g., xs(t). Discrete-time x[n] and digital xq[n], sampled and quantized xsq(t)\n &lt;/dd&gt;&lt;dt class='enumerate-enumitem'&gt;&lt;span class='ec-lmbx-10x-x-109'&gt;f&lt;/span&gt;\n\n\n\nDistinguish the signal x(t) (or x[n]) from a specific sample x(t0). At least in the beginning of the text.\n\n\nf\n\n\nUnits are taken from the international system: length in meters, time in seconds, x(t) and x[n] in Volts, X(f) in Volts/Hz and PSDs in Watts/Hz.\n\n\nf\n\n\n\n𝔼 is the expected value and Var is the variance.\n\n\nf\n\n\nThe superscripts *, T and H denote complex conjugate, transpose and Hermitian, respectively. For matrices, AH = (A∗)T .\n\n\nf\n\n\nTwo Latin sentences will be used: i.e. (id est, which means “that is”) and e.g. (exempli gratia, which means “for example”)\n\n\nf\n\n\nWhenever possible, constants and variables will be represented by upper and lower case letters, respectively\n\n\nf\n\n\nVectors are bold lower-case (e.g., x) and matrices are bold upper-case (e.g., X). When dealing with transforms, a bold upper-case letter is also used. For example, the vector X is the transform of x. The context should be enough to disambiguate transforms and matrices.\n\n\nf\n\n\nAs in Matlab/Octave, the column m of a M × N matrix A is represented by A(:,m), with m = 0,…,M − 1. Similarly, A(n,:) denotes the n-th row, with n = 0,…,N − 1.\n\n Note that the first index in Matlab/Octaveis 1.\n &lt;/dd&gt;&lt;dt class='enumerate-enumitem'&gt;&lt;span class='ec-lmbx-10x-x-109'&gt;f&lt;/span&gt;\n\n\n\nLower case can eventually denote frequency-domain variables\n\n\nf\n\n\n\n|⋅| magnitude or absolute value of a complex scalar.\n\n\nf\n\n\nang(⋅) angle of a complex scalar\n\n\nf\n\n\n\n∥x∥ is the norm of vector x\n\n\nf\n\n\nEstimates are indicated by a hat over the symbol (e.g., x^)\n\n\nf\n\n\ndiag is the main diagonal of a matrix\n\n\nf\n\n\nA matrix   [   a  b       ] can be denoted using Matlab/Octave syntax as [a,b;c,d].\n\n\nf\n\n\nThe (i,j)-th element of a matrix A, at row i and column j, is represented by ai,j.\n\n\nf\n\n\nWhen the value of a variable x is given in dB it will be denoted with a subscript xdB.\n\n\nf\n\n\nIn multicarrier systems, subscripts denote the user n while the superscripts denote the tone k\n\n\n\n\n\n\n\n\n\nTable 1: Nomenclature of special frequencies.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSymbol \n\n\nDescription \n\n\nReference \n\n\n\n\n\n\n\n\n\n\n\n\n\nωc or fc\n\n\nCutoff frequency: where gain falls by 1∕2 ( −3 dB) \n\n\nFigure 3.2\n\n\n\n\n\n\n\n\n\n\n\n\n\nωn\n\n\nNatural frequency, for example, of a resonator \n\n\nFigure 3.23\n\n\n\n\n\n\n\n\n\n\n\n\n\nω0\n\n\nCenter frequency of a pole \n\n\nFigure 3.23\n\n\n\n\n\n\n\n\n\n\n\n\n\nfp\n\n\nPassband frequency \n\n\nFigure 3.2\n\n\n\n\n\n\n\n\n\n\n\n\n\nfr\n\n\nStopband (or rejection) frequency \n\n\nFigure 3.2\n\n\n\n\n\n\n\n\n\n\n\n\n\nFs\n\n\nSampling frequency (typically in Hz or samples per sec.) \n\n\nEq. (1.11) \n\n\n\n\n\n\n\n\n\n\n\n\n\nFs∕2\n\n\nNyquist (or folding) frequency \n\n\nEq. (2.31) \n\n\n\n\n\n\n\n\n\n\n\n\n                                                                                                                                                                                                               &lt;/div&gt;\n                                                                          \n                                                                          \n\n\n\n\n\n2  Main Abbreviations\n\n &lt;ul class='itemize1'&gt;\n &lt;li class='itemize'&gt;ADSL - Asymmetric digital subscriber line\n &lt;/li&gt;\n &lt;li class='itemize'&gt;AGC - Automatic gain control\n &lt;/li&gt;\n &lt;li class='itemize'&gt;AM - Amplitude modulation\n &lt;/li&gt;\n &lt;li class='itemize'&gt;AR - Auto-regressive\n &lt;/li&gt;\n &lt;li class='itemize'&gt;BER - Bit error rate\n &lt;/li&gt;\n &lt;li class='itemize'&gt;BW - Channel bandwidth\n &lt;/li&gt;\n &lt;li class='itemize'&gt;CO - Central office\n &lt;/li&gt;\n &lt;li class='itemize'&gt;CPE - Customer premises equipment\n &lt;/li&gt;\n &lt;li class='itemize'&gt;CR - Cognitive radio\n &lt;/li&gt;\n &lt;li class='itemize'&gt;DCT - Discrete cosine transform\n &lt;/li&gt;\n &lt;li class='itemize'&gt;DFT - Discrete Fourier transform\n &lt;/li&gt;\n &lt;li class='itemize'&gt;DSB - Double-sideband\n &lt;/li&gt;\n &lt;li class='itemize'&gt;DSB-SC - Double-sideband suppressed-carrier\n &lt;/li&gt;\n                                                                          \n                                                                          \n &lt;li class='itemize'&gt;DSP - Digital signal processing\n &lt;/li&gt;\n &lt;li class='itemize'&gt;DTFS - Discrete-time Fourier series\n &lt;/li&gt;\n &lt;li class='itemize'&gt;DTFT - Discrete-time Fourier transform\n &lt;/li&gt;\n &lt;li class='itemize'&gt;ESD - Energy spectral density\n &lt;/li&gt;\n &lt;li class='itemize'&gt;FA - Fixed-margin\n &lt;/li&gt;\n &lt;li class='itemize'&gt;FB - Frequency burst\n &lt;/li&gt;\n &lt;li class='itemize'&gt;FDD - Frequency division duplex\n &lt;/li&gt;\n &lt;li class='itemize'&gt;FDM - Frequency division multiplexing\n &lt;/li&gt;\n &lt;li class='itemize'&gt;FDMA - Frequency division multiple access\n &lt;/li&gt;\n &lt;li class='itemize'&gt;GMSK - Gaussian minimum-shift keying\n &lt;/li&gt;\n &lt;li class='itemize'&gt;GR - GNU Radio\n &lt;/li&gt;\n &lt;li class='itemize'&gt;ISI - Intersymbol interference\n &lt;/li&gt;\n &lt;li class='itemize'&gt;LPC - Linear predictive coding\n &lt;/li&gt;\n &lt;li class='itemize'&gt;LSB - Least significant bit\n &lt;/li&gt;\n &lt;li class='itemize'&gt;MA - Margin-adaptive\n                                                                          \n                                                                          \n &lt;/li&gt;\n &lt;li class='itemize'&gt;MAP - Maximum a posteriori\n &lt;/li&gt;\n &lt;li class='itemize'&gt;MF - Matched filter\n &lt;/li&gt;\n &lt;li class='itemize'&gt;ML - Maximum likelihood\n &lt;/li&gt;\n &lt;li class='itemize'&gt;MSB - Most significant bit\n &lt;/li&gt;\n &lt;li class='itemize'&gt;NRZ - Non-return-to-zero\n &lt;/li&gt;\n &lt;li class='itemize'&gt;PA - Power-adaptive\n &lt;/li&gt;\n &lt;li class='itemize'&gt;PAM - Pulse amplitude modulation\n &lt;/li&gt;\n &lt;li class='itemize'&gt;PBX - Private branch exchange\n &lt;/li&gt;\n &lt;li class='itemize'&gt;PHY - Physical layer\n &lt;/li&gt;\n &lt;li class='itemize'&gt;PLL - Phase-locked loop\n &lt;/li&gt;\n &lt;li class='itemize'&gt;POTS - Plain old telephony system\n &lt;/li&gt;\n &lt;li class='itemize'&gt;PSD - Power spectral density\n &lt;/li&gt;\n &lt;li class='itemize'&gt;RA - Rate-adaptive\n &lt;/li&gt;\n &lt;li class='itemize'&gt;RF - Radio-frequency signals\n &lt;/li&gt;\n &lt;li class='itemize'&gt;ROC - Region of convergence\n &lt;/li&gt;\n &lt;li class='itemize'&gt;Rx - Receiver\n &lt;/li&gt;\n &lt;li class='itemize'&gt;RZ - Return-to-zero\n &lt;/li&gt;\n &lt;li class='itemize'&gt;SER - Symbol error rate\n &lt;/li&gt;\n &lt;li class='itemize'&gt;SNR - Signal to noise ratio: the signal power divided by the noise power.\n &lt;/li&gt;\n &lt;li class='itemize'&gt;SSB - Single-sideband\n &lt;/li&gt;\n &lt;li class='itemize'&gt;STFT - Short-time Fourier transform\n &lt;/li&gt;\n &lt;li class='itemize'&gt;SVD - Singular value decomposition\n &lt;/li&gt;\n &lt;li class='itemize'&gt;TDD - Time-division duplex\n &lt;/li&gt;\n &lt;li class='itemize'&gt;TDM - Time-division multiplex\n &lt;/li&gt;\n &lt;li class='itemize'&gt;TDMA - Time-division multiple access\n &lt;/li&gt;\n &lt;li class='itemize'&gt;Tx - Transmitter\n &lt;/li&gt;\n &lt;li class='itemize'&gt;USRP - Universal software radio peripheral\n &lt;/li&gt;\n &lt;li class='itemize'&gt;ZFE - Zero-forcing equalization&lt;/li&gt;&lt;/ul&gt;\n\n\n\n\n3  Main Symbols\n\n\n\n\n\n\n\n\n\n\n\n𝜀¯x\n\n\nAverage energy per tone\n\n\n\n\n\nC¯\n\n\nCapacity of the discrete-time AWGN channel\n\n\n\n\n\nG(f)\n\n\nEnergy spectral density (ESD)\n\n\n\n\n\nM\n\n\nSet of quantizer outputs and set of constellation symbols\n\n\n\n\n\nP\n\n\nPower (Watts), calligraphic distinguishes from probabilities P\n\n\n\n\n\nη\n\n\nEfficiency\n\n\n\n\n\n𝔼[X]\n\n\nExpected value of a random variable\n\n\n\n\n\nFs\n\n\nSampling frequency, Fs = 1∕Ts\n\n\n\n\n\nγ\n\n\nNoise margin\n\n\n\n\n\nγt\n\n\nTarget margin\n\n\n\n\n\nm^(t)\n\n\nDecoded symbol, receiver output\n\n\n\n\n\nP^(ejΩ)\n\n\nEstimated periodogram\n\n\n\n\n\nR^x[k]\n\n\nAutocorrelation for a finite-duration signal\n\n\n\n\n\nŜx(ejΩ)\n\n\nPeriodogram\n\n\n\n\n\nP\n\n\nAverage power\n\n\n\n\n\nΩ\n\n\nDiscrete-time angular frequency in radians\n\n\n\n\n\nE¯c\n\n\nAverage energy of a constellation (Joules)\n\n\n\n\n\nRsym\n\n\n\nRsym = 1∕Tsym, Which is called the symbol rate \n\n\n\n\n\nRsym\n\n\nSignaling frequency or symbol rate Rsym = 1∕Tsym\n\n\n\n\n\nσ\n\n\nStandard deviation\n\n\n\n\n\nT s\n\n\nSampling interval\n\n\n\n\n\nTsym\n\n\nSymbol interval\n\n\n\n\n\nb\n\n\nNumber of bits per sample\n\n\n\n\nBW\n\n\nBandwidth\n\n\n\n\n\nC\n\n\nCorrelation between two random variables X and Y \n\n\n\n\n\nD\n\n\nSignal space or constellation dimension (e. g., D = 2 for QAM)\n\n\n\n\n\nc(t)\n\n\nCarrier signal, typically a sinusoid cos ⁡ (2πfct + ϕc)\n\n\n\n\n\nEp\n\n\nEnergy of the shaping pulse\n\n\n\n\n\nf0\n\n\nFundamental frequency (Hz)\n\n\n\n\n\nfc\n\n\nCutoff frequency\n\n\n\n\n\nfp\n\n\nBandpass frequency\n\n\n\n\n\nfr\n\n\nStopband frequency\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nH(ejΩ)\n\n\nFrequency response of a discrete-time system\n\n\n\n\n\nH(f)\n\n\nFrequency response of a continuous-time system\n\n\n\n\n\nH(s)\n\n\nTransfer function of a continuous-time system\n\n\n\n\n\nh(t) or h[n]\n\n\nImpulse response of a system\n\n\n\n\n\nH(z)\n\n\nTransfer function of a discrete-time system\n\n\n\n\n\nL\n\n\nOversampling factor, given by L = Tsym∕Ts\n\n\n\n\n\nLCP\n\n\nPrefix-cyclic length\n\n\n\n\n\nM\n\n\nNumber of symbols in M-ary modulation\n\n\n\n\n\nm(t)\n\n\nInformation source\n\n\n\n\n\nmi\n\n\nSymbol of a constellation of M symbols\n\n\n\n\n\nMS\n\n\nMean-square\n\n\n\n\n\nN\n\n\nFFT length, and other uses, e. g., number of samples per frame\n\n\n\n\n\nN\n\n\nGaussian (normal) distribution N(μ,σ2)\n\n\n\n\n\nν\n\n\nNoise in continuous ν(t) and discrete-time ν[n]\n\n\n\n\n\nP(ω)\n\n\nSpectrum of a shaping pulse in continuous-time\n\n\n\n\n\np(t)\n\n\nShaping pulse (such as the raised cosine)\n\n\n\n\n\nQ\n\n\nQuality factor or Q-factor\n\n\n\n\n\nr(t)\n\n\nReceived signal\n\n\n\n\n\nRX(s,t)\n\n\nAutocorrelation function\n\n\n\n\n\nS(f)\n\n\nContinuous-time power spectral density (PSD)\n\n\n\n\n\nS(ejΩ)\n\n\nDiscrete-time PSD\n\n\n\n\n\ns(t)\n\n\nTransmitter output, which is the channel input\n\n\n\n\n\nSms[k]\n\n\nMean-square spectrum\n\n\n\n\n\nW(ejΩ)\n\n\nDiscrete-time Fourier transform of w[n]\n\n\n\n\n\nw[n]\n\n\nRectangular window\n\n\n\n\n\nX(τ,f)\n\n\nShort-time Fourier transform of x(t)\n\n\n\n\n\nX(ejΩ)\n\n\nDiscrete-time Fourier transform of x[n]\n\n\n\n\n\nX(f)\n\n\nContinuous-time Fourier transform of x(t) with f in Hertz\n\n\n\n\n\nX(ω)\n\n\nContinuous-time Fourier transform of x(t) with ω in rad/s\n\n\n\n\n\nX(s)\n\n\nLaplace transform of x(t)\n\n\n\n\n\nx(t)\n\n\nContinuous-time signal\n\n\n\n\n                                                                                                                                                                                           &lt;/div&gt;\n\n\n\n\n\n\n\n\n\n\nX(z)\n\n\nZ transform of x[n]\n\n\n\n\n\nx[n]\n\n\nDiscrete-time signal\n\n\n\n\n\nxb[n]\n\n\nBinary representation of a signal\n\n\n\n\n\nxi[n]\n\n\nInteger-valued signal\n\n\n\n\n\nxq(t)\n\n\nQuantized continuous-time signal\n\n\n\n\n\nxq[n]\n\n\nDigital signal\n\n\n\n\n\nxs(t)\n\n\nSampled signal\n\n\n\n\n\nXN(ejΩ)\n\n\nDTFT of windowed version of x[n] with N samples\n\n\n\n\n\nXT (f)\n\n\nContinuous-time Fourier transform of xT (t)\n\n\n\n\n\nxT (t)\n\n\nTruncated version of x(t) with duration T\n\n\n\n\n                                                                                                                                                                                    &lt;/div&gt;\n                                                                          \n                                                                          \n\n\n\n[prev] [prev-tail] [front] [up]"
  },
  {
    "objectID": "ak_dsp_bookap1.html",
    "href": "ak_dsp_bookap1.html",
    "title": "Digital Signal Processing with Python, Matlab or Octave",
    "section": "",
    "text": "&lt;!DOCTYPE html&gt;\n\n\n\nA *\n\n\n\n\n\n\n\n\n\n\n\n\n[next] [prev] [prev-tail] [tail] [up]\n\n\n\nA.  *\n\n\n\nAppendices\n\n\n\n\n[next] [prev] [prev-tail] [front] [up]"
  }
]